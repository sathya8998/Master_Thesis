{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "syjx8MlAmpNw",
        "outputId": "3eb77ccc-0686-4c6d-9fb8-7005a5bbe398"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (1.13.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.11/dist-packages (0.13.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: imbalanced-learn in /usr/local/lib/python3.11/dist-packages (0.13.0)\n",
            "Collecting hmmlearn\n",
            "  Downloading hmmlearn-0.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n",
            "Collecting fastdtw\n",
            "  Downloading fastdtw-0.3.4.tar.gz (133 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.4/133.4 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting tslearn\n",
            "  Downloading tslearn-0.6.3-py3-none-any.whl.metadata (14 kB)\n",
            "Collecting lime\n",
            "  Downloading lime-0.2.0.1.tar.gz (275 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m275.7/275.7 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.18.0)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.11/dist-packages (5.24.1)\n",
            "Collecting keras-tuner\n",
            "  Downloading keras_tuner-1.4.7-py3-none-any.whl.metadata (5.4 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: sklearn-compat<1,>=0.1 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn) (0.1.3)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.11/dist-packages (from tslearn) (0.61.0)\n",
            "Requirement already satisfied: scikit-image>=0.12 in /usr/local/lib/python3.11/dist-packages (from lime) (0.25.2)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.25.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.70.0)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.18.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.12.1)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from plotly) (9.0.0)\n",
            "Collecting kt-legacy (from keras-tuner)\n",
            "  Downloading kt_legacy-1.0.5-py3-none-any.whl.metadata (221 bytes)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.14.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.1.31)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.11/dist-packages (from scikit-image>=0.12->lime) (3.4.2)\n",
            "Requirement already satisfied: imageio!=2.35.0,>=2.33 in /usr/local/lib/python3.11/dist-packages (from scikit-image>=0.12->lime) (2.37.0)\n",
            "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.11/dist-packages (from scikit-image>=0.12->lime) (2025.2.18)\n",
            "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.11/dist-packages (from scikit-image>=0.12->lime) (0.4)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: llvmlite<0.45,>=0.44.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba->tslearn) (0.44.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n",
            "Downloading hmmlearn-0.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (165 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m165.9/165.9 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tslearn-0.6.3-py3-none-any.whl (374 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.4/374.4 kB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading keras_tuner-1.4.7-py3-none-any.whl (129 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.1/129.1 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading kt_legacy-1.0.5-py3-none-any.whl (9.6 kB)\n",
            "Building wheels for collected packages: fastdtw, lime\n",
            "  Building wheel for fastdtw (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fastdtw: filename=fastdtw-0.3.4-cp311-cp311-linux_x86_64.whl size=542097 sha256=306fbd43412c587005e6a865a58f96d0bef2b46b9daa8912d296e4076576ff9e\n",
            "  Stored in directory: /root/.cache/pip/wheels/5c/8a/f6/fd3df9a9714677410a5ccbf3ca519e66db4a54a1c46ea95332\n",
            "  Building wheel for lime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for lime: filename=lime-0.2.0.1-py3-none-any.whl size=283834 sha256=94a7c793876d15eb59c0d70d027e453d1d4b9bae6b1c8cc828a31b12f3a00f03\n",
            "  Stored in directory: /root/.cache/pip/wheels/85/fa/a3/9c2d44c9f3cd77cf4e533b58900b2bf4487f2a17e8ec212a3d\n",
            "Successfully built fastdtw lime\n",
            "Installing collected packages: kt-legacy, fastdtw, tslearn, lime, hmmlearn, keras-tuner\n",
            "Successfully installed fastdtw-0.3.4 hmmlearn-0.3.3 keras-tuner-1.4.7 kt-legacy-1.0.5 lime-0.2.0.1 tslearn-0.6.3\n"
          ]
        }
      ],
      "source": [
        "!pip install numpy pandas scipy matplotlib seaborn scikit-learn imbalanced-learn hmmlearn fastdtw tslearn lime tensorflow plotly keras-tuner tqdm\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jgkx1_dkA2RE",
        "outputId": "4abc2891-638d-473b-c7eb-972bae929d90"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[0m\u001b[01;34mBayesianOptimization_outer_fold_1\u001b[0m/  \u001b[01;34mHyperband_outer_fold_1\u001b[0m/  \u001b[01;34mRandomSearch_outer_fold_1\u001b[0m/\n",
            "\u001b[01;34mBayesianOptimization_outer_fold_2\u001b[0m/  \u001b[01;34mHyperband_outer_fold_2\u001b[0m/  \u001b[01;34mRandomSearch_outer_fold_2\u001b[0m/\n"
          ]
        }
      ],
      "source": [
        "ls /content/drive/MyDrive/tuner_dir\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tskHeYAZBerN",
        "outputId": "183e9eaf-f258-46b8-c3db-894086fe7481"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "total 24\n",
            "drwx------ 3 root root 4096 Mar  6 15:13 BayesianOptimization_outer_fold_1\n",
            "drwx------ 3 root root 4096 Mar  6 15:13 BayesianOptimization_outer_fold_2\n",
            "drwx------ 3 root root 4096 Mar  6 15:13 Hyperband_outer_fold_1\n",
            "drwx------ 3 root root 4096 Mar  6 15:13 Hyperband_outer_fold_2\n",
            "drwx------ 3 root root 4096 Mar  6 15:13 RandomSearch_outer_fold_1\n",
            "drwx------ 3 root root 4096 Mar  6 15:13 RandomSearch_outer_fold_2\n"
          ]
        }
      ],
      "source": [
        "!ls -l /content/drive/MyDrive/tuner_dir\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ygij3rEjBudb"
      },
      "outputs": [],
      "source": [
        "!chmod -R 777 /content/drive/MyDrive/tuner_dir\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z1u_gjfKB1Ng",
        "outputId": "a3245e39-6e85-439a-98a6-099f15f11f6b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{}\n"
          ]
        }
      ],
      "source": [
        "with open(\"/content/drive/MyDrive/tuner_dir/BayesianOptimization_outer_fold_1/tuner0.json\", \"r\") as f:\n",
        "    print(f.read()[:500])  # Print first 500 characters\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KL9C9vxKB4-M",
        "outputId": "f3380cea-386a-4111-9f98-1a941707fce1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "total 32K\n",
            "drwxr-xr-x 8 root root 4.0K Mar  6 15:16 .\n",
            "drwxr-xr-x 1 root root 4.0K Mar  6 15:09 ..\n",
            "drwx------ 3 root root 4.0K Mar  6 15:16 BayesianOptimization_outer_fold_1\n",
            "drwx------ 3 root root 4.0K Mar  6 15:16 BayesianOptimization_outer_fold_2\n",
            "drwx------ 3 root root 4.0K Mar  6 15:16 Hyperband_outer_fold_1\n",
            "drwx------ 3 root root 4.0K Mar  6 15:16 Hyperband_outer_fold_2\n",
            "drwx------ 3 root root 4.0K Mar  6 15:16 RandomSearch_outer_fold_1\n",
            "drwx------ 3 root root 4.0K Mar  6 15:16 RandomSearch_outer_fold_2\n"
          ]
        }
      ],
      "source": [
        "!cp -r /content/drive/MyDrive/tuner_dir /content/\n",
        "!ls -lah /content/tuner_dir/\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z66H1B72CHHP",
        "outputId": "2220b87a-6150-4918-dd46-f3ced1e1d689"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "total 6.5K\n",
            "-rwx------ 1 root root 1.9K Mar  5 14:10 oracle.json\n",
            "drwx------ 2 root root 4.0K Mar  6 15:13 trial_0\n",
            "-rwx------ 1 root root    2 Mar  5 14:10 tuner0.json\n"
          ]
        }
      ],
      "source": [
        "!ls -lah /content/drive/MyDrive/tuner_dir/BayesianOptimization_outer_fold_1/\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MHQFeqRNBnqp",
        "outputId": "4af34591-66a0-42e5-b258-82a1d9d6b3a8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/tuner_dir/RandomSearch_outer_fold_1/tuner0.json\n",
            "/content/drive/MyDrive/tuner_dir/RandomSearch_outer_fold_1/oracle.json\n",
            "/content/drive/MyDrive/tuner_dir/RandomSearch_outer_fold_1/trial_0/trial.json\n",
            "/content/drive/MyDrive/tuner_dir/RandomSearch_outer_fold_1/trial_0/build_config.json\n",
            "/content/drive/MyDrive/tuner_dir/RandomSearch_outer_fold_1/trial_0/checkpoint.weights.h5\n",
            "/content/drive/MyDrive/tuner_dir/BayesianOptimization_outer_fold_1/oracle.json\n",
            "/content/drive/MyDrive/tuner_dir/BayesianOptimization_outer_fold_1/trial_0/build_config.json\n",
            "/content/drive/MyDrive/tuner_dir/BayesianOptimization_outer_fold_1/trial_0/trial.json\n",
            "/content/drive/MyDrive/tuner_dir/BayesianOptimization_outer_fold_1/trial_0/checkpoint.weights.h5\n",
            "/content/drive/MyDrive/tuner_dir/BayesianOptimization_outer_fold_1/tuner0.json\n",
            "/content/drive/MyDrive/tuner_dir/Hyperband_outer_fold_1/trial_0000/trial.json\n",
            "/content/drive/MyDrive/tuner_dir/Hyperband_outer_fold_1/trial_0000/build_config.json\n",
            "/content/drive/MyDrive/tuner_dir/Hyperband_outer_fold_1/trial_0000/checkpoint.weights.h5\n",
            "/content/drive/MyDrive/tuner_dir/Hyperband_outer_fold_1/oracle.json\n",
            "/content/drive/MyDrive/tuner_dir/Hyperband_outer_fold_1/tuner0.json\n",
            "/content/drive/MyDrive/tuner_dir/Hyperband_outer_fold_2/tuner0.json\n",
            "/content/drive/MyDrive/tuner_dir/Hyperband_outer_fold_2/trial_0000/build_config.json\n",
            "/content/drive/MyDrive/tuner_dir/Hyperband_outer_fold_2/trial_0000/trial.json\n",
            "/content/drive/MyDrive/tuner_dir/Hyperband_outer_fold_2/trial_0000/checkpoint.weights.h5\n",
            "/content/drive/MyDrive/tuner_dir/Hyperband_outer_fold_2/oracle.json\n",
            "/content/drive/MyDrive/tuner_dir/RandomSearch_outer_fold_2/tuner0.json\n",
            "/content/drive/MyDrive/tuner_dir/RandomSearch_outer_fold_2/oracle.json\n",
            "/content/drive/MyDrive/tuner_dir/RandomSearch_outer_fold_2/trial_0/checkpoint.weights.h5\n",
            "/content/drive/MyDrive/tuner_dir/RandomSearch_outer_fold_2/trial_0/build_config.json\n",
            "/content/drive/MyDrive/tuner_dir/RandomSearch_outer_fold_2/trial_0/trial.json\n",
            "/content/drive/MyDrive/tuner_dir/BayesianOptimization_outer_fold_2/tuner0.json\n",
            "/content/drive/MyDrive/tuner_dir/BayesianOptimization_outer_fold_2/oracle.json\n",
            "/content/drive/MyDrive/tuner_dir/BayesianOptimization_outer_fold_2/trial_0/checkpoint.weights.h5\n",
            "/content/drive/MyDrive/tuner_dir/BayesianOptimization_outer_fold_2/trial_0/trial.json\n",
            "/content/drive/MyDrive/tuner_dir/BayesianOptimization_outer_fold_2/trial_0/build_config.json\n"
          ]
        }
      ],
      "source": [
        "!find /content/drive/MyDrive/tuner_dir -type f\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KyOm0Fa1ADib"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GZKnVH_KUNKD"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "import logging\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import scipy.io as sio\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import scipy.signal as signal\n",
        "from scipy.signal import butter, sosfilt, filtfilt\n",
        "from scipy.interpolate import interp1d\n",
        "from fractions import Fraction\n",
        "\n",
        "from sklearn.decomposition import FastICA, NMF, PCA\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder, label_binarize\n",
        "from sklearn.metrics import (confusion_matrix, classification_report, roc_auc_score, roc_curve, auc,\n",
        "                             accuracy_score, precision_score, recall_score, f1_score)\n",
        "from sklearn.model_selection import (KFold, train_test_split, GridSearchCV, StratifiedKFold)\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "from imblearn.over_sampling import SMOTE, RandomOverSampler\n",
        "\n",
        "from hmmlearn import hmm\n",
        "\n",
        "from fastdtw import fastdtw\n",
        "from tslearn.metrics import dtw_path, dtw\n",
        "from tslearn.preprocessing import TimeSeriesScalerMeanVariance\n",
        "\n",
        "from lime import lime_tabular\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.metrics import Metric\n",
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras.layers import (\n",
        "    Input, LSTM, Dense, Dropout, Bidirectional, BatchNormalization,\n",
        "    Conv1D, GlobalMaxPooling1D, GlobalAveragePooling1D, MultiHeadAttention, Layer\n",
        ")\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "\n",
        "import plotly.express as px\n",
        "\n",
        "from keras_tuner.tuners import BayesianOptimization, RandomSearch, Hyperband\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "#============================================================================\n",
        "# 1. USER PARAMETERS & FILE PATHS\n",
        "#============================================================================\n",
        "file_path = r\"/content/Take+2024-10-17+07.08.33+PM_XYZ.csv\"\n",
        "trigger_file_path = \"/content/trigger_signal.mat\"\n",
        "emg_files = [\n",
        "    '/content/Muovi1_EMG.mat',\n",
        "    '/content/Muovi2_EMG.mat',\n",
        "    '/content/Muovi3_EMG.mat',\n",
        "    '/content/Muovi+2_EMG.mat',\n",
        "]\n",
        "\n",
        "marker_headers_row = 3\n",
        "data_start_row = 7\n",
        "AXIS_ORDER = [\"X\", \"Y\", \"Z\"]\n",
        "INTERPOLATION_METHOD = \"linear\"\n",
        "SKIP_UNLABELED = True\n",
        "ZERO_MARKER = \"RCA\"  # Marker to recenter coordinates on\n",
        "\n",
        "#============================================================================\n",
        "# 2. READ THE RAW CSV LINES\n",
        "#============================================================================\n",
        "with open(file_path, \"r\", newline=\"\") as f:\n",
        "    reader = csv.reader(f)\n",
        "    lines = list(reader)\n",
        "\n",
        "if marker_headers_row >= len(lines) or data_start_row >= len(lines):\n",
        "    raise ValueError(\"Header or data start rows exceed total lines in file.\")\n",
        "\n",
        "#============================================================================\n",
        "# 3. DYNAMICALLY FIND STARTING COLUMN FOR MARKER DATA\n",
        "#============================================================================\n",
        "raw_header = lines[marker_headers_row]\n",
        "STARTING_COLUMN = None\n",
        "for idx, col in enumerate(raw_header):\n",
        "    if \"Sathy:LASIS\" in col:\n",
        "        STARTING_COLUMN = idx\n",
        "        break\n",
        "\n",
        "if STARTING_COLUMN is None:\n",
        "    raise ValueError(\"Could not find starting marker column (e.g., 'Sathy:LASIS').\")\n",
        "\n",
        "print(f\"'Sathy:LASIS' found at column index {STARTING_COLUMN}\")\n",
        "raw_marker_labels = raw_header[STARTING_COLUMN:]\n",
        "num_raw_cols = len(raw_marker_labels)\n",
        "print(f\"Detected {num_raw_cols} marker columns starting from index {STARTING_COLUMN}.\")\n",
        "\n",
        "num_groups = num_raw_cols // 3\n",
        "if num_raw_cols % 3 != 0:\n",
        "    print(\"WARNING: Number of marker columns is not a multiple of 3. Some data may be ignored.\")\n",
        "\n",
        "#============================================================================\n",
        "# 4. PARSE MARKER LABELS\n",
        "#============================================================================\n",
        "marker_names = []\n",
        "group_indices = []\n",
        "for g in range(num_groups):\n",
        "    cX, cY, cZ = 3 * g, 3 * g + 1, 3 * g + 2\n",
        "    label_x = raw_marker_labels[cX].strip()\n",
        "    label_y = raw_marker_labels[cY].strip()\n",
        "    label_z = raw_marker_labels[cZ].strip()\n",
        "\n",
        "    def strip_final_axis(lab):\n",
        "        for ax in [\":X\", \":Y\", \":Z\"]:\n",
        "            if lab.endswith(ax):\n",
        "                return lab[:-2].strip()\n",
        "        return lab\n",
        "\n",
        "    name_x = strip_final_axis(label_x)\n",
        "    name_y = strip_final_axis(label_y)\n",
        "    name_z = strip_final_axis(label_z)\n",
        "\n",
        "    if name_x == name_y == name_z:\n",
        "        marker_name = name_x\n",
        "    else:\n",
        "        raise ValueError(f\"Inconsistent marker naming: {label_x}, {label_y}, {label_z}\")\n",
        "\n",
        "    if SKIP_UNLABELED and \"Unlabeled\" in marker_name:\n",
        "        continue\n",
        "\n",
        "    marker_names.append(marker_name)\n",
        "    group_indices.append((cX, cY, cZ))\n",
        "\n",
        "print(f\"Kept {len(marker_names)} markers.\")\n",
        "\n",
        "final_headers = []\n",
        "for mname in marker_names:\n",
        "    safe_name = mname.replace(\":\", \"_\")\n",
        "    for ax in AXIS_ORDER:\n",
        "        final_headers.append(f\"{safe_name}_{ax}\")\n",
        "\n",
        "#============================================================================\n",
        "# 5. EXTRACT NUMERIC DATA AND FRAME/TIME\n",
        "#============================================================================\n",
        "data_rows = lines[data_start_row:]\n",
        "frame_col = [int(row[0].strip()) for row in data_rows]\n",
        "time_col = [float(row[1].strip()) for row in data_rows]\n",
        "\n",
        "all_numeric_rows = []\n",
        "for row in data_rows:\n",
        "    slice_ = row[STARTING_COLUMN:]\n",
        "    float_vals = []\n",
        "    for (ix, iy, iz) in group_indices:\n",
        "        valx = slice_[ix] if ix < len(slice_) else \"\"\n",
        "        valy = slice_[iy] if iy < len(slice_) else \"\"\n",
        "        valz = slice_[iz] if iz < len(slice_) else \"\"\n",
        "\n",
        "        def to_float_or_nan(v):\n",
        "            try:\n",
        "                return float(v.strip()) if v.strip() != \"\" else np.nan\n",
        "            except:\n",
        "                return np.nan\n",
        "\n",
        "        float_vals.extend([to_float_or_nan(valx), to_float_or_nan(valy), to_float_or_nan(valz)])\n",
        "    all_numeric_rows.append(float_vals)\n",
        "\n",
        "df = pd.DataFrame(all_numeric_rows, columns=final_headers)\n",
        "df.insert(0, \"Frame\", frame_col)\n",
        "df.insert(1, \"Time (Seconds)\", time_col)\n",
        "\n",
        "#============================================================================\n",
        "# 6. INTERPOLATE MISSING VALUES\n",
        "#============================================================================\n",
        "df = df.interpolate(method=INTERPOLATION_METHOD, limit_direction=\"both\", axis=0)\n",
        "df = df.round(6)\n",
        "print(f\"DataFrame shape after interpolation: {df.shape}\")\n",
        "\n",
        "#============================================================================\n",
        "# 7. LOAD TRIGGER SIGNAL AND EXTRACT SIMPLE START/END TIMES\n",
        "#============================================================================\n",
        "trigger_data = sio.loadmat(trigger_file_path)\n",
        "trigger_signal = trigger_data['Data'][0, 0].squeeze()\n",
        "trigger_time = trigger_data['Time'][0, 0].squeeze()\n",
        "\n",
        "trigger_start = trigger_time[0]\n",
        "trigger_end = trigger_time[-1]\n",
        "print(f\"Trigger start time: {trigger_start}, end time: {trigger_end}\")\n",
        "\n",
        "#============================================================================\n",
        "# 8. ALIGN AND TRIM MARKER DATA BASED ON TRIGGER SIGNAL\n",
        "#============================================================================\n",
        "aligned_markers_data = df[\n",
        "    (df[\"Time (Seconds)\"] >= trigger_start) &\n",
        "    (df[\"Time (Seconds)\"] <= trigger_end)\n",
        "].copy()\n",
        "\n",
        "# Remove duplicate columns to avoid reindexing issues\n",
        "aligned_markers_data = aligned_markers_data.loc[:, ~aligned_markers_data.columns.duplicated()]\n",
        "\n",
        "#============================================================================\n",
        "# 9. RECENTER MARKER COORDINATES RELATIVE TO RCA\n",
        "#============================================================================\n",
        "rca_columns = {}\n",
        "for axis in AXIS_ORDER:\n",
        "    plain = f\"{ZERO_MARKER}_{axis}\"\n",
        "    prefixed = f\"Sathy_{ZERO_MARKER}_{axis}\"\n",
        "    if plain in aligned_markers_data.columns:\n",
        "        rca_columns[axis] = plain\n",
        "    elif prefixed in aligned_markers_data.columns:\n",
        "        rca_columns[axis] = prefixed\n",
        "    else:\n",
        "        raise ValueError(f\"Column for RCA axis {axis} not found in marker data.\")\n",
        "\n",
        "for axis in AXIS_ORDER:\n",
        "    rca_col = rca_columns[axis]\n",
        "    rca_series = aligned_markers_data[rca_col]\n",
        "    if isinstance(rca_series, pd.DataFrame):\n",
        "        rca_series = rca_series.iloc[:, 0]\n",
        "\n",
        "    axis_cols = [col for col in aligned_markers_data.columns\n",
        "                 if col.endswith(f\"_{axis}\")\n",
        "                 and col not in [\"Time (Seconds)\", \"Frame\", rca_col]]\n",
        "    for col in axis_cols:\n",
        "        aligned_markers_data.loc[:, col] = aligned_markers_data[col] - rca_series\n",
        "\n",
        "#============================================================================\n",
        "# X. RESAMPLE MARKER DATA TO 2000 Hz\n",
        "#============================================================================\n",
        "time_array = aligned_markers_data[\"Time (Seconds)\"].values\n",
        "orig_fs = 1 / np.median(np.diff(time_array))\n",
        "target_fs = 2000.0\n",
        "ratio = target_fs / orig_fs\n",
        "ratio_frac = Fraction(ratio).limit_denominator(1000)\n",
        "p = ratio_frac.numerator\n",
        "q = ratio_frac.denominator\n",
        "\n",
        "marker_cols = [col for col in aligned_markers_data.columns if col not in [\"Time (Seconds)\", \"Frame\"]]\n",
        "marker_data = aligned_markers_data[marker_cols].values\n",
        "\n",
        "resampled_data = signal.resample_poly(marker_data, up=p, down=q, axis=0)\n",
        "\n",
        "new_len = resampled_data.shape[0]\n",
        "new_time = np.linspace(time_array[0], time_array[-1], new_len)\n",
        "\n",
        "resampled_df = pd.DataFrame(resampled_data, columns=marker_cols)\n",
        "resampled_df.insert(0, \"Time (Seconds)\", new_time)\n",
        "resampled_df.insert(0, \"Frame\", range(len(resampled_df)))\n",
        "\n",
        "aligned_markers_data = resampled_df\n",
        "\n",
        "#----------------------------------------------------------------------------\n",
        "# Remove specified columns and prefixes from marker data\n",
        "#----------------------------------------------------------------------------\n",
        "for col in [\"Active 256_X\", \"Active 256_Y\", \"Active 256_Z\"]:\n",
        "    if col in aligned_markers_data.columns:\n",
        "        aligned_markers_data.drop(columns=[col], inplace=True)\n",
        "\n",
        "new_columns = {col: col.replace(\"Sathy_\", \"\") for col in aligned_markers_data.columns if \"Sathy_\" in col}\n",
        "aligned_markers_data.rename(columns=new_columns, inplace=True)\n",
        "\n",
        "#============================================================================\n",
        "# 10. APPLY STANDARD SCALER, SAVE AND VISUALIZE ALIGNED, RECENTERED & RESAMPLED MARKER DATA\n",
        "#============================================================================\n",
        "# Apply StandardScaler to marker columns (excluding \"Frame\" and \"Time (Seconds)\")\n",
        "marker_columns = [col for col in aligned_markers_data.columns if col not in [\"Frame\", \"Time (Seconds)\"]]\n",
        "scaler_markers = StandardScaler()\n",
        "aligned_markers_data[marker_columns] = scaler_markers.fit_transform(aligned_markers_data[marker_columns])\n",
        "\n",
        "aligned_markers_data.to_csv('/content/aligned_markers_data_cleaned.csv', index=False)\n",
        "print(\"Aligned, recentered, and resampled marker data saved.\")\n",
        "\n",
        "for col in aligned_markers_data.columns:\n",
        "    if col not in [\"Time (Seconds)\", \"Frame\"]:\n",
        "        plt.figure(figsize=(12, 6))\n",
        "        plt.plot(aligned_markers_data[\"Time (Seconds)\"], aligned_markers_data[col], label=col)\n",
        "        plt.axvline(trigger_start, color='green', linestyle='--', label='Trigger Start')\n",
        "        plt.axvline(trigger_end, color='red', linestyle='--', label='Trigger End')\n",
        "        plt.title(f'Aligned & Recentered & Resampled Marker Data ({col})')\n",
        "        plt.xlabel('Time (s)')\n",
        "        plt.ylabel('Position (relative to RCA)')\n",
        "        plt.legend()\n",
        "        plt.grid()\n",
        "        plt.show()\n",
        "\n",
        "#============================================================================\n",
        "# 11. VISUALIZE TRIGGER SIGNAL WITH BOUNDS\n",
        "#============================================================================\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(trigger_time, trigger_signal, label='Trigger Signal')\n",
        "plt.axvline(trigger_start, color='green', linestyle='--', label='Trigger Start')\n",
        "plt.axvline(trigger_end, color='red', linestyle='--', label='Trigger End')\n",
        "plt.title('Trigger Signal')\n",
        "plt.xlabel('Time (s)')\n",
        "plt.ylabel('Signal Amplitude')\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()\n",
        "\n",
        "#============================================================================\n",
        "# 12. FUNCTION TO ALIGN AND TRIM EMG DATA BASED ON TRIGGER\n",
        "#============================================================================\n",
        "def align_emg_data(file_path, trigger_start, trigger_end):\n",
        "    emg_data = sio.loadmat(file_path)\n",
        "\n",
        "    # Extract Data and Time arrays\n",
        "    signal_data = np.array(emg_data['Data'][0, 0])\n",
        "    time_data = np.array(emg_data['Time'][0, 0]).squeeze()\n",
        "\n",
        "    # Create boolean indices for alignment\n",
        "    aligned_indices = (time_data >= trigger_start) & (time_data <= trigger_end)\n",
        "    aligned_time = time_data[aligned_indices]\n",
        "\n",
        "    # Determine how to index signal_data based on its dimensions\n",
        "    if signal_data.ndim == 1:\n",
        "        aligned_signal = signal_data[aligned_indices]\n",
        "    elif signal_data.ndim == 2:\n",
        "        # If the second dimension matches time length, index along axis 1\n",
        "        if signal_data.shape[1] == time_data.shape[0]:\n",
        "            aligned_signal = signal_data[:, aligned_indices]\n",
        "        # If the first dimension matches time length, index along axis 0\n",
        "        elif signal_data.shape[0] == time_data.shape[0]:\n",
        "            aligned_signal = signal_data[aligned_indices, :]\n",
        "        else:\n",
        "            raise ValueError(\"Signal data dimensions do not match time data length.\")\n",
        "    else:\n",
        "        raise ValueError(\"Signal data has unexpected number of dimensions.\")\n",
        "\n",
        "    # Ensure aligned_signal is 2D for consistent processing\n",
        "    if aligned_signal.ndim == 1:\n",
        "        aligned_signal = aligned_signal.reshape(-1, 1)\n",
        "\n",
        "    return aligned_time, aligned_signal\n",
        "\n",
        "#============================================================================\n",
        "# 13. PROCESS, VISUALIZE, AND SAVE ALL EMG FILES WITH ICA, INTERPOLATION & CSVs\n",
        "#============================================================================\n",
        "combined_emg_dfs = []\n",
        "# Use marker time as reference for interpolation\n",
        "ref_time = aligned_markers_data[\"Time (Seconds)\"].values\n",
        "\n",
        "for i, emg_file in enumerate(emg_files):\n",
        "    aligned_time, aligned_signal = align_emg_data(emg_file, trigger_start, trigger_end)\n",
        "\n",
        "    # ------------------------------------------------------------------------\n",
        "    # Apply StandardScaler before ICA to improve the ICA results\n",
        "    scaler_before_ica = StandardScaler()\n",
        "    aligned_signal = scaler_before_ica.fit_transform(aligned_signal)\n",
        "    # ------------------------------------------------------------------------\n",
        "\n",
        "    # Identify completely constant channels\n",
        "    channel_variances = np.var(aligned_signal, axis=0)\n",
        "    non_constant_channels = [ch for ch in range(aligned_signal.shape[1]) if channel_variances[ch] > 1e-12]\n",
        "    constant_channels = [ch for ch in range(aligned_signal.shape[1]) if ch not in non_constant_channels]\n",
        "\n",
        "    # Apply ICA on non-constant channels\n",
        "    if len(non_constant_channels) > 0:\n",
        "        try:\n",
        "            ica = FastICA(n_components=len(non_constant_channels), random_state=0)\n",
        "            sources = ica.fit_transform(aligned_signal[:, non_constant_channels])\n",
        "            reconstructed_non_constant = ica.inverse_transform(sources)\n",
        "            # Initialize reconstruction array\n",
        "            reconstructed_signal = np.zeros_like(aligned_signal)\n",
        "            # Fill reconstructed signals for non-constant channels\n",
        "            for idx, ch in enumerate(non_constant_channels):\n",
        "                reconstructed_signal[:, ch] = reconstructed_non_constant[:, idx]\n",
        "            # For constant channels, fill with average of non-constant reconstructed signals\n",
        "            if len(constant_channels) > 0:\n",
        "                avg_signal = np.mean(reconstructed_non_constant, axis=1)\n",
        "                for ch in constant_channels:\n",
        "                    reconstructed_signal[:, ch] = avg_signal\n",
        "            aligned_signal = reconstructed_signal\n",
        "\n",
        "            # ----------------------------------------------------------------\n",
        "            # Optionally reapply StandardScaler after ICA reconstruction\n",
        "            scaler_post = StandardScaler()\n",
        "            aligned_signal = scaler_post.fit_transform(aligned_signal)\n",
        "            # ----------------------------------------------------------------\n",
        "\n",
        "            print(f\"ICA applied to {emg_file} successfully.\")\n",
        "        except Exception as e:\n",
        "            print(f\"ICA failed for {emg_file} with error: {e}. Proceeding without ICA.\")\n",
        "    else:\n",
        "        print(f\"No non-constant channels for ICA in {emg_file}. Skipping ICA.\")\n",
        "\n",
        "    # Ensure no NaNs or Infs after ICA\n",
        "    aligned_signal = np.nan_to_num(aligned_signal, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "\n",
        "    channel_count = aligned_signal.shape[1]\n",
        "    # Interpolate EMG signals onto marker time grid\n",
        "    interp_emg = np.zeros((len(ref_time), channel_count))\n",
        "    for ch in range(channel_count):\n",
        "        interp_emg[:, ch] = np.interp(ref_time, aligned_time, aligned_signal[:, ch])\n",
        "\n",
        "    new_len = len(ref_time)\n",
        "    new_time = ref_time.copy()\n",
        "\n",
        "    # Prepare DataFrame for current EMG file channels\n",
        "    emg_df_dict = {\n",
        "         \"Frame\": range(new_len),\n",
        "         \"Time (Seconds)\": new_time,\n",
        "    }\n",
        "    for ch in range(channel_count):\n",
        "        emg_df_dict[f\"EMG_Channel_{i+1}_Ch{ch+1}\"] = interp_emg[:, ch]\n",
        "    emg_df = pd.DataFrame(emg_df_dict)\n",
        "\n",
        "    # Save separate CSV for each EMG file containing its channels\n",
        "    emg_csv_path = f'/content/aligned_EMG_File_{i+1}.csv'\n",
        "    emg_df.to_csv(emg_csv_path, index=False)\n",
        "    print(f\"Aligned EMG CSV saved to {emg_csv_path}\")\n",
        "\n",
        "    combined_emg_dfs.append(emg_df)\n",
        "\n",
        "    # ---- Visualization for each channel in the current EMG file ----\n",
        "    for ch in range(channel_count):\n",
        "        col_name = f\"EMG_Channel_{i+1}_Ch{ch+1}\"\n",
        "        plt.figure(figsize=(12, 6))\n",
        "        # Use interp_emg for plotting to match new_time dimensions\n",
        "        plt.plot(new_time, interp_emg[:, ch], label=col_name)\n",
        "        plt.axvline(trigger_start, color='green', linestyle='--', label='Trigger Start')\n",
        "        plt.axvline(trigger_end, color='red', linestyle='--', label='Trigger End')\n",
        "        plt.title(f'Aligned EMG Data ({col_name})')\n",
        "        plt.xlabel('Time (s)')\n",
        "        plt.ylabel('Signal Amplitude')\n",
        "        plt.legend()\n",
        "        plt.grid()\n",
        "        plt.show()\n",
        "\n",
        "    # Save aligned EMG data as .mat for the entire file (all channels)\n",
        "    aligned_emg_data = {\n",
        "        'Aligned_Time': new_time,\n",
        "        'Aligned_Signal': interp_emg\n",
        "    }\n",
        "    save_path = f'/content/aligned_EMG_File_{i + 1}.mat'\n",
        "    sio.savemat(save_path, aligned_emg_data)\n",
        "    print(f\"Aligned EMG saved to {save_path}\")\n",
        "\n",
        "#----------------------------------------------------------------------------\n",
        "# Combine all EMG files' channels into a single CSV\n",
        "#----------------------------------------------------------------------------\n",
        "if combined_emg_dfs:\n",
        "    combined_df = combined_emg_dfs[0][[\"Frame\", \"Time (Seconds)\"]].copy()\n",
        "    for df in combined_emg_dfs:\n",
        "        for col in df.columns:\n",
        "            if col not in [\"Frame\", \"Time (Seconds)\"]:\n",
        "                combined_df[col] = df[col]\n",
        "    combined_csv_path = '/content/combined_EMG_data.csv'\n",
        "    combined_df.to_csv(combined_csv_path, index=False)\n",
        "    print(f\"Combined EMG CSV saved to {combined_csv_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iHFCEycwUYFe"
      },
      "outputs": [],
      "source": [
        "# -------------------------------------------------------------------------\n",
        "# 1. READ YOUR CSV AND PREPARE FUNCTIONS FOR FEATURE CALCULATION\n",
        "# -------------------------------------------------------------------------\n",
        "input_path = '/content/combined_EMG_data.csv'\n",
        "df = pd.read_csv(input_path)\n",
        "\n",
        "# Identify columns\n",
        "frame_col = 'Frame'\n",
        "time_col = 'Time (Seconds)'\n",
        "channel_cols = [col for col in df.columns if col.startswith('EMG_Channel')]\n",
        "\n",
        "# -------------------------------------------------------------------------\n",
        "# 2. APPLY BANDPASS FILTERING TO EACH EMG CHANNEL\n",
        "# -------------------------------------------------------------------------\n",
        "# Define bandpass filter parameters\n",
        "lowcut = 20.0    # Hz\n",
        "highcut = 450.0  # Hz\n",
        "order = 4\n",
        "\n",
        "# Compute sampling frequency dynamically from time column\n",
        "time_values = df[time_col].values\n",
        "fs = 1 / np.mean(np.diff(time_values))\n",
        "\n",
        "nyq = 0.5 * fs\n",
        "low = lowcut / nyq\n",
        "high = highcut / nyq\n",
        "sos = butter(order, [low, high], btype='band', output='sos')\n",
        "\n",
        "# Apply bandpass filter to each channel\n",
        "for ch in channel_cols:\n",
        "    df[ch] = sosfilt(sos, df[ch].values)\n",
        "\n",
        "# -------------------------------------------------------------------------\n",
        "# 3. COMPUTE WINDOW SIZE DYNAMICALLY BASED ON SAMPLING RATE\n",
        "# -------------------------------------------------------------------------\n",
        "desired_window_sec = 0.1\n",
        "window_size = int(fs * desired_window_sec)\n",
        "if window_size < 1:\n",
        "    window_size = 1\n",
        "\n",
        "# -------------------------------------------------------------------------\n",
        "# 4. USE ROLLING WINDOW OPERATIONS FOR EFFICIENT FEATURE COMPUTATION\n",
        "# -------------------------------------------------------------------------\n",
        "features = {frame_col: df[frame_col], time_col: df[time_col]}\n",
        "\n",
        "for ch in tqdm(channel_cols, desc=\"Processing channels\"):\n",
        "    series = df[ch]\n",
        "    abs_series = series.abs()\n",
        "    squared_series = series.pow(2)\n",
        "    abs_diff = series.diff().abs()\n",
        "\n",
        "    # Compute rolling features\n",
        "    WL = abs_diff.rolling(window=window_size, min_periods=1).sum()\n",
        "    MAV = abs_series.rolling(window=window_size, min_periods=1).mean()\n",
        "    IAV = abs_series.rolling(window=window_size, min_periods=1).sum()\n",
        "    RMS = squared_series.rolling(window=window_size, min_periods=1).mean().pow(0.5)\n",
        "    # Avoid division by zero for AAC calculation\n",
        "    AAC = WL / (window_size - 1) if window_size > 1 else WL\n",
        "\n",
        "    # Store features in the dictionary\n",
        "    features[f'{ch}_WL'] = WL\n",
        "    features[f'{ch}_MAV'] = MAV\n",
        "    features[f'{ch}_IAV'] = IAV\n",
        "    features[f'{ch}_RMS'] = RMS\n",
        "    features[f'{ch}_AAC'] = AAC\n",
        "\n",
        "# -------------------------------------------------------------------------\n",
        "# 5. CONVERT FEATURES DICTIONARY TO DATAFRAME\n",
        "# -------------------------------------------------------------------------\n",
        "features_df = pd.DataFrame(features)\n",
        "\n",
        "# -------------------------------------------------------------------------\n",
        "# 6. APPLY MIN-MAX SCALER TO FEATURE COLUMNS\n",
        "# -------------------------------------------------------------------------\n",
        "scaler = MinMaxScaler()\n",
        "feature_columns = [col for col in features_df.columns if col not in [frame_col, time_col]]\n",
        "features_df[feature_columns] = scaler.fit_transform(features_df[feature_columns])\n",
        "\n",
        "# -------------------------------------------------------------------------\n",
        "# 7. COUNT NaN AND ZERO VALUES FOR EACH PROBE BEFORE INTERPOLATION\n",
        "# -------------------------------------------------------------------------\n",
        "probes = {1: [], 2: [], 3: [], 4: []}\n",
        "\n",
        "for col in features_df.columns:\n",
        "    if col.startswith(\"EMG_Channel_1\"):\n",
        "        probes[1].append(col)\n",
        "    elif col.startswith(\"EMG_Channel_2\"):\n",
        "        probes[2].append(col)\n",
        "    elif col.startswith(\"EMG_Channel_3\"):\n",
        "        probes[3].append(col)\n",
        "    elif col.startswith(\"EMG_Channel_4\"):\n",
        "        probes[4].append(col)\n",
        "\n",
        "nan_counts_before = {}\n",
        "zero_counts_before = {}\n",
        "\n",
        "for probe, cols in probes.items():\n",
        "    nan_counts_before[probe] = features_df[cols].isna().sum().sum()\n",
        "    zero_counts_before[probe] = (features_df[cols] == 0).sum().sum()\n",
        "\n",
        "# -------------------------------------------------------------------------\n",
        "# 8. APPLY LINEAR INTERPOLATION TO FILL NaN AND ZERO VALUES\n",
        "# -------------------------------------------------------------------------\n",
        "for col in features_df.columns:\n",
        "    if col in [frame_col, time_col]:\n",
        "        continue\n",
        "    # Convert zeros to NaNs for interpolation, then interpolate\n",
        "    features_df[col] = features_df[col].replace(0, np.nan)\n",
        "    features_df[col] = features_df[col].interpolate(method='linear', limit_direction='both')\n",
        "\n",
        "# -------------------------------------------------------------------------\n",
        "# 9. COUNT NaN AND ZERO VALUES AFTER INTERPOLATION\n",
        "# -------------------------------------------------------------------------\n",
        "nan_counts_after = {}\n",
        "zero_counts_after = {}\n",
        "\n",
        "for probe, cols in probes.items():\n",
        "    nan_counts_after[probe] = features_df[cols].isna().sum().sum()\n",
        "    zero_counts_after[probe] = (features_df[cols] == 0).sum().sum()\n",
        "\n",
        "# -------------------------------------------------------------------------\n",
        "# 10. COUNT NEGATIVE VALUES BEFORE ABSOLUTE VALUE TRANSFORMATION\n",
        "# -------------------------------------------------------------------------\n",
        "neg_counts_before = {}\n",
        "for probe, cols in probes.items():\n",
        "    neg_counts_before[probe] = (features_df[cols] < 0).sum().sum()\n",
        "\n",
        "print(\"\\n=== Negative Value Summary BEFORE ABSOLUTE VALUE TRANSFORMATION ===\\n\")\n",
        "for probe in probes:\n",
        "    print(f\"Probe {probe}: {neg_counts_before[probe]} negative values\")\n",
        "\n",
        "# -------------------------------------------------------------------------\n",
        "# 11. APPLY ABSOLUTE VALUE TRANSFORMATION TO ENSURE ALL FEATURE VALUES ARE POSITIVE\n",
        "# -------------------------------------------------------------------------\n",
        "features_df[feature_columns] = features_df[feature_columns].abs()\n",
        "\n",
        "# -------------------------------------------------------------------------\n",
        "# 12. COUNT NEGATIVE VALUES AFTER ABSOLUTE VALUE TRANSFORMATION\n",
        "# -------------------------------------------------------------------------\n",
        "neg_counts_after = {}\n",
        "for probe, cols in probes.items():\n",
        "    neg_counts_after[probe] = (features_df[cols] < 0).sum().sum()\n",
        "\n",
        "print(\"\\n=== Negative Value Summary AFTER ABSOLUTE VALUE TRANSFORMATION ===\\n\")\n",
        "for probe in probes:\n",
        "    print(f\"Probe {probe}: {neg_counts_after[probe]} negative values\")\n",
        "\n",
        "# -------------------------------------------------------------------------\n",
        "# 13. SAVE FEATURES DATAFRAME TO CSV\n",
        "# -------------------------------------------------------------------------\n",
        "output_path = '/content/emg_all_samples_features.csv'\n",
        "features_df.to_csv(output_path, index=False)\n",
        "\n",
        "# -------------------------------------------------------------------------\n",
        "# 14. PRINT NaN and ZERO VALUE SUMMARY\n",
        "# -------------------------------------------------------------------------\n",
        "print(\"\\n=== NaN and Zero Value Summary ===\\n\")\n",
        "for probe in probes:\n",
        "    print(f\"Probe {probe}:\")\n",
        "    print(f\"  NaN values before interpolation: {nan_counts_before[probe]}\")\n",
        "    print(f\"  Zero values before interpolation: {zero_counts_before[probe]}\")\n",
        "    print(f\"  NaN values after interpolation: {nan_counts_after[probe]}\")\n",
        "    print(f\"  Zero values after interpolation: {zero_counts_after[probe]}\\n\")\n",
        "\n",
        "print(f\"Per-sample channel features saved to: {output_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "od314sfTAEby"
      },
      "outputs": [],
      "source": [
        "# -------------------------------\n",
        "# Setup logging\n",
        "# -------------------------------\n",
        "logging.basicConfig(level=logging.INFO,\n",
        "                    format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "\n",
        "# -------------------------------\n",
        "# Define STRIDE intervals (no transition intervals)\n",
        "# -------------------------------\n",
        "STRIDE_INTERVALS = [\n",
        "    (0, 5937), (5938, 9521), (15938, 18914),\n",
        "    (25059, 28499), (28500, 31683), (35059, 38099),\n",
        "    (38100, 41203), (47684, 50606), (56885, 59990),\n",
        "    (66006, 69046), (75526, 79271), (85384, 88231),\n",
        "    (88232, 91271), (96936, 99848), (99849, 105864),\n",
        "    (105865, 108570), (115114, 118026), (118027, 121067),\n",
        "    (127611, 130587), (135852, 138698), (138699, 141516),\n",
        "    (147147, 150252), (150253, 152508), (152509, 155726)\n",
        "]\n",
        "\n",
        "def label_frames(frame_array):\n",
        "    \"\"\"\n",
        "    Label frames based on predefined stride intervals.\n",
        "    Frames within any stride interval are labeled as 1,\n",
        "    and frames outside these intervals (including beyond 155726) are labeled as 3.\n",
        "    \"\"\"\n",
        "    labels = np.full(frame_array.shape, 3)\n",
        "    for start, end in STRIDE_INTERVALS:\n",
        "        condition = (frame_array >= start) & (frame_array <= end)\n",
        "        labels[condition] = 1\n",
        "    return labels\n",
        "\n",
        "# --------------------------------------------------\n",
        "# Compute Explained Variance\n",
        "# --------------------------------------------------\n",
        "def compute_explained_variance(X_full, X_reconstructed):\n",
        "    total_variance = np.sum((X_full - np.mean(X_full, axis=0)) ** 2)\n",
        "    residual_variance = np.sum((X_full - X_reconstructed) ** 2)\n",
        "    explained_variance_ratio = 1 - residual_variance / total_variance\n",
        "    return explained_variance_ratio * 100\n",
        "\n",
        "# --------------------------------------------------\n",
        "# Optional Post-processing: Spline Interpolation\n",
        "# --------------------------------------------------\n",
        "def post_process_activations(activations):\n",
        "    processed = activations.copy()\n",
        "    n_frames = processed.shape[0]\n",
        "    x_full = np.arange(n_frames)\n",
        "    for j in range(processed.shape[1]):\n",
        "        col = processed[:, j]\n",
        "        mask = np.isnan(col) | (col == 0)\n",
        "        if np.sum(~mask) < 2:\n",
        "            logging.warning(f\"Not enough valid data for spline interpolation in synergy {j}.\")\n",
        "            continue\n",
        "        interp_func = interp1d(x_full[~mask], col[~mask], kind='cubic', fill_value=\"extrapolate\")\n",
        "        col[mask] = interp_func(x_full[mask])\n",
        "        processed[:, j] = col\n",
        "    return np.abs(processed)\n",
        "\n",
        "# --------------------------------------------------\n",
        "# GPU-based NMF using Multiplicative Updates (PyTorch)\n",
        "# --------------------------------------------------\n",
        "def torch_nmf(X, n_synergies=5, max_iter=20000, tol=1e-4, device='cuda'):\n",
        "    eps = 1e-8\n",
        "    n_frames, n_features = X.shape\n",
        "    W = torch.rand((n_frames, n_synergies), device=device)\n",
        "    H = torch.rand((n_synergies, n_features), device=device)\n",
        "    for i in range(max_iter):\n",
        "        # Update H\n",
        "        numerator = torch.mm(W.t(), X)\n",
        "        denominator = torch.mm(torch.mm(W.t(), W), H) + eps\n",
        "        H = H * (numerator / denominator)\n",
        "        # Update W\n",
        "        numerator = torch.mm(X, H.t())\n",
        "        denominator = torch.mm(W, torch.mm(H, H.t())) + eps\n",
        "        W = W * (numerator / denominator)\n",
        "        if i % 50 == 0:\n",
        "            X_approx = torch.mm(W, H)\n",
        "            error = torch.norm(X - X_approx, p='fro')\n",
        "            logging.info(f\"NMF iter {i}, error: {error.item():.6f}\")\n",
        "            if error < tol:\n",
        "                logging.info(f\"NMF converged at iter {i} with error {error.item():.4f}\")\n",
        "                break\n",
        "    return W, H\n",
        "\n",
        "# --------------------------------------------------\n",
        "# Enhanced Neural Network Mapping from Reduced EMG to Synergy Activations\n",
        "# --------------------------------------------------\n",
        "class EnhancedMappingNet(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super(EnhancedMappingNet, self).__init__()\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(input_dim, 128),\n",
        "            nn.BatchNorm1d(128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(128, 64),\n",
        "            nn.BatchNorm1d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(64, output_dim),\n",
        "            nn.ReLU()  # Ensure nonnegative outputs\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc(x)\n",
        "\n",
        "def train_enhanced_mapping_net(X_reduced_np, S_full_np, max_epochs=20000, lr=1e-3, device='cuda'):\n",
        "    # Convert numpy arrays to torch tensors\n",
        "    X_reduced = torch.from_numpy(X_reduced_np).float().to(device)\n",
        "    S_full = torch.from_numpy(S_full_np).float().to(device)\n",
        "    input_dim = X_reduced.shape[1]\n",
        "    output_dim = S_full.shape[1]\n",
        "\n",
        "    # Initialize the enhanced mapping network\n",
        "    model = EnhancedMappingNet(input_dim, output_dim).to(device)\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "    # Scheduler to reduce learning rate if loss plateaus\n",
        "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=1000, verbose=True)\n",
        "\n",
        "    for epoch in range(max_epochs):\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "        S_pred = model(X_reduced)\n",
        "        loss = criterion(S_pred, S_full)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step(loss)\n",
        "        if epoch % 100 == 0:\n",
        "            logging.info(f\"EnhancedMappingNet Epoch {epoch}, Loss: {loss.item():.6f}\")\n",
        "    return model\n",
        "\n",
        "# --------------------------------------------------\n",
        "# Channel Selection Methods\n",
        "# --------------------------------------------------\n",
        "def anatomical_selection(block3_indices):\n",
        "    \"\"\"\n",
        "    Method 1: Anatomical mapping.\n",
        "    \"\"\"\n",
        "    # Hypothetical predetermined VL channels (adjust based on your anatomical mapping)\n",
        "    anatomical_indices = [330, 331, 332, 333, 334, 335, 336, 337, 338, 339,340,341,342,343,344]\n",
        "    return anatomical_indices\n",
        "\n",
        "def variance_based_selection(X_full, block3_indices, top_k=10):\n",
        "    \"\"\"\n",
        "    Method 2: Preliminary analysis based on variance.\n",
        "    Computes the variance of each channel (from block3_indices) and selects the top_k channels.\n",
        "    \"\"\"\n",
        "    variances = np.var(X_full[:, block3_indices], axis=0)\n",
        "    # Get indices of top_k channels (relative to block3)\n",
        "    top_indices_local = np.argsort(variances)[-top_k:]\n",
        "    # Map local indices back to absolute indices in X_full.\n",
        "    selected_indices = [block3_indices[i] for i in top_indices_local]\n",
        "    return selected_indices\n",
        "\n",
        "def l1_feature_selection(X_reduced, synergy_activations, top_k=10, device='cuda'):\n",
        "    \"\"\"\n",
        "    Method 3: Feature selection using a simple linear model with L1 regularization.\n",
        "    Trains a linear mapping from the full block3 data (X_reduced) to synergy activations,\n",
        "    then selects channels with the highest average absolute weight values.\n",
        "    \"\"\"\n",
        "    n_samples, n_features = X_reduced.shape\n",
        "    n_outputs = synergy_activations.shape[1]\n",
        "    model = nn.Linear(n_features, n_outputs).to(device)\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "    X_tensor = torch.from_numpy(X_reduced).float().to(device)\n",
        "    Y_tensor = torch.from_numpy(synergy_activations).float().to(device)\n",
        "    epochs = 500\n",
        "    l1_lambda = 1e-3\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(X_tensor)\n",
        "        loss = criterion(outputs, Y_tensor)\n",
        "        # Add L1 regularization on weights\n",
        "        l1_norm = sum(p.abs().sum() for p in model.parameters())\n",
        "        loss = loss + l1_lambda * l1_norm\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    # After training, get the weights from the linear layer.\n",
        "    with torch.no_grad():\n",
        "        weights = model.weight.cpu().numpy()  # shape: (n_outputs, n_features)\n",
        "    # Average absolute weight across outputs for each channel.\n",
        "    avg_abs_weights = np.mean(np.abs(weights), axis=0)  # shape: (n_features,)\n",
        "    # Select the top_k channels (indices relative to the block3 subset)\n",
        "    top_indices_local = np.argsort(avg_abs_weights)[-top_k:]\n",
        "    return top_indices_local  # these are relative indices within X_reduced\n",
        "\n",
        "# --------------------------------------------------\n",
        "# Main Function integrating the full pipeline and channel selection methods\n",
        "# --------------------------------------------------\n",
        "def main():\n",
        "    # -------------------------------\n",
        "    # Step 1: Load EMG data (adjust path as needed)\n",
        "    # -------------------------------\n",
        "    emg_file = \"/content/emg_all_samples_features(3).csv\"\n",
        "    if not os.path.exists(emg_file):\n",
        "        logging.error(f\"EMG file not found at {emg_file}\")\n",
        "        return\n",
        "    try:\n",
        "        emg_df = pd.read_csv(emg_file)\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error loading file: {e}\")\n",
        "        return\n",
        "    logging.info(f\"EMG data loaded with shape: {emg_df.shape}\")\n",
        "\n",
        "    # -------------------------------\n",
        "    # Step 2: Ensure required columns exist and add if needed\n",
        "    # -------------------------------\n",
        "    if \"Frame\" not in emg_df.columns:\n",
        "        emg_df.insert(0, \"Frame\", np.arange(len(emg_df)))\n",
        "    if \"Time (Seconds)\" not in emg_df.columns:\n",
        "        logging.error(\"Missing 'Time (Seconds)' column in EMG data.\")\n",
        "        return\n",
        "\n",
        "    # -------------------------------\n",
        "    # Step 3: Label Frames based on stride intervals\n",
        "    # -------------------------------\n",
        "    frame_indices = emg_df[\"Frame\"].values\n",
        "    emg_df[\"Label\"] = label_frames(frame_indices)\n",
        "    logging.info(\"Frame labeling complete.\")\n",
        "    logging.info(f\"Label distribution: {emg_df['Label'].value_counts().to_dict()}\")\n",
        "\n",
        "    # -------------------------------\n",
        "    # Step 3.5: Filter to only include frames within stride intervals (Label 1)\n",
        "    # -------------------------------\n",
        "    emg_df = emg_df[emg_df[\"Label\"] == 1]\n",
        "    logging.info(f\"Filtered EMG data shape (only stride intervals): {emg_df.shape}\")\n",
        "\n",
        "    # -------------------------------\n",
        "    # Step 4: Extract EMG Data from Blocks\n",
        "    # -------------------------------\n",
        "    # Assuming the first two columns are Frame and Time, and the last column is Label.\n",
        "    # The remaining columns are the EMG features.\n",
        "    emg_data = emg_df.iloc[:, 2:-1]\n",
        "    n_frames, n_features = emg_data.shape\n",
        "    logging.info(f\"Extracted EMG data with shape: {emg_data.shape}\")\n",
        "\n",
        "    # Rename columns for clarity and fill missing data\n",
        "    emg_data.columns = [f\"EMG_{i+1}\" for i in range(n_features)]\n",
        "    emg_data = emg_data.fillna(0)\n",
        "\n",
        "    # Rectify EMG signals and convert to float32\n",
        "    X_full = np.abs(emg_data.values).astype(np.float32)\n",
        "    logging.info(f\"Rectified full EMG data shape: {X_full.shape}\")\n",
        "\n",
        "    # -------------------------------\n",
        "    # Step 5: GPU-accelerated NMF on Full EMG\n",
        "    # -------------------------------\n",
        "    n_synergies = 5  # Typical for gait analysis\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    logging.info(f\"Using device: {device}\")\n",
        "    X_full_torch = torch.from_numpy(X_full).to(device)\n",
        "    W_torch, H_torch = torch_nmf(X_full_torch, n_synergies=n_synergies, max_iter=500, device=device)\n",
        "    synergy_activations = W_torch.cpu().numpy()   # Shape: (n_frames, n_synergies)\n",
        "    synergy_weights = H_torch.cpu().numpy()         # Shape: (n_synergies, n_features)\n",
        "    logging.info(f\"Extracted synergy activations shape: {synergy_activations.shape}\")\n",
        "    logging.info(f\"Extracted synergy weights shape: {synergy_weights.shape}\")\n",
        "\n",
        "    # -------------------------------\n",
        "    # Step 6: Compute Explained Variance on Full Data\n",
        "    # -------------------------------\n",
        "    X_reconstructed = synergy_activations.dot(synergy_weights)\n",
        "    explained_percentage = compute_explained_variance(X_full, X_reconstructed)\n",
        "    logging.info(f\"Explained Variance by {n_synergies} synergies: {explained_percentage:.2f}%\")\n",
        "    print(f\"Explained Variance by {n_synergies} synergies: {explained_percentage:.2f}%\")\n",
        "\n",
        "    # -------------------------------\n",
        "    # Step 7: Post-process Synergy Activations (Optional)\n",
        "    # -------------------------------\n",
        "    synergy_activations_fixed = post_process_activations(synergy_activations)\n",
        "\n",
        "    # Save synergy activations if needed\n",
        "    synergy_df = pd.DataFrame(synergy_activations_fixed,\n",
        "                              columns=[f\"Synergy_{i+1}\" for i in range(n_synergies)])\n",
        "    synergy_df[\"Frame\"] = emg_df[\"Frame\"].values\n",
        "    synergy_df[\"Time (Seconds)\"] = emg_df[\"Time (Seconds)\"].values\n",
        "    synergy_df[\"Label\"] = emg_df[\"Label\"].values\n",
        "    synergy_df.to_csv(\"muscle_synergy_activations.csv\", index=False)\n",
        "    logging.info(\"Saved synergy activations to muscle_synergy_activations.csv\")\n",
        "\n",
        "    # -------------------------------\n",
        "    # Step 8: Define Block 3 indices\n",
        "    # -------------------------------\n",
        "    # For example, assume Block 3 corresponds to columns 320 to 479 in X_full.\n",
        "    block3_indices = list(range(320, 480))\n",
        "    X_block3 = X_full[:, block3_indices]\n",
        "\n",
        "    # -------------------------------\n",
        "    # Step 9: Channel Selection Methods\n",
        "    # -------------------------------\n",
        "    # Method 1: Anatomical mapping (predetermined channels)\n",
        "    anatomical_indices = anatomical_selection(block3_indices)\n",
        "    method1_indices = anatomical_indices  # absolute indices\n",
        "    logging.info(f\"Method 1 (Anatomical) selected channels: {method1_indices}\")\n",
        "\n",
        "    # Method 2: Variance-based selection (top 10 channels by variance)\n",
        "    method2_indices = variance_based_selection(X_full, block3_indices, top_k=10)\n",
        "    logging.info(f\"Method 2 (Variance-based) selected channels: {method2_indices}\")\n",
        "\n",
        "    # Method 3: L1 feature selection (using linear model with L1 regularization)\n",
        "    # Train on all channels in Block 3 first:\n",
        "    top_local_indices = l1_feature_selection(X_block3, synergy_activations_fixed, top_k=10, device=device)\n",
        "    # Convert relative indices (within block 3) to absolute indices in X_full:\n",
        "    method3_indices = [block3_indices[i] for i in top_local_indices]\n",
        "    logging.info(f\"Method 3 (L1 feature selection) selected channels: {method3_indices}\")\n",
        "\n",
        "    # -------------------------------\n",
        "    # Step 10: Train and Evaluate Enhanced Mapping for Each Method\n",
        "    # -------------------------------\n",
        "    methods = {\n",
        "        \"Anatomical\": method1_indices,\n",
        "        \"Variance\": method2_indices,\n",
        "        \"L1\": method3_indices\n",
        "    }\n",
        "    results = {}\n",
        "\n",
        "    for method_name, indices in methods.items():\n",
        "        logging.info(f\"Training EnhancedMappingNet using {method_name} selection with channels: {indices}\")\n",
        "        # Extract reduced EMG data using the selected channels.\n",
        "        X_reduced_method = X_full[:, indices]\n",
        "        # Train the enhanced mapping network.\n",
        "        model = train_enhanced_mapping_net(X_reduced_method, synergy_activations_fixed,\n",
        "                                           max_epochs=20000, lr=1e-3, device=device)\n",
        "        # Evaluate performance:\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            X_reduced_tensor = torch.from_numpy(X_reduced_method).float().to(device)\n",
        "            S_pred_nn = model(X_reduced_tensor).cpu().numpy()\n",
        "        X_reconstructed_nn = S_pred_nn.dot(synergy_weights)\n",
        "        explained_percentage_nn = compute_explained_variance(X_full, X_reconstructed_nn)\n",
        "        logging.info(f\"{method_name} selection - Explained Variance (full EMG from reduced set): {explained_percentage_nn:.2f}%\")\n",
        "        print(f\"{method_name} selection - Explained Variance (full EMG from reduced set): {explained_percentage_nn:.2f}%\")\n",
        "        results[method_name] = {\n",
        "            \"explained_variance\": explained_percentage_nn,\n",
        "            \"selected_indices\": indices,\n",
        "            \"model\": model,\n",
        "            \"X_reduced\": X_reduced_method,\n",
        "            \"S_pred_nn\": S_pred_nn\n",
        "        }\n",
        "\n",
        "    # -------------------------------\n",
        "    # Step 11: Select Best Method and Save Reconstructed EMG\n",
        "    # -------------------------------\n",
        "    best_method = max(results, key=lambda k: results[k][\"explained_variance\"])\n",
        "    best_result = results[best_method]\n",
        "    logging.info(f\"Best method: {best_method} with explained variance {best_result['explained_variance']:.2f}%\")\n",
        "    print(f\"Best method: {best_method} with explained variance {best_result['explained_variance']:.2f}%\")\n",
        "\n",
        "    df_reconstructed = pd.DataFrame(best_result[\"S_pred_nn\"].dot(synergy_weights),\n",
        "                                    columns=[f\"EMG_{i+1}\" for i in range(n_features)])\n",
        "    df_reconstructed[\"Frame\"] = emg_df[\"Frame\"].values\n",
        "    df_reconstructed.to_csv(\"reconstructed_full_emg_from_best_method.csv\", index=False)\n",
        "    logging.info(\"Saved reconstructed EMG to reconstructed_full_emg_from_best_method.csv\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tc6d_PBOUpvM"
      },
      "outputs": [],
      "source": [
        "# --------------------------------------------------\n",
        "# GLOBAL INTERVALS and CONSTANTS\n",
        "# --------------------------------------------------\n",
        "STRIDE_INTERVALS = [\n",
        "    (0, 5937), (5938, 9521), (15938, 18914),\n",
        "    (25059, 28499), (28500, 31683), (35059, 38099),\n",
        "    (38100, 41203), (47684, 50606), (56885, 59990),\n",
        "    (66006, 69046), (75526, 79271), (85384, 88231),\n",
        "    (88232, 91271), (96936, 99848), (99849, 105864),\n",
        "    (105865, 108570), (115114, 118026), (118027, 121067),\n",
        "    (127611, 130587), (135852, 138698), (138699, 141516),\n",
        "    (147147, 150252), (150253, 152508), (152509, 155726)\n",
        "]\n",
        "THRESHOLD_FRAME = 155727  # Frames >= this value are outside our stride intervals\n",
        "\n",
        "DESIRED_DISTRIBUTION = np.array([0.10, 0.40, 0.10, 0.20, 0.20])\n",
        "LAMBDA_PENALTY = 100\n",
        "\n",
        "# --------------------------------------------------\n",
        "# SETUP LOGGING\n",
        "# --------------------------------------------------\n",
        "logging.basicConfig(\n",
        "    level=logging.DEBUG,\n",
        "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
        "    handlers=[logging.StreamHandler(sys.stdout)],\n",
        "    force=True\n",
        ")\n",
        "\n",
        "def step_log(step_number, message):\n",
        "    logging.info(f\"Step {step_number}: {message}\")\n",
        "\n",
        "# --------------------------------------------------\n",
        "# HELPER FUNCTIONS\n",
        "# --------------------------------------------------\n",
        "def label_frames_custom(frame_indices, stride_intervals, threshold_frame):\n",
        "    labels = np.zeros_like(frame_indices)\n",
        "    for start, end in stride_intervals:\n",
        "        if start < threshold_frame:\n",
        "            actual_end = min(end, threshold_frame - 1)\n",
        "            labels[(frame_indices >= start) & (frame_indices <= actual_end)] = 1\n",
        "    return labels\n",
        "\n",
        "def enforce_symmetry(cov, min_covar=1e-3):\n",
        "    cov_sym = 0.5 * (cov + cov.T)\n",
        "    eigvals, eigvecs = np.linalg.eigh(cov_sym)\n",
        "    eigvals[eigvals < min_covar] = min_covar\n",
        "    cov_pd = eigvecs @ np.diag(eigvals) @ eigvecs.T\n",
        "    return cov_pd\n",
        "\n",
        "def stationary_distribution(transmat):\n",
        "    eigvals, eigvecs = np.linalg.eig(transmat.T)\n",
        "    idx = np.argmin(np.abs(eigvals - 1))\n",
        "    stat = np.real(eigvecs[:, idx])\n",
        "    stat = stat / np.sum(stat)\n",
        "    return stat\n",
        "\n",
        "# --------------------------------------------------\n",
        "# DATA LOADING FUNCTIONS\n",
        "# --------------------------------------------------\n",
        "def load_marker_data(file_path):\n",
        "    logging.debug(\"Loading gait marker CSV data.\")\n",
        "    if not os.path.exists(file_path):\n",
        "        logging.error(f\"File not found: {file_path}\")\n",
        "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
        "    df = pd.read_csv(file_path)\n",
        "    df.columns = df.columns.str.strip()\n",
        "    if 'Frame' not in df.columns:\n",
        "        df.insert(0, 'Frame', np.arange(len(df)))\n",
        "    if 'Time (Seconds)' not in df.columns:\n",
        "        raise KeyError(\"'Time (Seconds)' column is missing.\")\n",
        "    logging.info(f\"Gait marker data loaded with shape {df.shape}\")\n",
        "    return df\n",
        "\n",
        "def load_synergy_data(file_path):\n",
        "    logging.debug(\"Loading muscle synergy CSV data.\")\n",
        "    if not os.path.exists(file_path):\n",
        "        logging.error(f\"File not found: {file_path}\")\n",
        "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
        "    df = pd.read_csv(file_path, sep=None, engine='python')\n",
        "    df.columns = df.columns.str.strip()\n",
        "    required = ['Synergy_1', 'Synergy_2', 'Synergy_3', 'Synergy_4', 'Synergy_5', 'Frame', 'Time (Seconds)']\n",
        "    for col in required:\n",
        "        if col not in df.columns:\n",
        "            raise KeyError(f\"Required column '{col}' missing in synergy data.\")\n",
        "    logging.info(f\"Muscle synergy data loaded with shape {df.shape}\")\n",
        "    return df\n",
        "\n",
        "# --------------------------------------------------\n",
        "# GAIT DATA PROCESSING FUNCTIONS (Markers)\n",
        "# --------------------------------------------------\n",
        "def compute_velocities(df, marker_prefixes):\n",
        "    logging.debug(\"Computing velocities.\")\n",
        "    df = df.sort_values('Frame').reset_index(drop=True)\n",
        "    df['dt'] = df['Time (Seconds)'].diff().fillna(0).replace(0, 1e-6)\n",
        "    for prefix in marker_prefixes:\n",
        "        for axis in ['X', 'Y', 'Z']:\n",
        "            pos_col = f\"{prefix}_{axis}\"\n",
        "            vel_col = f\"{prefix}_V{axis}\"\n",
        "            if pos_col not in df.columns:\n",
        "                df[vel_col] = 0\n",
        "            else:\n",
        "                df[vel_col] = df[pos_col].diff() / df['dt']\n",
        "                df[vel_col] = df[vel_col].fillna(0)\n",
        "    df.drop(columns=['dt'], inplace=True)\n",
        "    return df\n",
        "\n",
        "def compute_accelerations(df, marker_prefixes):\n",
        "    logging.debug(\"Computing accelerations.\")\n",
        "    df['dt_acc'] = df['Time (Seconds)'].diff().fillna(0).replace(0, 1e-6)\n",
        "    for prefix in marker_prefixes:\n",
        "        for axis in ['X', 'Y', 'Z']:\n",
        "            vel_col = f\"{prefix}_V{axis}\"\n",
        "            acc_col = f\"{prefix}_A{axis}\"\n",
        "            if vel_col not in df.columns:\n",
        "                df[acc_col] = 0\n",
        "            else:\n",
        "                df[acc_col] = df[vel_col].diff() / df['dt_acc']\n",
        "                df[acc_col] = df[acc_col].fillna(0)\n",
        "    df.drop(columns=['dt_acc'], inplace=True)\n",
        "    return df\n",
        "\n",
        "def compute_angular_features(df):\n",
        "    logging.debug(\"Computing angular features.\")\n",
        "    def compute_angle(p1, p2, p3):\n",
        "        v1 = p1 - p2\n",
        "        v2 = p3 - p2\n",
        "        dot_product = np.einsum('ij,ij->i', v1, v2)\n",
        "        norms = np.linalg.norm(v1, axis=1) * np.linalg.norm(v2, axis=1) + 1e-8\n",
        "        angles = np.arccos(np.clip(dot_product / norms, -1.0, 1.0))\n",
        "        return np.degrees(angles)\n",
        "    angle_defs = {\n",
        "        'R_Knee_Angle': ('RGT', 'RLE', 'RLM'),\n",
        "        'L_Knee_Angle': ('LGT', 'LLE', 'LLM')\n",
        "    }\n",
        "    for angle_name, (p1, p2, p3) in angle_defs.items():\n",
        "        req_cols = [f\"{m}_{axis}\" for m in [p1, p2, p3] for axis in ['X', 'Y', 'Z']]\n",
        "        if not all(col in df.columns for col in req_cols):\n",
        "            df[angle_name] = 0\n",
        "        else:\n",
        "            p1_coords = df[[f\"{p1}_X\", f\"{p1}_Y\", f\"{p1}_Z\"]].values\n",
        "            p2_coords = df[[f\"{p2}_X\", f\"{p2}_Y\", f\"{p2}_Z\"]].values\n",
        "            p3_coords = df[[f\"{p3}_X\", f\"{p3}_Y\", f\"{p3}_Z\"]].values\n",
        "            df[angle_name] = compute_angle(p1_coords, p2_coords, p3_coords)\n",
        "    return df\n",
        "\n",
        "def compute_angular_momentum(df, marker_prefixes):\n",
        "    logging.debug(\"Computing angular momentum.\")\n",
        "    for marker in marker_prefixes:\n",
        "        pos_cols = [f\"{marker}_X\", f\"{marker}_Y\", f\"{marker}_Z\"]\n",
        "        vel_cols = [f\"{marker}_V{axis}\" for axis in ['X', 'Y', 'Z']]\n",
        "        target_cols = [f\"{marker}_Angular_Momentum_{axis}\" for axis in ['X', 'Y', 'Z']]\n",
        "        if not all(col in df.columns for col in pos_cols + vel_cols):\n",
        "            for col in target_cols:\n",
        "                df[col] = 0\n",
        "            continue\n",
        "        pos = df[pos_cols].values\n",
        "        vel = df[vel_cols].values\n",
        "        omega = np.cross(pos, vel)\n",
        "        df[target_cols] = omega\n",
        "    return df\n",
        "\n",
        "def butter_lowpass_filter_multi(data, cutoff=5, fs=2000, order=6):\n",
        "    nyq = 0.5 * fs\n",
        "    normal_cutoff = cutoff / nyq\n",
        "    b, a = butter(order, normal_cutoff, btype='low', analog=False)\n",
        "    logging.debug(\"Filtering data with shape: %s\", data.shape)\n",
        "    if data.shape[0] <= 21:\n",
        "        logging.error(\"Input data length (%d) is too short for filtering.\", data.shape[0])\n",
        "    return filtfilt(b, a, data, axis=0)\n",
        "\n",
        "def extract_features(df, window_size, feature_combination, marker_prefixes):\n",
        "    logging.debug(f\"Extracting gait features with window_size={window_size} and combination={feature_combination}\")\n",
        "    expected = []\n",
        "    for m in marker_prefixes:\n",
        "        for ax in ['X', 'Y', 'Z']:\n",
        "            expected.extend([f'{m}_{ax}', f'{m}_V{ax}', f'{m}_A{ax}'])\n",
        "    stat_types = [st for st in ['var', 'poly'] if st in feature_combination]\n",
        "    for st in stat_types:\n",
        "        for m in marker_prefixes:\n",
        "            for ax in ['X', 'Y', 'Z']:\n",
        "                expected.extend([f'{m}_{ax}_{st}', f'{m}_V{ax}_{st}', f'{m}_A{ax}_{st}'])\n",
        "    distance_pairs = [\n",
        "        ('RLE', 'LLE'), ('RGT', 'LGT'),\n",
        "        ('RLE', 'RLM'), ('LLE', 'LLM'),\n",
        "        ('RGT', 'RLE'), ('LGT', 'LLE')\n",
        "    ]\n",
        "    for pair in distance_pairs:\n",
        "        expected.append(f'{pair[0]}_{pair[1]}_dist')\n",
        "    for m in marker_prefixes:\n",
        "        expected.append(f'{m}_A_mag')\n",
        "        for axis in ['X', 'Y', 'Z']:\n",
        "            expected.append(f'{m}_Angular_Momentum_{axis}')\n",
        "    feats = pd.DataFrame(index=df.index, columns=expected, dtype=float)\n",
        "    for col in expected:\n",
        "        if col in df.columns:\n",
        "            feats[col] = df[col].values\n",
        "    for st in stat_types:\n",
        "        for m in marker_prefixes:\n",
        "            for ax in ['X', 'Y', 'Z']:\n",
        "                col_base = f'{m}_{ax}'\n",
        "                if col_base in df.columns:\n",
        "                    if st == 'var':\n",
        "                        feats[f'{col_base}_var'] = df[col_base].rolling(window_size, min_periods=1).var().fillna(0)\n",
        "                    elif st == 'poly':\n",
        "                        feats[f'{col_base}_poly'] = df[col_base].rolling(window_size, min_periods=3).apply(\n",
        "                            lambda x: np.polyfit(np.arange(len(x)), x, 2)[-1] if len(x) >= 3 else 0, raw=True\n",
        "                        ).fillna(0)\n",
        "                for prefix in [f'{m}_V{ax}', f'{m}_A{ax}']:\n",
        "                    if prefix in df.columns:\n",
        "                        if st == 'var':\n",
        "                            feats[f'{prefix}_var'] = df[prefix].rolling(window_size, min_periods=1).var().fillna(0)\n",
        "                        elif st == 'poly':\n",
        "                            feats[f'{prefix}_poly'] = df[prefix].rolling(window_size, min_periods=3).apply(\n",
        "                                lambda x: np.polyfit(np.arange(len(x)), x, 2)[-1] if len(x) >= 3 else 0, raw=True\n",
        "                            ).fillna(0)\n",
        "    for pair in distance_pairs:\n",
        "        m1, m2 = pair\n",
        "        dist_col = f'{m1}_{m2}_dist'\n",
        "        if all(col in df.columns for col in [f'{m1}_X', f'{m1}_Y', f'{m1}_Z', f'{m2}_X', f'{m2}_Y', f'{m2}_Z']):\n",
        "            feats[dist_col] = np.linalg.norm(\n",
        "                df[[f'{m1}_X', f'{m1}_Y', f'{m1}_Z']].values -\n",
        "                df[[f'{m2}_X', f'{m2}_Y', f'{m2}_Z']].values, axis=1\n",
        "            )\n",
        "        else:\n",
        "            feats[dist_col] = 0\n",
        "    for m in marker_prefixes:\n",
        "        acc_cols = [f'{m}_A{ax}' for ax in ['X', 'Y', 'Z']]\n",
        "        if all(col in df.columns for col in acc_cols):\n",
        "            feats[f'{m}_A_mag'] = np.linalg.norm(df[acc_cols].values, axis=1)\n",
        "        else:\n",
        "            feats[f'{m}_A_mag'] = 0\n",
        "    feats.fillna(0, inplace=True)\n",
        "    logging.debug(f\"Extracted gait features shape: {feats.shape}\")\n",
        "    return feats\n",
        "\n",
        "def extract_synergy_features(df, window_size):\n",
        "    logging.debug(\"Extracting EMG synergy features.\")\n",
        "    synergy_cols = [f\"Synergy_{i}\" for i in range(1, 6)]\n",
        "    feats = df[synergy_cols].copy()\n",
        "    for col in synergy_cols:\n",
        "        feats[f\"{col}_var\"] = df[col].rolling(window_size, min_periods=1).var().fillna(0)\n",
        "        feats[f\"{col}_poly\"] = df[col].rolling(window_size, min_periods=3).apply(\n",
        "            lambda x: np.polyfit(np.arange(len(x)), x, 2)[-1] if len(x) >= 3 else 0, raw=True\n",
        "        ).fillna(0)\n",
        "    feats.fillna(0, inplace=True)\n",
        "    logging.debug(f\"Extracted EMG synergy features shape: {feats.shape}\")\n",
        "    return feats\n",
        "\n",
        "# --------------------------------------------------\n",
        "# HMM TRAINING FUNCTIONS\n",
        "# --------------------------------------------------\n",
        "def train_markers_hmm(X, n_components=5, covariance_type='full', n_iter=10, *,\n",
        "                      alpha, beta, gamma, theta, delta):\n",
        "    logging.info(\"Training markers HMM with 5 states using gait marker features.\")\n",
        "    markers_model = hmm.GaussianHMM(\n",
        "        n_components=n_components,\n",
        "        covariance_type=covariance_type,\n",
        "        n_iter=n_iter,\n",
        "        random_state=42,\n",
        "        init_params=\"mc\",\n",
        "        min_covar=1e-3\n",
        "    )\n",
        "    markers_model.startprob_ = np.zeros(n_components)\n",
        "    markers_model.startprob_[0] = 1.0\n",
        "    transmat = np.array([\n",
        "         [alpha,       1 - alpha, 0.0,      0.0,       0.0],\n",
        "         [0.0,         beta,      1 - beta, 0.0,       0.0],\n",
        "         [0.0,         0.0,       gamma,    1 - gamma,  0.0],\n",
        "         [0.0,         0.0,       0.0,      delta,      1 - delta],\n",
        "         [1 - theta,   0.0,       0.0,      0.0,        theta]\n",
        "    ])\n",
        "    markers_model.transmat_ = transmat\n",
        "    markers_model.fit(X)\n",
        "    if covariance_type == 'full':\n",
        "        markers_model.covars_ = np.array([enforce_symmetry(cov) for cov in markers_model.covars_])\n",
        "    if np.isnan(markers_model.means_).any():\n",
        "        global_mean = np.mean(X, axis=0)\n",
        "        markers_model.means_ = np.where(np.isnan(markers_model.means_), global_mean, markers_model.means_)\n",
        "    return markers_model\n",
        "\n",
        "def train_synergy_hmm(X, n_components=5, covariance_type='full', n_iter=10, *,\n",
        "                      alpha, beta, gamma, theta, delta):\n",
        "    logging.info(\"Training synergy HMM with 5 states using EMG synergy features.\")\n",
        "    synergy_model = hmm.GaussianHMM(\n",
        "        n_components=n_components,\n",
        "        covariance_type=covariance_type,\n",
        "        n_iter=n_iter,\n",
        "        random_state=42,\n",
        "        init_params=\"mc\",\n",
        "        min_covar=1e-3\n",
        "    )\n",
        "    synergy_model.startprob_ = np.zeros(n_components)\n",
        "    synergy_model.startprob_[0] = 1.0\n",
        "    transmat = np.array([\n",
        "         [alpha,       1 - alpha, 0.0,      0.0,       0.0],\n",
        "         [0.0,         beta,      1 - beta, 0.0,       0.0],\n",
        "         [0.0,         0.0,       gamma,    1 - gamma,  0.0],\n",
        "         [0.0,         0.0,       0.0,      delta,      1 - delta],\n",
        "         [1 - theta,   0.0,       0.0,      0.0,        theta]\n",
        "    ])\n",
        "    synergy_model.transmat_ = transmat\n",
        "    synergy_model.fit(X)\n",
        "    if covariance_type == 'full':\n",
        "        synergy_model.covars_ = np.array([enforce_symmetry(cov) for cov in synergy_model.covars_])\n",
        "    if np.isnan(synergy_model.means_).any():\n",
        "        global_mean = np.mean(X, axis=0)\n",
        "        synergy_model.means_ = np.where(np.isnan(synergy_model.means_), global_mean, synergy_model.means_)\n",
        "    return synergy_model\n",
        "\n",
        "def fuse_hmm_predictions(model_markers, model_synergy, X_markers, X_synergy,\n",
        "                         weight_markers=0.0001, weight_synergy=0.9999):\n",
        "    logging.info(\"Fusing HMM predictions from markers and synergy models.\")\n",
        "    _, posteriors_markers = model_markers.score_samples(X_markers)\n",
        "    _, posteriors_synergy = model_synergy.score_samples(X_synergy)\n",
        "    combined_posteriors = weight_markers * posteriors_markers + weight_synergy * posteriors_synergy\n",
        "    fused_states = np.argmax(combined_posteriors, axis=1) + 1  # 1-indexed states\n",
        "    return fused_states\n",
        "\n",
        "# --------------------------------------------------\n",
        "# METRIC: Distribution Error Calculation\n",
        "# --------------------------------------------------\n",
        "def compute_distribution_error(predictions, frames, stride_intervals, desired_distribution):\n",
        "    errors = []\n",
        "    for start, end in stride_intervals:\n",
        "        idx = np.where((frames >= start) & (frames <= end))[0]\n",
        "        if len(idx) == 0:\n",
        "            continue\n",
        "        pred_stride = predictions[idx]\n",
        "        total_frames = len(pred_stride)\n",
        "        actual_distribution = np.zeros(5)\n",
        "        unique_states, counts = np.unique(pred_stride, return_counts=True)\n",
        "        for state, count in zip(unique_states, counts):\n",
        "            if 1 <= state <= 5:\n",
        "                actual_distribution[state - 1] = count / total_frames\n",
        "        error = np.sum(np.abs(actual_distribution - desired_distribution))\n",
        "        errors.append(error)\n",
        "    return np.mean(errors) if errors else np.inf\n",
        "\n",
        "# --------------------------------------------------\n",
        "# PRECOMPUTATION: Process data once for tuning\n",
        "# --------------------------------------------------\n",
        "def precompute_markers_features():\n",
        "    gait_csv_path = \"/content/aligned_markers_data_cleaned(3).csv\"\n",
        "    df_gait = load_marker_data(gait_csv_path)\n",
        "    df_gait['Label'] = label_frames_custom(df_gait['Frame'].values, STRIDE_INTERVALS, THRESHOLD_FRAME)\n",
        "    df_gait = df_gait[df_gait['Label'] == 1].copy()\n",
        "    marker_prefixes = ['RGT', 'LGT', 'RLE', 'LLE', 'RLM', 'LLM']\n",
        "    df_gait = compute_velocities(df_gait, marker_prefixes)\n",
        "    df_gait = compute_accelerations(df_gait, marker_prefixes)\n",
        "    df_gait = compute_angular_features(df_gait)\n",
        "    df_gait = compute_angular_momentum(df_gait, ['RGT', 'LGT', 'RLE', 'LLE'])\n",
        "    window_size = 150\n",
        "    feature_combination = ['var', 'poly']\n",
        "    gait_feats = extract_features(df_gait, window_size, feature_combination, marker_prefixes)\n",
        "    pca = PCA(n_components=5)\n",
        "    X_gait_pca = pca.fit_transform(gait_feats)\n",
        "    return X_gait_pca, df_gait['Frame'].values\n",
        "\n",
        "def precompute_synergy_features():\n",
        "    emg_csv_path = \"/content/muscle_synergy_activations(14).csv\"\n",
        "    df_emg = load_synergy_data(emg_csv_path)\n",
        "    df_emg['Label'] = label_frames_custom(df_emg['Frame'].values, STRIDE_INTERVALS, THRESHOLD_FRAME)\n",
        "    df_emg = df_emg[df_emg['Label'] == 1].copy()\n",
        "    window_size = 150\n",
        "    synergy_feats = extract_synergy_features(df_emg, window_size)\n",
        "    X_emg = synergy_feats.values\n",
        "    return X_emg, df_emg['Frame'].values\n",
        "\n",
        "# Precompute and store globally for tuning\n",
        "GLOBAL_MARKERS_DATA, MARKERS_FRAMES = precompute_markers_features()\n",
        "GLOBAL_SYNERGY_DATA, SYNERGY_FRAMES = precompute_synergy_features()\n",
        "\n",
        "# --------------------------------------------------\n",
        "# TUNING FUNCTION: Markers HMM\n",
        "# --------------------------------------------------\n",
        "def tune_pipeline_markers(config):\n",
        "    X_gait_pca = GLOBAL_MARKERS_DATA\n",
        "    frames = MARKERS_FRAMES\n",
        "    markers_model = train_markers_hmm(\n",
        "        X_gait_pca, n_components=5, n_iter=10,\n",
        "        alpha=config[\"alpha\"], beta=config[\"beta\"],\n",
        "        gamma=config[\"gamma\"], delta=config[\"delta\"], theta=config[\"theta\"]\n",
        "    )\n",
        "    stat_markers = stationary_distribution(markers_model.transmat_)\n",
        "    penalty = np.sum(np.abs(stat_markers - DESIRED_DISTRIBUTION))\n",
        "    predictions = markers_model.predict(X_gait_pca) + 1\n",
        "    dist_error = compute_distribution_error(predictions, frames, STRIDE_INTERVALS, DESIRED_DISTRIBUTION)\n",
        "    combined_error = dist_error + LAMBDA_PENALTY * penalty\n",
        "    tune.report({\"score\": -combined_error})\n",
        "\n",
        "# --------------------------------------------------\n",
        "# TUNING FUNCTION: Synergy HMM using ASHA\n",
        "# --------------------------------------------------\n",
        "def tune_pipeline_synergy(config):\n",
        "    X_emg = GLOBAL_SYNERGY_DATA\n",
        "    frames = SYNERGY_FRAMES\n",
        "    synergy_model = train_synergy_hmm(\n",
        "        X_emg, n_components=5, n_iter=10,\n",
        "        alpha=config[\"alpha\"], beta=config[\"beta\"],\n",
        "        gamma=config[\"gamma\"], delta=config[\"delta\"], theta=config[\"theta\"]\n",
        "    )\n",
        "    stat_synergy = stationary_distribution(synergy_model.transmat_)\n",
        "    penalty = np.sum(np.abs(stat_synergy - DESIRED_DISTRIBUTION))\n",
        "    predictions = synergy_model.predict(X_emg) + 1\n",
        "    dist_error = compute_distribution_error(predictions, frames, STRIDE_INTERVALS, DESIRED_DISTRIBUTION)\n",
        "    combined_error = dist_error + LAMBDA_PENALTY * penalty\n",
        "    tune.report({\"score\": -combined_error})\n",
        "\n",
        "# --------------------------------------------------\n",
        "# FINAL TRAINING FUNCTION: Full Processing, Model Training, Fusion, and Saving Results\n",
        "# --------------------------------------------------\n",
        "def final_training(markers_tuned_params, synergy_tuned_params):\n",
        "    start_time = time.time()\n",
        "    step_log(1, \"Loading gait marker data and EMG synergy data.\")\n",
        "    gait_csv_path = \"/content/aligned_markers_data_cleaned(3).csv\"\n",
        "    emg_csv_path  = \"/content/muscle_synergy_activations(14).csv\"\n",
        "    try:\n",
        "        df_gait = load_marker_data(gait_csv_path)\n",
        "        df_emg = load_synergy_data(emg_csv_path)\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Data loading error: {e}\")\n",
        "        return\n",
        "    step_log(1.5, \"Merging gait and EMG synergy data on 'Frame' and 'Time (Seconds)'.\")\n",
        "    df_merged = pd.merge(df_gait, df_emg, on=['Frame', 'Time (Seconds)'], how='inner', suffixes=('_gait', '_emg'))\n",
        "    logging.info(f\"Merged data shape: {df_merged.shape}\")\n",
        "    df_merged['Label'] = label_frames_custom(df_merged['Frame'].values, STRIDE_INTERVALS, THRESHOLD_FRAME)\n",
        "    df_train = df_merged[df_merged['Label'] == 1].copy()\n",
        "    logging.info(f\"Using {df_train.shape[0]} stride frames for training.\")\n",
        "    if df_train.empty:\n",
        "        logging.error(\"No training data available after labeling. Check STRIDE_INTERVALS and THRESHOLD_FRAME.\")\n",
        "        return\n",
        "    marker_prefixes = ['RGT', 'LGT', 'RLE', 'LLE', 'RLM', 'LLM']\n",
        "    step_log(3, \"Computing velocities, accelerations, angular features and angular momentum on training gait data.\")\n",
        "    df_train = compute_velocities(df_train, marker_prefixes)\n",
        "    df_train = compute_accelerations(df_train, marker_prefixes)\n",
        "    df_train = compute_angular_features(df_train)\n",
        "    df_train = compute_angular_momentum(df_train, ['RGT', 'LGT', 'RLE', 'LLE'])\n",
        "    step_log(4, \"Applying low-pass filter to training gait data.\")\n",
        "    pos_cols = [col for col in df_train.columns if any(col.startswith(prefix + '_') and col.endswith(('_X', '_Y', '_Z')) for prefix in marker_prefixes)]\n",
        "    vel_cols = [col for col in df_train.columns if any(col.startswith(prefix + '_V') and col.endswith(('_X', '_Y', '_Z')) for prefix in marker_prefixes)]\n",
        "    acc_cols = [col for col in df_train.columns if any(col.startswith(prefix + '_A') and col.endswith(('_X', '_Y', '_Z')) for prefix in marker_prefixes)]\n",
        "    ang_mom_cols = [col for col in df_train.columns if 'Angular_Momentum' in col]\n",
        "    cols_to_filter = list(set(pos_cols + vel_cols + acc_cols + ang_mom_cols))\n",
        "    if cols_to_filter:\n",
        "        try:\n",
        "            data_to_filter = df_train[cols_to_filter].values\n",
        "            filtered_data = butter_lowpass_filter_multi(data_to_filter)\n",
        "            df_train[cols_to_filter] = filtered_data\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Low-pass filtering error: {e}\")\n",
        "            return\n",
        "    step_log(5, \"Extracting gait and EMG synergy features from training data.\")\n",
        "    window_size = 150\n",
        "    feature_combination = ['var', 'poly']\n",
        "    gait_feats = extract_features(df_train, window_size, feature_combination, marker_prefixes)\n",
        "    synergy_feats = extract_synergy_features(df_train, window_size)\n",
        "    pca = PCA(n_components=5)\n",
        "    X_gait_pca = pca.fit_transform(gait_feats)\n",
        "    logging.info(f\"PCA on gait features completed. Explained variance ratio: {pca.explained_variance_ratio_}\")\n",
        "    X_emg = synergy_feats.values\n",
        "    markers_hmm = train_markers_hmm(\n",
        "        X_gait_pca, n_components=5, n_iter=10,\n",
        "        alpha=markers_tuned_params['alpha'],\n",
        "        beta=markers_tuned_params['beta'],\n",
        "        gamma=markers_tuned_params['gamma'],\n",
        "        delta=markers_tuned_params['delta'],\n",
        "        theta=markers_tuned_params['theta']\n",
        "    )\n",
        "    synergy_hmm = train_synergy_hmm(\n",
        "        X_emg, n_components=5, n_iter=10,\n",
        "        alpha=synergy_tuned_params['alpha'],\n",
        "        beta=synergy_tuned_params['beta'],\n",
        "        gamma=synergy_tuned_params['gamma'],\n",
        "        delta=synergy_tuned_params['delta'],\n",
        "        theta=synergy_tuned_params['theta']\n",
        "    )\n",
        "    step_log(10, \"Predicting stride stages using pre-computed training features.\")\n",
        "    predictions = fuse_hmm_predictions(markers_hmm, synergy_hmm, X_gait_pca, X_emg)\n",
        "    unique_states, counts = np.unique(predictions, return_counts=True)\n",
        "    state_distribution = {state: count for state, count in zip(unique_states, counts)}\n",
        "    logging.info(f\"Predicted state distribution: {state_distribution}\")\n",
        "    mean_state = np.mean(predictions)\n",
        "    logging.info(f\"Mean predicted stride stage: {mean_state:.2f}\")\n",
        "    results = df_train[['Frame', 'Time (Seconds)']].copy()\n",
        "    results['PredictedStage'] = predictions\n",
        "    output_csv_path = \"/content/predicted_stride_stages_final.csv\"\n",
        "    results.to_csv(output_csv_path, index=False)\n",
        "    logging.info(f\"Predicted stride stage segmentation saved to {output_csv_path}\")\n",
        "    step_log(11, \"Script complete. Dynamic stride stage mapping is complete.\")\n",
        "    elapsed = time.time() - start_time\n",
        "    logging.info(f\"Total elapsed time: {elapsed:.2f} seconds\")\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# MAIN: Run Tuning and Final Training\n",
        "# ------------------------------------------------------------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    import ray\n",
        "    from ray.tune.schedulers import HyperBandScheduler, ASHAScheduler\n",
        "    ray.init(ignore_reinit_error=True, include_dashboard=False)\n",
        "\n",
        "    # Define hyperparameter search space\n",
        "    search_space = {\n",
        "        \"alpha\": tune.uniform(0.05, 0.95),\n",
        "        \"beta\": tune.uniform(0.05, 0.95),\n",
        "        \"gamma\": tune.uniform(0.05, 0.95),\n",
        "        \"delta\": tune.uniform(0.05, 0.95),\n",
        "        \"theta\": tune.uniform(0.05, 0.95)\n",
        "    }\n",
        "\n",
        "    # ---------------------------\n",
        "    # Tuning for markers using HyperBand\n",
        "    # ---------------------------\n",
        "    markers_scheduler = HyperBandScheduler(\n",
        "        time_attr=\"training_iteration\",\n",
        "        max_t=100,\n",
        "        metric=\"score\",\n",
        "        mode=\"max\"\n",
        "    )\n",
        "\n",
        "    analysis_markers = tune.run(\n",
        "        tune_pipeline_markers,\n",
        "        config=search_space,\n",
        "        scheduler=markers_scheduler,\n",
        "        num_samples=1,\n",
        "        reuse_actors=True,\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    markers_tuned_params = analysis_markers.get_best_config(metric=\"score\", mode=\"max\")\n",
        "    print(\"Best markers hyperparameters (HyperBand):\", markers_tuned_params)\n",
        "\n",
        "    # ---------------------------\n",
        "    # Tuning for synergy using ASHA\n",
        "    # ---------------------------\n",
        "    synergy_scheduler = ASHAScheduler(\n",
        "        time_attr=\"training_iteration\",\n",
        "        max_t=100,\n",
        "        metric=\"score\",\n",
        "        mode=\"max\"\n",
        "    )\n",
        "\n",
        "    analysis_synergy = tune.run(\n",
        "        tune_pipeline_synergy,\n",
        "        config=search_space,\n",
        "        scheduler=synergy_scheduler,\n",
        "        num_samples=1,\n",
        "        reuse_actors=True,\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    synergy_tuned_params = analysis_synergy.get_best_config(metric=\"score\", mode=\"max\")\n",
        "    print(\"Best synergy hyperparameters (ASHA):\", synergy_tuned_params)\n",
        "\n",
        "    # Run final training using the tuned hyperparameters\n",
        "    final_training(markers_tuned_params, synergy_tuned_params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ALZ9p-fVTW1"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the muscle synergy data\n",
        "df_synergy = pd.read_csv(\"/content/muscle_synergy_activations(14) (1).csv\")\n",
        "\n",
        "# Load the predicted stride stages data\n",
        "df_predicted = pd.read_csv(\"/content/predicted_stride_stages_final.csv\")\n",
        "\n",
        "# Merge based on the 'Frame' column\n",
        "df_merged = pd.merge(df_synergy, df_predicted[['Frame', 'PredictedStage']], on='Frame', how='left')\n",
        "\n",
        "# Save the updated file with the new PredictedStage column at the end\n",
        "df_merged.to_csv(\"/content/final_labels.csv\", index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fl58UvCWVVz0"
      },
      "outputs": [],
      "source": [
        "# Load the final_labels.csv file\n",
        "df_final = pd.read_csv(\"/content/final_labels.csv\")\n",
        "\n",
        "# Define the stride intervals\n",
        "STRIDE_INTERVALS = [\n",
        "    (0, 5937), (5938, 9521), (15938, 18914),\n",
        "    (25059, 28499), (28500, 31683), (35059, 38099),\n",
        "    (38100, 41203), (47684, 50606), (56885, 59990),\n",
        "    (66006, 69046), (75526, 79271), (85384, 88231),\n",
        "    (88232, 91271), (96936, 99848), (99849, 105864),\n",
        "    (105865, 108570), (115114, 118026), (118027, 121067),\n",
        "    (127611, 130587), (135852, 138698), (138699, 141516),\n",
        "    (147147, 150252), (150253, 152508), (152509, 155726)\n",
        "]\n",
        "\n",
        "# Create a boolean mask for rows with 'Frame' values within any of the intervals\n",
        "mask = pd.Series(False, index=df_final.index)\n",
        "for start, end in STRIDE_INTERVALS:\n",
        "    mask |= (df_final['Frame'] >= start) & (df_final['Frame'] <= end)\n",
        "\n",
        "# Filter the DataFrame to only include rows in the specified stride intervals\n",
        "df_stride = df_final[mask].copy()\n",
        "\n",
        "# Remove rows containing any NaN values\n",
        "df_clean = df_stride.dropna()\n",
        "\n",
        "# Verify that all NaN values have been removed\n",
        "nan_count_after = df_clean.isna().sum().sum()\n",
        "print(\"Total number of NaN values after removal:\", nan_count_after)\n",
        "\n",
        "# Optionally, save the cleaned DataFrame to a new CSV file\n",
        "df_clean.to_csv(\"/content/final_labels_cleaned.csv\", index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CpLA4tjpVWqz"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from hmmlearn import hmm\n",
        "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import ray\n",
        "from ray import tune\n",
        "from ray.tune.schedulers import ASHAScheduler\n",
        "\n",
        "# --------------------------------------------------\n",
        "# Desired state distribution and penalty weight\n",
        "# --------------------------------------------------\n",
        "DESIRED_DISTRIBUTION = np.array([0.10, 0.40, 0.10, 0.20, 0.20])\n",
        "LAMBDA_PENALTY = 100  # Penalty weight for deviation from the target distribution\n",
        "\n",
        "# --------------------------------------------------\n",
        "# Helper function to enforce symmetry and positive–definiteness\n",
        "# --------------------------------------------------\n",
        "def enforce_symmetry(cov, min_covar=1e-3):\n",
        "    cov_sym = 0.5 * (cov + cov.T)\n",
        "    eigvals, eigvecs = np.linalg.eigh(cov_sym)\n",
        "    eigvals[eigvals < min_covar] = min_covar  # Ensure all eigenvalues are at least min_covar\n",
        "    cov_pd = eigvecs @ np.diag(eigvals) @ eigvecs.T\n",
        "    return cov_pd\n",
        "\n",
        "# --------------------------------------------------\n",
        "# Helper function to filter synergy data to stride intervals\n",
        "# --------------------------------------------------\n",
        "def filter_stride_intervals(df, frame_col='Frame'):\n",
        "    STRIDE_INTERVALS = [\n",
        "        (0, 5937), (5938, 9521), (15938, 18914),\n",
        "        (25059, 28499), (28500, 31683), (35059, 38099),\n",
        "        (38100, 41203), (47684, 50606), (56885, 59990),\n",
        "        (66006, 69046), (75526, 79271), (85384, 88231),\n",
        "        (88232, 91271), (96936, 99848), (99849, 105864),\n",
        "        (105865, 108570), (115114, 118026), (118027, 121067),\n",
        "        (127611, 130587), (135852, 138698), (138699, 141516),\n",
        "        (147147, 150252), (150253, 152508), (152509, 155726)\n",
        "    ]\n",
        "    mask = pd.Series(False, index=df.index)\n",
        "    for start, end in STRIDE_INTERVALS:\n",
        "        mask |= (df[frame_col] >= start) & (df[frame_col] <= end)\n",
        "    return df[mask].copy()\n",
        "\n",
        "# --------------------------------------------------\n",
        "# Compute deviation from the desired state distribution\n",
        "# --------------------------------------------------\n",
        "def compute_distribution_error(model):\n",
        "    stat_dist = stationary_distribution(model.transmat_)\n",
        "    error = np.sum(np.abs(stat_dist - DESIRED_DISTRIBUTION))\n",
        "    return error\n",
        "\n",
        "def stationary_distribution(transmat):\n",
        "    eigvals, eigvecs = np.linalg.eig(transmat.T)\n",
        "    idx = np.argmin(np.abs(eigvals - 1))\n",
        "    stat = np.real(eigvecs[:, idx])\n",
        "    total = np.sum(stat)\n",
        "    stat = stat / total if total != 0 else np.array([0.10, 0.40, 0.10, 0.20, 0.20])\n",
        "    return stat\n",
        "\n",
        "# --------------------------------------------------\n",
        "# Tuning function using ASHA for synergy HMM\n",
        "# --------------------------------------------------\n",
        "def tune_pipeline_synergy_eval(config):\n",
        "    # Load synergy data with ground-truth labels\n",
        "    df_synergy = pd.read_csv(\"/content/final_labels_cleaned.csv\")\n",
        "    synergy_cols = ['Synergy_1', 'Synergy_2', 'Synergy_3', 'Synergy_4', 'Synergy_5']\n",
        "    df_synergy = df_synergy[['Frame'] + synergy_cols + ['PredictedStage']]\n",
        "\n",
        "    # Filter only stride intervals and non-zero labels\n",
        "    df_stride = df_synergy[df_synergy['PredictedStage'] != 0].copy()\n",
        "    df_stride.reset_index(drop=True, inplace=True)\n",
        "    df_stride = filter_stride_intervals(df_stride, frame_col='Frame')\n",
        "    df_stride.reset_index(drop=True, inplace=True)\n",
        "\n",
        "    # Extract features and ground truth\n",
        "    X = df_stride[synergy_cols].values\n",
        "    y_true = df_stride['PredictedStage'].values\n",
        "\n",
        "    # Check for NaN or inf in X and replace if necessary\n",
        "    if np.isnan(X).any() or np.isinf(X).any():\n",
        "        print(\"Warning: Input X contains NaN or Inf. Replacing with zeros.\")\n",
        "        X = np.nan_to_num(X)\n",
        "\n",
        "    # Define transition matrix with hyperparameters\n",
        "    transmat = np.array([\n",
        "        [config[\"alpha\"],  1 - config[\"alpha\"],  0.0,               0.0,                0.0],\n",
        "        [0.0,              config[\"beta\"],       1 - config[\"beta\"], 0.0,                0.0],\n",
        "        [0.0,              0.0,                  config[\"gamma\"],    1 - config[\"gamma\"], 0.0],\n",
        "        [0.0,              0.0,                  0.0,                config[\"delta\"],     1 - config[\"delta\"]],\n",
        "        [1 - config[\"theta\"], 0.0,               0.0,                0.0,                config[\"theta\"]]\n",
        "    ])\n",
        "\n",
        "    # Train the HMM\n",
        "    synergy_hmm = hmm.GaussianHMM(n_components=5, covariance_type='full',\n",
        "                                  n_iter=10, random_state=42, init_params='mc', min_covar=1e-3)\n",
        "    synergy_hmm.startprob_ = np.array([1, 0, 0, 0, 0])\n",
        "    synergy_hmm.transmat_ = transmat\n",
        "    synergy_hmm.fit(X)\n",
        "\n",
        "    # Enforce symmetry on covariance\n",
        "    synergy_hmm.covars_ = np.array([enforce_symmetry(cov) for cov in synergy_hmm.covars_])\n",
        "\n",
        "    # Predict gait stages (convert 0-index to 1-index)\n",
        "    predicted_states = synergy_hmm.predict(X) + 1\n",
        "    acc = accuracy_score(y_true, predicted_states)\n",
        "\n",
        "    # Compute the penalty for distribution error\n",
        "    distribution_error = compute_distribution_error(synergy_hmm)\n",
        "    total_score = acc - LAMBDA_PENALTY * distribution_error\n",
        "\n",
        "    # Check for invalid total_score values\n",
        "    if np.isnan(total_score) or np.isinf(total_score):\n",
        "        total_score = -1e6  # Assign a large penalty for invalid cases\n",
        "\n",
        "    tune.report({\"score\": total_score, \"accuracy\": acc})\n",
        "\n",
        "# --------------------------------------------------\n",
        "# Hyperparameter tuning setup using ASHA\n",
        "# --------------------------------------------------\n",
        "search_space = {\n",
        "    \"alpha\": tune.uniform(0.05, 0.95),\n",
        "    \"beta\":  tune.uniform(0.05, 0.95),\n",
        "    \"gamma\": tune.uniform(0.05, 0.95),\n",
        "    \"delta\": tune.uniform(0.05, 0.95),\n",
        "    \"theta\": tune.uniform(0.05, 0.95)\n",
        "}\n",
        "\n",
        "asha_scheduler = ASHAScheduler(\n",
        "    time_attr=\"training_iteration\",\n",
        "    max_t=100,\n",
        "    metric=\"score\",\n",
        "    mode=\"max\"\n",
        ")\n",
        "\n",
        "ray.init(ignore_reinit_error=True, include_dashboard=False)\n",
        "\n",
        "analysis = tune.run(\n",
        "    tune_pipeline_synergy_eval,\n",
        "    config=search_space,\n",
        "    scheduler=asha_scheduler,\n",
        "    num_samples=100,\n",
        "    resources_per_trial={\"CPU\": 4},\n",
        "    reuse_actors=True,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "best_config = analysis.get_best_config(metric=\"score\", mode=\"max\")\n",
        "print(\"Best synergy hyperparameters (ASHA):\", best_config)\n",
        "\n",
        "# --------------------------------------------------\n",
        "# Final training using best hyperparameters\n",
        "# --------------------------------------------------\n",
        "df_synergy = pd.read_csv(\"/content/final_labels_cleaned.csv\")\n",
        "synergy_cols = ['Synergy_1', 'Synergy_2', 'Synergy_3', 'Synergy_4', 'Synergy_5']\n",
        "df_synergy = df_synergy[['Frame'] + synergy_cols + ['PredictedStage']]\n",
        "\n",
        "df_stride = df_synergy[df_synergy['PredictedStage'] != 0].copy()\n",
        "df_stride.reset_index(drop=True, inplace=True)\n",
        "df_stride = filter_stride_intervals(df_stride, frame_col='Frame')\n",
        "df_stride.reset_index(drop=True, inplace=True)\n",
        "\n",
        "X = df_stride[synergy_cols].values\n",
        "y_true = df_stride['PredictedStage'].values\n",
        "\n",
        "# Check X for NaN or inf\n",
        "if np.isnan(X).any() or np.isinf(X).any():\n",
        "    print(\"Warning: Final training input X contains NaN or Inf. Replacing with zeros.\")\n",
        "    X = np.nan_to_num(X)\n",
        "\n",
        "transmat = np.array([\n",
        "    [best_config[\"alpha\"], 1 - best_config[\"alpha\"], 0.0, 0.0, 0.0],\n",
        "    [0.0, best_config[\"beta\"], 1 - best_config[\"beta\"], 0.0, 0.0],\n",
        "    [0.0, 0.0, best_config[\"gamma\"], 1 - best_config[\"gamma\"], 0.0],\n",
        "    [0.0, 0.0, 0.0, best_config[\"delta\"], 1 - best_config[\"delta\"]],\n",
        "    [1 - best_config[\"theta\"], 0.0, 0.0, 0.0, best_config[\"theta\"]]\n",
        "])\n",
        "\n",
        "synergy_hmm = hmm.GaussianHMM(n_components=5, covariance_type='full', n_iter=10,\n",
        "                              random_state=42, init_params='mc', min_covar=1e-3)\n",
        "synergy_hmm.startprob_ = np.array([1, 0, 0, 0, 0])\n",
        "synergy_hmm.transmat_ = transmat\n",
        "synergy_hmm.fit(X)\n",
        "\n",
        "# Enforce symmetry on covariance in final model as well\n",
        "synergy_hmm.covars_ = np.array([enforce_symmetry(cov) for cov in synergy_hmm.covars_])\n",
        "\n",
        "predicted_states = synergy_hmm.predict(X) + 1\n",
        "print(classification_report(y_true, predicted_states))\n",
        "\n",
        "# Compute the confusion matrix\n",
        "cm = confusion_matrix(y_true, predicted_states)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(cm)\n",
        "\n",
        "# Plot the confusion matrix as a heatmap\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "plt.xlabel('Predicted Labels')\n",
        "plt.ylabel('True Labels')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mngjRSffVZum"
      },
      "outputs": [],
      "source": [
        "class AttentionLayer(Layer):\n",
        "    \"\"\"\n",
        "    Self-Attention Layer reproducing Eqs. (11)–(15) exactly as in the paper:\n",
        "\n",
        "    (11) Q = H W_q\n",
        "    (12) K = H W_k,  V = H W_v\n",
        "    (13) A = softmax((Q K^T) / sqrt(d_k))\n",
        "    (14) Z(t) = Σ_i [A(t, i) * v_i]\n",
        "    (15) z = Σ_t [Z(t)]  (final context vector)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, **kwargs):\n",
        "        super(AttentionLayer, self).__init__(**kwargs)\n",
        "        self.attention_weights = None  # Will store A for inspection if needed\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        \"\"\"\n",
        "        Create weight matrices W_q, W_k, W_v.\n",
        "\n",
        "        Args:\n",
        "            input_shape: (batch_size, time_steps, features)\n",
        "        \"\"\"\n",
        "        try:\n",
        "            d_model = input_shape[-1]  # 'features' dimension\n",
        "            self.W_q = self.add_weight(\n",
        "                name=\"W_q\",\n",
        "                shape=(d_model, d_model),\n",
        "                initializer=\"glorot_uniform\",\n",
        "                trainable=True,\n",
        "            )\n",
        "            self.W_k = self.add_weight(\n",
        "                name=\"W_k\",\n",
        "                shape=(d_model, d_model),\n",
        "                initializer=\"glorot_uniform\",\n",
        "                trainable=True,\n",
        "            )\n",
        "            self.W_v = self.add_weight(\n",
        "                name=\"W_v\",\n",
        "                shape=(d_model, d_model),\n",
        "                initializer=\"glorot_uniform\",\n",
        "                trainable=True,\n",
        "            )\n",
        "\n",
        "            super(AttentionLayer, self).build(input_shape)\n",
        "            logging.getLogger(__name__).info(\"Built AttentionLayer with W_q, W_k, W_v.\")\n",
        "        except Exception as e:\n",
        "            logging.getLogger(__name__).error(f\"Error during AttentionLayer build: {e}\")\n",
        "\n",
        "    def call(self, H):\n",
        "        \"\"\"\n",
        "        Forward pass that implements Eqs. (11)–(15).\n",
        "\n",
        "        Args:\n",
        "            H: Tensor of shape (batch_size, time_steps, features).\n",
        "\n",
        "        Returns:\n",
        "            context: Final context vector z (Eq. (15)) of shape (batch_size, features).\n",
        "        \"\"\"\n",
        "        try:\n",
        "            # === Eq. (11): Q = H W_q\n",
        "            Q = tf.matmul(H, self.W_q)  # (batch_size, time_steps, d_model)\n",
        "\n",
        "            # === Eq. (12): K = H W_k, V = H W_v\n",
        "            K = tf.matmul(H, self.W_k)  # (batch_size, time_steps, d_model)\n",
        "            V = tf.matmul(H, self.W_v)  # (batch_size, time_steps, d_model)\n",
        "\n",
        "            # === Eq. (13): A = softmax((QK^T) / sqrt(d_k))\n",
        "            d_k = tf.cast(tf.shape(K)[-1], tf.float32)  # dimension for scaling\n",
        "            scores = tf.matmul(Q, K, transpose_b=True)  # (batch_size, time_steps, time_steps)\n",
        "            scores = scores / tf.sqrt(d_k)\n",
        "            A = tf.nn.softmax(scores, axis=-1)  # (batch_size, time_steps, time_steps)\n",
        "\n",
        "            # === Eq. (14): Z(t) = Σ_i [A(t,i) * v_i]\n",
        "            # We get the entire Z by matrix multiplication A * V:\n",
        "            # shape: (batch_size, time_steps, d_model)\n",
        "            Z = tf.matmul(A, V)\n",
        "\n",
        "            # === Eq. (15): z = Σ_t [Z(t)]\n",
        "            # shape: (batch_size, d_model)\n",
        "            context = tf.reduce_sum(Z, axis=1)\n",
        "\n",
        "            # Save attention weights for possible inspection\n",
        "            self.attention_weights = A\n",
        "\n",
        "            return context\n",
        "\n",
        "        except Exception as e:\n",
        "            logging.getLogger(__name__).error(f\"Error during AttentionLayer call: {e}\")\n",
        "            return H  # fallback to input if something goes wrong\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        \"\"\"\n",
        "        Output shape is (batch_size, features).\n",
        "        \"\"\"\n",
        "        return (input_shape[0], input_shape[-1])\n",
        "\n",
        "    def get_attention_weights(self):\n",
        "        \"\"\"\n",
        "        Returns the attention weights A from Eq. (13),\n",
        "        shape: (batch_size, time_steps, time_steps).\n",
        "        \"\"\"\n",
        "        return self.attention_weights\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "beWcvogMVp-D"
      },
      "outputs": [],
      "source": [
        "class LSTMModelBuilder:\n",
        "    \"\"\"Handles building various machine learning models.\"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def build_model_hp(hp, input_shape, num_classes):\n",
        "        \"\"\"\n",
        "        Builds and compiles a Bidirectional LSTM model based on the provided hyperparameters.\n",
        "\n",
        "        This method is intended for use with hyperparameter tuning libraries such as Keras Tuner.\n",
        "\n",
        "        Args:\n",
        "            hp (HyperParameters): Hyperparameters instance to sample hyperparameters.\n",
        "            input_shape (tuple): Shape of the input data (time_steps, features).\n",
        "            num_classes (int): Number of output classes for classification.\n",
        "\n",
        "        Returns:\n",
        "            tf.keras.Model: Compiled Bidirectional LSTM model.\n",
        "\n",
        "        Raises:\n",
        "            Exception: If there is an error during model building.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            inputs = Input(shape=input_shape)\n",
        "            units_1 = hp.Int('units_1', min_value=32, max_value=128, step=32)\n",
        "            dropout_rate_1 = hp.Float('dropout_rate_1', min_value=0.1, max_value=0.5, step=0.1)\n",
        "            second_lstm = hp.Boolean('second_lstm')\n",
        "            if second_lstm:\n",
        "                units_2 = hp.Int('units_2', min_value=32, max_value=128, step=32)\n",
        "                dropout_rate_2 = hp.Float('dropout_rate_2', min_value=0.1, max_value=0.5, step=0.1)\n",
        "            fc_units = hp.Int('fc_units', min_value=128, max_value=512, step=128)\n",
        "            fc_dropout = hp.Float('fc_dropout', min_value=0.1, max_value=0.5, step=0.1)\n",
        "            learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])\n",
        "            l1_reg = hp.Float('l1_regularization', min_value=1e-6, max_value=1e-2, sampling='LOG', default=1e-4)\n",
        "            l2_reg = hp.Float('l2_regularization', min_value=1e-6, max_value=1e-2, sampling='LOG', default=1e-4)\n",
        "\n",
        "            lstm_layer_1 = LSTM(units=units_1, return_sequences=True,\n",
        "                                kernel_regularizer=regularizers.l1_l2(l1=l1_reg, l2=l2_reg))\n",
        "            lstm_1 = Bidirectional(lstm_layer_1)(inputs)\n",
        "            bn_1 = BatchNormalization()(lstm_1)\n",
        "            dropout_1_layer = Dropout(rate=dropout_rate_1)(bn_1)\n",
        "\n",
        "            if second_lstm:\n",
        "                lstm_layer_2 = LSTM(units=units_2, return_sequences=True,\n",
        "                                    kernel_regularizer=regularizers.l1_l2(l1=l1_reg, l2=l2_reg))\n",
        "                lstm_2 = Bidirectional(lstm_layer_2)(dropout_1_layer)\n",
        "                bn_2 = BatchNormalization()(lstm_2)\n",
        "                dropout_2_layer = Dropout(rate=dropout_rate_2)(bn_2)\n",
        "            else:\n",
        "                dropout_2_layer = dropout_1_layer\n",
        "\n",
        "            attention = AttentionLayer()(dropout_2_layer)\n",
        "            fc_layer = Dense(units=fc_units, activation='relu',\n",
        "                             kernel_regularizer=regularizers.l1_l2(l1=l1_reg, l2=l2_reg))\n",
        "            fc = fc_layer(attention)\n",
        "            bn_fc = BatchNormalization()(fc)\n",
        "            fc_dropout_layer = Dropout(rate=fc_dropout)(bn_fc)\n",
        "            outputs = Dense(units=num_classes, activation='softmax')(fc_dropout_layer)\n",
        "            model = Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "            optimizer_instance = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "            model.compile(optimizer=optimizer_instance,\n",
        "                          loss='sparse_categorical_crossentropy',\n",
        "                          metrics=['accuracy', F1Score(num_classes=num_classes)])\n",
        "            return model\n",
        "        except Exception as e:\n",
        "            logging.getLogger(__name__).error(f\"Error during LSTM model building: {e}\")\n",
        "            return None\n",
        "\n",
        "    @staticmethod\n",
        "    def build_lstm_model_final(hp_dict, input_shape, num_classes):\n",
        "        \"\"\"\n",
        "        Builds and compiles a Bidirectional LSTM model based on the provided hyperparameters.\n",
        "\n",
        "        This method is intended for building the final model using the best hyperparameters found\n",
        "        during hyperparameter tuning.\n",
        "\n",
        "        Args:\n",
        "            hp_dict (dict): Dictionary containing hyperparameter values.\n",
        "            input_shape (tuple): Shape of the input data (time_steps, features).\n",
        "            num_classes (int): Number of output classes for classification.\n",
        "\n",
        "        Returns:\n",
        "            tf.keras.Model: Compiled Bidirectional LSTM model.\n",
        "\n",
        "        Raises:\n",
        "            KeyError: If a required hyperparameter is missing.\n",
        "            ValueError: If a hyperparameter value is invalid.\n",
        "            Exception: If there is an error during model building.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            logger.info(f\"Building LSTM model with hyperparameters: {hp_dict}\")\n",
        "            for key, value in hp_dict.items():\n",
        "                 logger.info(f\"{key}: {value}\")\n",
        "\n",
        "            inputs = Input(shape=input_shape)\n",
        "\n",
        "            # Extract and convert hyperparameters with default values\n",
        "            units_1 = int(hp_dict.get('units_1', 64))\n",
        "            dropout_rate_1 = float(hp_dict.get('dropout_rate_1', 0.1))\n",
        "            second_lstm = hp_dict.get('second_lstm', False)\n",
        "            if isinstance(second_lstm, str):\n",
        "                second_lstm = second_lstm.lower() == 'true'\n",
        "\n",
        "            if second_lstm:\n",
        "                units_2 = int(hp_dict.get('units_2', 64))\n",
        "                dropout_rate_2 = float(hp_dict.get('dropout_rate_2', 0.1))\n",
        "            fc_units = int(hp_dict.get('fc_units', 128))\n",
        "            fc_dropout = float(hp_dict.get('fc_dropout', 0.1))\n",
        "            learning_rate = float(hp_dict.get('learning_rate', 0.001))\n",
        "            l1_reg = float(hp_dict.get('l1_regularization', 0.0))\n",
        "            l2_reg = float(hp_dict.get('l2_regularization', 0.0))\n",
        "\n",
        "            # Build the model\n",
        "            lstm_layer_1 = LSTM(\n",
        "                units=units_1,\n",
        "                return_sequences=True,\n",
        "                kernel_regularizer=regularizers.l1_l2(l1=l1_reg, l2=l2_reg)\n",
        "            )\n",
        "            lstm_1 = Bidirectional(lstm_layer_1)(inputs)\n",
        "            bn_1 = BatchNormalization()(lstm_1)\n",
        "            dropout_1_layer = Dropout(rate=dropout_rate_1)(bn_1)\n",
        "\n",
        "            if second_lstm:\n",
        "                lstm_layer_2 = LSTM(\n",
        "                    units=units_2,\n",
        "                    return_sequences=True,\n",
        "                    kernel_regularizer=regularizers.l1_l2(l1=l1_reg, l2=l2_reg)\n",
        "                )\n",
        "                lstm_2 = Bidirectional(lstm_layer_2)(dropout_1_layer)\n",
        "                bn_2 = BatchNormalization()(lstm_2)\n",
        "                dropout_2_layer = Dropout(rate=dropout_rate_2)(bn_2)\n",
        "            else:\n",
        "                dropout_2_layer = dropout_1_layer\n",
        "\n",
        "            attention = AttentionLayer()(dropout_2_layer)\n",
        "            fc_layer = Dense(\n",
        "                units=fc_units,\n",
        "                activation='relu',\n",
        "                kernel_regularizer=regularizers.l1_l2(l1=l1_reg, l2=l2_reg)\n",
        "            )\n",
        "            fc = fc_layer(attention)\n",
        "            bn_fc = BatchNormalization()(fc)\n",
        "            fc_dropout_layer = Dropout(rate=fc_dropout)(bn_fc)\n",
        "            outputs = Dense(units=num_classes, activation='softmax')(fc_dropout_layer)\n",
        "            model = Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "            optimizer_instance = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "            model.compile(\n",
        "                optimizer=optimizer_instance,\n",
        "                loss='sparse_categorical_crossentropy',\n",
        "                metrics=['accuracy', F1Score(num_classes=num_classes)]\n",
        "            )\n",
        "            return model\n",
        "        except KeyError as e:\n",
        "            logger.error(f\"Missing hyperparameter: {e}\")\n",
        "            return None\n",
        "        except ValueError as e:\n",
        "            logger.error(f\"Invalid hyperparameter value: {e}\")\n",
        "            return None\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error during final LSTM model building: {e}\")\n",
        "            return None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zznGwk9kVvCI"
      },
      "outputs": [],
      "source": [
        "class CNNModelBuilder:\n",
        "    \"\"\"Handles building CNN models with hyperparameter tuning using an explicit Input layer.\"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def build_model_hp(hp, input_shape, num_classes):\n",
        "        \"\"\"\n",
        "        Builds and compiles a CNN model for hyperparameter tuning.\n",
        "        Uses an explicit Input layer to avoid warnings and to guarantee proper tensor shapes.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            model = Sequential()\n",
        "            # Use an explicit Input layer instead of passing input_shape to the first Conv1D.\n",
        "            model.add(Input(shape=input_shape))\n",
        "\n",
        "            # Determine the number of Conv1D layers to add\n",
        "            conv_layers = hp.Int('conv_layers', min_value=1, max_value=3, step=1)\n",
        "            for i in range(conv_layers):\n",
        "                filters = hp.Int(f'filters_{i}', min_value=32, max_value=128, step=32)\n",
        "                kernel_size = hp.Choice(f'kernel_size_{i}', values=[3, 5, 7])\n",
        "                activation = hp.Choice(f'activation_{i}', values=['relu', 'tanh'])\n",
        "                # Add Conv1D layer with padding to maintain dimensions\n",
        "                model.add(Conv1D(filters=filters,\n",
        "                                 kernel_size=kernel_size,\n",
        "                                 activation=activation,\n",
        "                                 padding='same'))\n",
        "                # Optionally add dropout after each convolutional layer\n",
        "                if hp.Boolean(f'dropout_{i}', default=False):\n",
        "                    dropout_rate = hp.Float(f'dropout_rate_{i}', min_value=0.1, max_value=0.5, step=0.1, default=0.1)\n",
        "                    model.add(Dropout(rate=dropout_rate))\n",
        "\n",
        "            # Apply GlobalMaxPooling1D after all convolutional layers\n",
        "            model.add(GlobalMaxPooling1D())\n",
        "\n",
        "            # Dense layers for further processing\n",
        "            dense_units = hp.Int('dense_units', min_value=64, max_value=256, step=64, default=64)\n",
        "            dense_dropout = hp.Float('dense_dropout', min_value=0.1, max_value=0.5, step=0.1, default=0.1)\n",
        "            model.add(Dense(units=dense_units, activation='relu'))\n",
        "            model.add(Dropout(rate=dense_dropout))\n",
        "\n",
        "            # Output layer with softmax activation for classification\n",
        "            model.add(Dense(units=num_classes, activation='softmax'))\n",
        "\n",
        "            # Compile the model with a custom F1Score metric\n",
        "            model.compile(\n",
        "                optimizer='adam',\n",
        "                loss='sparse_categorical_crossentropy',\n",
        "                metrics=['accuracy', F1Score(num_classes=num_classes)]\n",
        "            )\n",
        "            return model\n",
        "        except Exception as e:\n",
        "            logging.getLogger(__name__).error(f\"Error during CNN model building: {e}\")\n",
        "            raise\n",
        "\n",
        "    @staticmethod\n",
        "    def build_cnn_model_final(hp_dict, input_shape, num_classes):\n",
        "        \"\"\"\n",
        "        Builds and compiles the final CNN model using the best hyperparameters found.\n",
        "        Uses an explicit Input layer to ensure correct tensor dimensions.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            model = Sequential()\n",
        "            model.add(Input(shape=input_shape))\n",
        "            conv_layers = hp_dict.get('conv_layers', 1)\n",
        "            for i in range(conv_layers):\n",
        "                filters = hp_dict.get(f'filters_{i}', 32)\n",
        "                kernel_size = hp_dict.get(f'kernel_size_{i}', 3)\n",
        "                activation = hp_dict.get(f'activation_{i}', 'relu')\n",
        "                model.add(Conv1D(filters=filters,\n",
        "                                 kernel_size=kernel_size,\n",
        "                                 activation=activation,\n",
        "                                 padding='same'))\n",
        "                if hp_dict.get(f'dropout_{i}', False):\n",
        "                    dropout_rate = hp_dict.get(f'dropout_rate_{i}', 0.1)\n",
        "                    model.add(Dropout(rate=dropout_rate))\n",
        "\n",
        "            model.add(GlobalMaxPooling1D())\n",
        "\n",
        "            dense_units = hp_dict.get('dense_units', 64)\n",
        "            dense_dropout = hp_dict.get('dense_dropout', 0.1)\n",
        "            model.add(Dense(units=dense_units, activation='relu'))\n",
        "            model.add(Dropout(rate=dense_dropout))\n",
        "\n",
        "            model.add(Dense(units=num_classes, activation='softmax'))\n",
        "\n",
        "            model.compile(\n",
        "                optimizer='adam',\n",
        "                loss='sparse_categorical_crossentropy',\n",
        "                metrics=['accuracy', F1Score(num_classes=num_classes)]\n",
        "            )\n",
        "            return model\n",
        "        except Exception as e:\n",
        "            logging.getLogger(__name__).error(f\"Error during final CNN model building: {e}\")\n",
        "            return None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hTWLc0bYVyw6"
      },
      "outputs": [],
      "source": [
        "class TransformerModelBuilder:\n",
        "    \"\"\"Handles building Transformer models with hyperparameter tuning.\"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def build_model_hp(hp, input_shape, num_classes):\n",
        "        \"\"\"\n",
        "        Builds and compiles a Transformer model based on the provided hyperparameters.\n",
        "\n",
        "        This method is intended for use with hyperparameter tuning libraries such as Keras Tuner.\n",
        "\n",
        "        Args:\n",
        "            hp (HyperParameters): Hyperparameters instance to sample hyperparameters.\n",
        "            input_shape (tuple): Shape of the input data (time_steps, features).\n",
        "            num_classes (int): Number of output classes for classification.\n",
        "\n",
        "        Returns:\n",
        "            tf.keras.Model: Compiled Transformer model.\n",
        "\n",
        "        Raises:\n",
        "            Exception: If there is an error during model building.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            inputs = Input(shape=input_shape)\n",
        "\n",
        "            # Hyperparameters for MultiHeadAttention\n",
        "            num_heads = hp.Int('num_heads', min_value=2, max_value=8, step=2)\n",
        "            ff_dim = hp.Int('ff_dim', min_value=64, max_value=256, step=64)\n",
        "\n",
        "            # Transformer block\n",
        "            attention = MultiHeadAttention(num_heads=num_heads, key_dim=ff_dim)(inputs, inputs)\n",
        "            attention = Dropout(rate=hp.Float('attention_dropout', min_value=0.1, max_value=0.5, step=0.1))(attention)\n",
        "            attention = BatchNormalization()(attention)\n",
        "\n",
        "            ffn = Dense(ff_dim, activation='relu')(attention)\n",
        "            ffn = Dense(input_shape[1])(ffn)  # Ensuring dimensionality matches\n",
        "            ffn = Dropout(rate=hp.Float('ffn_dropout', min_value=0.1, max_value=0.5, step=0.1))(ffn)\n",
        "            ffn = BatchNormalization()(ffn)\n",
        "\n",
        "            out = tf.keras.layers.Add()([attention, ffn])\n",
        "            out = GlobalAveragePooling1D()(out)\n",
        "\n",
        "            # Dense layers\n",
        "            dense_units = hp.Int('dense_units', min_value=64, max_value=256, step=64)\n",
        "            dense_dropout = hp.Float('dense_dropout', min_value=0.1, max_value=0.5, step=0.1)\n",
        "            out = Dense(units=dense_units, activation='relu')(out)\n",
        "            out = Dropout(rate=dense_dropout)(out)\n",
        "\n",
        "            # Output layer\n",
        "            outputs = Dense(units=num_classes, activation='softmax')(out)\n",
        "\n",
        "            model = Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "            # Compile model with custom F1Score metric\n",
        "            model.compile(\n",
        "                optimizer='adam',\n",
        "                loss='sparse_categorical_crossentropy',\n",
        "                metrics=['accuracy', F1Score(num_classes=num_classes)]\n",
        "            )\n",
        "            return model\n",
        "        except Exception as e:\n",
        "            logging.getLogger(__name__).error(f\"Error during Transformer model building: {e}\")\n",
        "            return None\n",
        "\n",
        "    @staticmethod\n",
        "    def build_transformer_model_final(hp_dict, input_shape, num_classes):\n",
        "        \"\"\"\n",
        "        Builds and compiles a Transformer model based on the provided hyperparameters.\n",
        "\n",
        "        This method is intended for building the final model using the best hyperparameters found\n",
        "        during hyperparameter tuning.\n",
        "\n",
        "        Args:\n",
        "            hp_dict (dict): Dictionary containing hyperparameter values.\n",
        "            input_shape (tuple): Shape of the input data (time_steps, features).\n",
        "            num_classes (int): Number of output classes for classification.\n",
        "\n",
        "        Returns:\n",
        "            tf.keras.Model: Compiled Transformer model.\n",
        "\n",
        "        Raises:\n",
        "            Exception: If there is an error during model building.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            inputs = Input(shape=input_shape)\n",
        "\n",
        "            # Extract hyperparameters from hp_dict\n",
        "            num_heads = hp_dict.get('num_heads', 2)\n",
        "            ff_dim = hp_dict.get('ff_dim', 64)\n",
        "            attention_dropout = hp_dict.get('attention_dropout', 0.1)\n",
        "            ffn_dropout = hp_dict.get('ffn_dropout', 0.1)\n",
        "            dense_units = hp_dict.get('dense_units', 64)\n",
        "            dense_dropout = hp_dict.get('dense_dropout', 0.1)\n",
        "\n",
        "            # Transformer block\n",
        "            attention = MultiHeadAttention(num_heads=num_heads, key_dim=ff_dim)(inputs, inputs)\n",
        "            attention = Dropout(rate=attention_dropout)(attention)\n",
        "            attention = BatchNormalization()(attention)\n",
        "\n",
        "            ffn = Dense(ff_dim, activation='relu')(attention)\n",
        "            ffn = Dense(input_shape[1])(ffn)  # Ensuring dimensionality matches\n",
        "            ffn = Dropout(rate=ffn_dropout)(ffn)\n",
        "            ffn = BatchNormalization()(ffn)\n",
        "\n",
        "            out = tf.keras.layers.Add()([attention, ffn])\n",
        "            out = GlobalAveragePooling1D()(out)\n",
        "\n",
        "            # Dense layers\n",
        "            out = Dense(units=dense_units, activation='relu')(out)\n",
        "            out = Dropout(rate=dense_dropout)(out)\n",
        "\n",
        "            # Output layer\n",
        "            outputs = Dense(units=num_classes, activation='softmax')(out)\n",
        "\n",
        "            model = Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "            # Compile model with custom F1Score metric\n",
        "            model.compile(\n",
        "                optimizer='adam',\n",
        "                loss='sparse_categorical_crossentropy',\n",
        "                metrics=['accuracy', F1Score(num_classes=num_classes)]\n",
        "            )\n",
        "            return model\n",
        "        except Exception as e:\n",
        "            logging.getLogger(__name__).error(f\"Error during final Transformer model building: {e}\")\n",
        "            return None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tidqixx6V03C"
      },
      "outputs": [],
      "source": [
        "def hyperparameter_tuning(model_builder, tuner_type, X_train, y_train, X_val, y_val, input_shape, num_classes, fold, strategy):\n",
        "\n",
        "    try:\n",
        "\n",
        "        if tuner_type == 'BayesianOptimization':\n",
        "            tuner = BayesianOptimization(\n",
        "                hypermodel=lambda hp: model_builder.build_model_hp(hp, input_shape, num_classes),\n",
        "                objective='val_f1_score',\n",
        "                max_trials=CONFIG['tuner_trials'],\n",
        "                executions_per_trial=1,\n",
        "                directory='tuner_dir',\n",
        "                project_name=f'{strategy}_outer_fold_{fold}'\n",
        "            )\n",
        "        elif tuner_type == 'RandomSearch':\n",
        "            tuner = RandomSearch(\n",
        "                hypermodel=lambda hp: model_builder.build_model_hp(hp, input_shape, num_classes),\n",
        "                objective='val_f1_score',\n",
        "                max_trials=CONFIG['tuner_trials'],\n",
        "                executions_per_trial=1,\n",
        "                directory='tuner_dir',\n",
        "                project_name=f'{strategy}_outer_fold_{fold}'\n",
        "            )\n",
        "        elif tuner_type == 'Hyperband':\n",
        "            tuner = Hyperband(\n",
        "                hypermodel=lambda hp: model_builder.build_model_hp(hp, input_shape, num_classes),\n",
        "                objective='val_f1_score',\n",
        "                max_epochs=CONFIG['epochs'],\n",
        "                factor=3,\n",
        "                directory='tuner_dir',\n",
        "                project_name=f'{strategy}_outer_fold_{fold}'\n",
        "            )\n",
        "        else:\n",
        "            logger.error(f\"Unsupported tuner type: {tuner_type}\")\n",
        "            return None, None\n",
        "\n",
        "        stop_early = EarlyStopping(\n",
        "            monitor='val_loss',\n",
        "            patience=CONFIG['early_stopping_patience'],\n",
        "            restore_best_weights=True\n",
        "        )\n",
        "\n",
        "        tuner.search(\n",
        "            X_train,\n",
        "            y_train,\n",
        "            epochs=CONFIG['epochs'],\n",
        "            validation_data=(X_val, y_val),\n",
        "            callbacks=[stop_early],\n",
        "            batch_size=CONFIG['batch_size'],\n",
        "            verbose=0\n",
        "        )\n",
        "\n",
        "        best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
        "        best_hps_dict = best_hps.values\n",
        "        logger.info(f\"Best hyperparameters for {strategy} on fold {fold}: {best_hps.values}\")\n",
        "\n",
        "        return tuner, best_hps\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error during hyperparameter tuning with {tuner_type}: {e}\")\n",
        "        return None, None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-4aS4QvyV3O2"
      },
      "outputs": [],
      "source": [
        "# -------------------------------------------------------------------\n",
        "# Setup logger\n",
        "# -------------------------------------------------------------------\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# -------------------------------------------------------------------\n",
        "# 1. Generate Confusion Matrix\n",
        "# -------------------------------------------------------------------\n",
        "def generate_confusion_matrix(y_true, y_pred, classes, save_path):\n",
        "    \"\"\"\n",
        "    Generate and save a confusion matrix plot.\n",
        "\n",
        "    Parameters:\n",
        "        y_true (np.ndarray): True labels.\n",
        "        y_pred (np.ndarray): Predicted labels.\n",
        "        classes (np.ndarray or list): List of unique classes.\n",
        "        save_path (str): Path to save the confusion matrix plot.\n",
        "\n",
        "    Returns:\n",
        "        cm (pd.DataFrame): Confusion matrix as a pandas DataFrame.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
        "        cm = confusion_matrix(y_true, y_pred, labels=classes)\n",
        "        cm_df = pd.DataFrame(\n",
        "            cm,\n",
        "            index=[f'True_{cls}' for cls in classes],\n",
        "            columns=[f'Pred_{cls}' for cls in classes]\n",
        "        )\n",
        "\n",
        "        plt.figure(figsize=(10, 8))\n",
        "        sns.heatmap(cm_df, annot=True, fmt='d', cmap='Blues')\n",
        "        plt.xlabel('Predicted')\n",
        "        plt.ylabel('True')\n",
        "        plt.title('Confusion Matrix')\n",
        "        plt.savefig(save_path)\n",
        "        plt.close()\n",
        "        logger.info(f\"Saved confusion matrix as '{save_path}'\")\n",
        "        return cm_df\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error generating confusion matrix: {e}\")\n",
        "        return None\n",
        "\n",
        "# -------------------------------------------------------------------\n",
        "# 2. Adjust Predicted Probabilities\n",
        "# -------------------------------------------------------------------\n",
        "def adjust_pred_proba(y_pred_proba, num_classes):\n",
        "    \"\"\"\n",
        "    Adjusts the predicted probabilities to match the number of classes\n",
        "    by padding or truncating columns.\n",
        "\n",
        "    Parameters:\n",
        "        y_pred_proba (np.ndarray): Predicted probabilities, shape (n_samples, current_classes).\n",
        "        num_classes (int): Required number of classes.\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: Adjusted probabilities, shape (n_samples, num_classes).\n",
        "    \"\"\"\n",
        "    current_classes = y_pred_proba.shape[1]\n",
        "    if current_classes < num_classes:\n",
        "        padding = np.zeros((y_pred_proba.shape[0], num_classes - current_classes))\n",
        "        y_pred_proba = np.hstack([y_pred_proba, padding])\n",
        "    elif current_classes > num_classes:\n",
        "        y_pred_proba = y_pred_proba[:, :num_classes]\n",
        "    return y_pred_proba\n",
        "\n",
        "# -------------------------------------------------------------------\n",
        "# 3. Prepare ROC AUC\n",
        "# -------------------------------------------------------------------\n",
        "def prepare_roc_auc(y_true, y_pred_proba, num_classes, label_encoder):\n",
        "    \"\"\"\n",
        "    Prepares y_true and y_pred_proba for ROC AUC computation by ensuring\n",
        "    that y_true is one-hot encoded and y_pred_proba has columns for all classes.\n",
        "\n",
        "    Parameters:\n",
        "        y_true (array-like): True labels of shape (n_samples,).\n",
        "        y_pred_proba (array-like): Predicted probabilities of shape (n_samples, n_classes_pred).\n",
        "        num_classes (int): Total number of classes.\n",
        "        label_encoder: A fitted LabelEncoder (if used), not strictly required unless you need to decode.\n",
        "\n",
        "    Returns:\n",
        "        y_true_binarized (np.ndarray): Shape (n_samples, num_classes).\n",
        "        y_pred_proba_padded (np.ndarray): Shape (n_samples, num_classes).\n",
        "    \"\"\"\n",
        "    # Binarize y_true\n",
        "    y_true_binarized = label_binarize(y_true, classes=range(num_classes))\n",
        "\n",
        "    # If y_pred_proba has fewer or more columns than num_classes, fix it\n",
        "    if y_pred_proba.shape[1] < num_classes:\n",
        "        padding = np.zeros((y_pred_proba.shape[0], num_classes - y_pred_proba.shape[1]))\n",
        "        y_pred_proba_padded = np.hstack([y_pred_proba, padding])\n",
        "    elif y_pred_proba.shape[1] > num_classes:\n",
        "        y_pred_proba_padded = y_pred_proba[:, :num_classes]\n",
        "    else:\n",
        "        y_pred_proba_padded = y_pred_proba\n",
        "\n",
        "    return y_true_binarized, y_pred_proba_padded\n",
        "\n",
        "# -------------------------------------------------------------------\n",
        "# 4. Compute ROC AUC\n",
        "# -------------------------------------------------------------------\n",
        "\n",
        "def compute_roc_auc(y_true, y_pred_prob, num_classes, save_path):\n",
        "    \"\"\"\n",
        "    Compute and plot ROC AUC for multi-class classification.\n",
        "\n",
        "    Parameters:\n",
        "        y_true (np.ndarray): True labels, shape (n_samples,).\n",
        "        y_pred_prob (np.ndarray): Predicted probabilities, shape (n_samples, n_classes).\n",
        "        num_classes (int): Number of classes.\n",
        "        save_path (str): Path to save the ROC AUC plot.\n",
        "\n",
        "    Returns:\n",
        "        roc_auc (float): Weighted ROC AUC score.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
        "\n",
        "        # Binarize the output\n",
        "        y_true_binarized = label_binarize(y_true, classes=range(num_classes))\n",
        "        # If there's only 1 class, expand\n",
        "        if y_true_binarized.shape[1] == 1:\n",
        "            y_true_binarized = np.hstack([1 - y_true_binarized, y_true_binarized])\n",
        "            logger.warning(\"Only one class present in y_true. ROC AUC might not be informative.\")\n",
        "\n",
        "        # Calculate weighted ROC AUC\n",
        "        roc_auc = roc_auc_score(\n",
        "            y_true_binarized,\n",
        "            y_pred_prob,\n",
        "            average='weighted',\n",
        "            multi_class='ovr'\n",
        "        )\n",
        "        logger.info(f\"ROC AUC Score: {roc_auc:.4f}\")\n",
        "\n",
        "        # Plot ROC curve for each class\n",
        "        fpr = dict()\n",
        "        tpr = dict()\n",
        "        roc_auc_dict = dict()\n",
        "        for i in range(num_classes):\n",
        "            fpr[i], tpr[i], _ = roc_curve(y_true_binarized[:, i], y_pred_prob[:, i])\n",
        "            roc_auc_dict[i] = auc(fpr[i], tpr[i])\n",
        "\n",
        "        plt.figure(figsize=(10, 8))\n",
        "        for i in range(num_classes):\n",
        "            plt.plot(fpr[i], tpr[i], label=f'Class {i} (AUC = {roc_auc_dict[i]:.2f})')\n",
        "        plt.plot([0, 1], [0, 1], 'k--')\n",
        "        plt.xlabel('False Positive Rate')\n",
        "        plt.ylabel('True Positive Rate')\n",
        "        plt.title('ROC AUC - Multi-class')\n",
        "        plt.legend(loc='lower right')\n",
        "        plt.savefig(save_path)\n",
        "        plt.close()\n",
        "        logger.info(f\"Saved ROC AUC plot as '{save_path}'\")\n",
        "\n",
        "        return roc_auc\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error computing ROC AUC: {e}\")\n",
        "        return None\n",
        "\n",
        "# -------------------------------------------------------------------\n",
        "# 5. Custom Keras Metric for F1\n",
        "# -------------------------------------------------------------------\n",
        "class F1Score(Metric):\n",
        "    \"\"\"\n",
        "    Custom TensorFlow/Keras metric for multi-class F1 score.\n",
        "    Computes the average F1 across all classes.\n",
        "    \"\"\"\n",
        "    def __init__(self, name='f1_score', num_classes=5, **kwargs):\n",
        "        super(F1Score, self).__init__(name=name, **kwargs)\n",
        "        self.num_classes = num_classes\n",
        "        self.true_positives = self.add_weight(name='tp', shape=(self.num_classes,), initializer='zeros')\n",
        "        self.false_positives = self.add_weight(name='fp', shape=(self.num_classes,), initializer='zeros')\n",
        "        self.false_negatives = self.add_weight(name='fn', shape=(self.num_classes,), initializer='zeros')\n",
        "\n",
        "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
        "        # Convert predictions and labels to integer labels\n",
        "        y_pred = K.argmax(y_pred, axis=1)\n",
        "        y_true = K.cast(y_true, 'int32')\n",
        "\n",
        "        # One-hot encoding\n",
        "        y_true_one_hot = tf.one_hot(y_true, depth=self.num_classes)\n",
        "        y_pred_one_hot = tf.one_hot(y_pred, depth=self.num_classes)\n",
        "\n",
        "        # Calculate true positives, false positives, and false negatives\n",
        "        tp = tf.reduce_sum(y_true_one_hot * y_pred_one_hot, axis=0)\n",
        "        fp = tf.reduce_sum((1 - y_true_one_hot) * y_pred_one_hot, axis=0)\n",
        "        fn = tf.reduce_sum(y_true_one_hot * (1 - y_pred_one_hot), axis=0)\n",
        "\n",
        "        self.true_positives.assign_add(tp)\n",
        "        self.false_positives.assign_add(fp)\n",
        "        self.false_negatives.assign_add(fn)\n",
        "\n",
        "    def result(self):\n",
        "        precision = self.true_positives / (self.true_positives + self.false_positives + K.epsilon())\n",
        "        recall = self.true_positives / (self.true_positives + self.false_negatives + K.epsilon())\n",
        "        f1 = 2 * ((precision * recall) / (precision + recall + K.epsilon()))\n",
        "        return tf.reduce_mean(f1)\n",
        "\n",
        "    def reset_states(self):\n",
        "        self.true_positives.assign(tf.zeros_like(self.true_positives))\n",
        "        self.false_positives.assign(tf.zeros_like(self.false_positives))\n",
        "        self.false_negatives.assign(tf.zeros_like(self.false_negatives))\n",
        "\n",
        "# -------------------------------------------------------------------\n",
        "# 6. ModelInterpretability (LIME, etc.)\n",
        "# -------------------------------------------------------------------\n",
        "class ModelInterpretability:\n",
        "    \"\"\"\n",
        "    Handles SHAP, LIME, and other interpretability analyses.\n",
        "    This example only implements LIME.\n",
        "    \"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def lime_analysis(model, X_train, y_train, X_test, y_test, feature_names, model_type):\n",
        "        \"\"\"\n",
        "        Perform LIME analysis to interpret model predictions for a given model type.\n",
        "\n",
        "        Parameters:\n",
        "        - model: Trained model object.\n",
        "        - X_train: Training features.\n",
        "        - y_train: Training labels.\n",
        "        - X_test: Test features.\n",
        "        - y_test: Test labels.\n",
        "        - feature_names: List of feature names.\n",
        "        - model_type: Type of the model ( 'LSTM', 'CNN', 'Transformer', etc.).\n",
        "        \"\"\"\n",
        "        try:\n",
        "            os.makedirs('visualizations', exist_ok=True)\n",
        "\n",
        "            if model_type in ['LSTM', 'CNN', 'Transformer']:\n",
        "                # Define a closure that reshapes 2D input back to 3D\n",
        "                def predict_fn_3d(data_2d):\n",
        "                    n_samples = data_2d.shape[0]\n",
        "                    win_size = X_test.shape[1]\n",
        "                    n_feats = X_test.shape[2]\n",
        "                    return model.predict(data_2d.reshape(n_samples, win_size, n_feats))\n",
        "\n",
        "                # Reshape for LIME\n",
        "                X_train_lime = X_train.reshape(X_train.shape[0], -1)\n",
        "                X_test_lime = X_test.reshape(X_test.shape[0], -1)\n",
        "\n",
        "                # Adjust feature names\n",
        "                feature_names = [f'Feature_{i+1}' for i in range(X_train_lime.shape[1])]\n",
        "                explain_func = predict_fn_3d\n",
        "\n",
        "            else:\n",
        "                logger.warning(f\"Model type '{model_type}' is not supported for LIME analysis.\")\n",
        "                return\n",
        "\n",
        "            # Ensure feature_names matches the final shape\n",
        "            if len(feature_names) != X_train_lime.shape[1]:\n",
        "                logger.warning(\"Mismatch between feature_names and data shape. Adjusting automatically.\")\n",
        "                feature_names = [f'Feature_{i+1}' for i in range(X_train_lime.shape[1])]\n",
        "\n",
        "            # Initialize LIME\n",
        "            explainer = lime_tabular.LimeTabularExplainer(\n",
        "                training_data=X_train_lime,\n",
        "                feature_names=feature_names,\n",
        "                class_names=[f\"Class {cls}\" for cls in np.unique(y_train)],\n",
        "                mode='classification'\n",
        "            )\n",
        "\n",
        "            # Choose a few random samples from X_test for demonstration\n",
        "            sample_size = min(3, len(X_test_lime))\n",
        "            if sample_size == 0:\n",
        "                logger.warning(\"No samples available for LIME analysis.\")\n",
        "                return\n",
        "\n",
        "            np.random.seed(42)\n",
        "            indices = np.random.choice(len(X_test_lime), size=sample_size, replace=False)\n",
        "\n",
        "            for sample_idx in indices:\n",
        "                explanation = explainer.explain_instance(\n",
        "                    X_test_lime[sample_idx],\n",
        "                    explain_func,\n",
        "                    num_features=10,\n",
        "                    top_labels=1\n",
        "                )\n",
        "\n",
        "                # Save explanation to HTML\n",
        "                explanation.save_to_file(\n",
        "                    f\"visualizations/LIME_Explanation_{model_type}_Sample_{sample_idx}.html\"\n",
        "                )\n",
        "                logger.info(\n",
        "                    f\"Saved LIME explanation for {model_type}, Sample {sample_idx} -> \"\n",
        "                    f\"visualizations/LIME_Explanation_{model_type}_Sample_{sample_idx}.html\"\n",
        "                )\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error during LIME analysis for {model_type}: {e}\", exc_info=True)\n",
        "\n",
        "# -------------------------------------------------------------------\n",
        "# 7. UtilityManager\n",
        "# -------------------------------------------------------------------\n",
        "class BaseManager:\n",
        "    \"\"\"Minimal placeholder base class if needed.\"\"\"\n",
        "    pass\n",
        "\n",
        "class UtilityManager(BaseManager):\n",
        "    \"\"\"\n",
        "    Contains utility functions used across various components.\n",
        "    selecting a small number of samples per class for interpretability.\n",
        "    \"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def select_three_samples_per_class(X, y, samples_per_class=3, num_classes=None):\n",
        "        \"\"\"\n",
        "        Selects a specified number of samples per class for interpretability analyses.\n",
        "\n",
        "        Parameters:\n",
        "            X (np.ndarray): Feature array, shape (n_samples, ...).\n",
        "            y (np.ndarray): Labels array, shape (n_samples,).\n",
        "            samples_per_class (int): Number of samples to select per class.\n",
        "            num_classes (int, optional): If provided, checks consistency.\n",
        "\n",
        "        Returns:\n",
        "            selected_X (np.ndarray): Selected feature samples.\n",
        "            selected_y (np.ndarray): Corresponding labels.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            selected_X = []\n",
        "            selected_y = []\n",
        "            unique_classes = np.unique(y)\n",
        "\n",
        "            if num_classes is not None and len(unique_classes) != num_classes:\n",
        "                logger.warning(f\"Expected {num_classes} classes, found {len(unique_classes)} unique classes.\")\n",
        "\n",
        "            # Fix random seed for reproducibility\n",
        "            np.random.seed(42)\n",
        "\n",
        "            for cls in unique_classes:\n",
        "                cls_indices = np.where(y == cls)[0]\n",
        "                if len(cls_indices) >= samples_per_class:\n",
        "                    selected_indices = np.random.choice(cls_indices, samples_per_class, replace=False)\n",
        "                elif len(cls_indices) > 0:\n",
        "                    selected_indices = cls_indices\n",
        "                    logger.warning(f\"Class {cls} has fewer than {samples_per_class} samples. Selecting all.\")\n",
        "                else:\n",
        "                    logger.warning(f\"Class {cls} has no samples. Skipping.\")\n",
        "                    continue\n",
        "\n",
        "                selected_X.append(X[selected_indices])\n",
        "                selected_y.extend([cls] * len(selected_indices))\n",
        "                logger.info(f\"Selected {len(selected_indices)} samples for class {cls}.\")\n",
        "\n",
        "            if selected_X:\n",
        "                return np.vstack(selected_X), np.array(selected_y)\n",
        "            else:\n",
        "                logger.error(\"No samples were selected. Please check the input data.\")\n",
        "                return np.array([]), np.array([])\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error selecting 3 samples per class: {e}\")\n",
        "            return None, None\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------------\n",
        "# 8. FeatureExtractor\n",
        "# -------------------------------------------------------------------\n",
        "class FeatureExtractor:\n",
        "    \"\"\"\n",
        "    Handles feature extraction and sequence creation from raw data.\n",
        "    Also aligns sequences via DTW if needed.\n",
        "    \"\"\"\n",
        "    @staticmethod\n",
        "    def create_sequences(data, window_size, overlap=0.50, labels=None):\n",
        "        \"\"\"\n",
        "        Create overlapping sequences from the feature data.\n",
        "        Optionally recompute labels for each sequence.\n",
        "\n",
        "        Returns:\n",
        "            If labels is provided:\n",
        "                tuple: (sequences, sequence_labels, center_indices)\n",
        "            Otherwise:\n",
        "                tuple: (sequences, None, center_indices)\n",
        "        \"\"\"\n",
        "        try:\n",
        "            step = max(int(window_size * (1 - overlap)), 1)\n",
        "            sequences = []\n",
        "            sequence_labels = [] if labels is not None else None\n",
        "            center_indices = []\n",
        "            for i in tqdm(range(0, len(data) - window_size + 1, step), desc=\"Creating Sequences\"):\n",
        "                seq = data[i : i + window_size]\n",
        "                sequences.append(seq)\n",
        "                center_index = i + window_size // 2\n",
        "                center_indices.append(center_index)\n",
        "                if labels is not None:\n",
        "                    sequence_labels.append(labels[center_index])\n",
        "            sequences = np.array(sequences)\n",
        "            if labels is not None:\n",
        "                sequence_labels = np.array(sequence_labels)\n",
        "                logger.info(f\"Created sequences: {sequences.shape} with sequence labels: {sequence_labels.shape}\")\n",
        "                return sequences, sequence_labels, center_indices\n",
        "            else:\n",
        "                logger.info(f\"Created sequences: {sequences.shape}\")\n",
        "                return sequences, None, center_indices\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error during sequence creation: {e}\")\n",
        "            return None, None, None\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def align_sequences_with_dtw(sequences, target_length=None):\n",
        "        \"\"\"\n",
        "        Align sequences to a reference sequence using Dynamic Time Warping (DTW)\n",
        "        with slope constraint P=1 and weighting coefficients (Sakoe & Chiba, 1978).\n",
        "\n",
        "        Args:\n",
        "            sequences (np.ndarray): (num_sequences, window_size, num_features).\n",
        "            target_length (int, optional): Desired length for all aligned sequences.\n",
        "\n",
        "        Returns:\n",
        "            np.ndarray: Aligned sequences, shape (num_sequences, target_length, num_features).\n",
        "        \"\"\"\n",
        "        try:\n",
        "            if sequences is None or sequences.ndim != 3:\n",
        "                logger.error(\"Invalid input to align_sequences_with_dtw. Must be 3D array.\")\n",
        "                return sequences\n",
        "\n",
        "            # Scale the sequences (normalization)\n",
        "            scaler = TimeSeriesScalerMeanVariance()\n",
        "            num_sequences, window_size, num_features = sequences.shape\n",
        "            sequences = scaler.fit_transform(sequences)\n",
        "\n",
        "            # Choose the reference sequence (first sequence)\n",
        "            reference = sequences[0]\n",
        "            reference_length = reference.shape[0]\n",
        "            if target_length is None:\n",
        "                target_length = reference_length\n",
        "\n",
        "            aligned = []\n",
        "\n",
        "            for idx, seq in enumerate(sequences):\n",
        "                if idx == 0:\n",
        "                    # The first sequence is our reference, no need to align\n",
        "                    aligned.append(seq)\n",
        "                    continue\n",
        "\n",
        "                if seq.ndim != 2 or reference.ndim != 2:\n",
        "                    logger.error(f\"Sequence {idx} or reference is not 2D. Skipping.\")\n",
        "                    aligned.append(seq)\n",
        "                    continue\n",
        "\n",
        "                try:\n",
        "                    # Compute DTW path with Sakoe & Chiba slope constraint (P=1)\n",
        "                    alignment_path, distance = dtw_path(seq, reference, global_constraint=\"sakoe_chiba\", sakoe_chiba_radius=1)\n",
        "                except Exception as e:\n",
        "                    logger.error(f\"DTW error on sequence {idx}: {e}\")\n",
        "                    aligned.append(seq)\n",
        "                    continue\n",
        "\n",
        "                # Extract DTW alignment indices\n",
        "                seq_indices, ref_indices = zip(*alignment_path)\n",
        "                seq_indices = np.array(seq_indices)\n",
        "                ref_indices = np.array(ref_indices)\n",
        "\n",
        "                # Introduce weighting coefficients (Sakoe & Chiba)\n",
        "                weight_matrix = np.ones_like(seq_indices)  # Default weight 1\n",
        "                diagonal_moves = (np.diff(seq_indices) == 1) & (np.diff(ref_indices) == 1)\n",
        "                weight_matrix[1:][diagonal_moves] = 2  # Diagonal moves get weight 2\n",
        "\n",
        "                # Remove duplicates for interpolation\n",
        "                unique_ref, unique_inds = np.unique(ref_indices, return_index=True)\n",
        "                unique_seq = seq_indices[unique_inds]\n",
        "                weight_matrix = weight_matrix[unique_inds]  # Apply weight only to unique points\n",
        "\n",
        "                if len(unique_ref) < 2:\n",
        "                    logger.warning(f\"Not enough unique points for interpolation in seq {idx}. Using original.\")\n",
        "                    aligned.append(seq)\n",
        "                    continue\n",
        "\n",
        "                try:\n",
        "                    # Interpolation function for time-warping\n",
        "                    f = interp1d(unique_ref, unique_seq, kind='linear', fill_value=\"extrapolate\")\n",
        "                except Exception as e:\n",
        "                    logger.error(f\"Error creating interpolation function for seq {idx}: {e}\")\n",
        "                    aligned.append(seq)\n",
        "                    continue\n",
        "\n",
        "                new_ref_indices = np.linspace(0, reference_length - 1, target_length)\n",
        "\n",
        "                try:\n",
        "                    new_seq_indices = f(new_ref_indices)\n",
        "                except Exception as e:\n",
        "                    logger.error(f\"Interpolation error for seq {idx}: {e}\")\n",
        "                    aligned.append(seq)\n",
        "                    continue\n",
        "\n",
        "                # Round & clip to valid indices\n",
        "                new_seq_indices = np.round(new_seq_indices).astype(int)\n",
        "                new_seq_indices = np.clip(new_seq_indices, 0, window_size - 1)\n",
        "\n",
        "                # Apply warping transformation\n",
        "                warped_seq = seq[new_seq_indices]\n",
        "                aligned.append(warped_seq)\n",
        "\n",
        "            aligned = np.array(aligned)\n",
        "            logger.info(f\"Aligned sequences shape: {aligned.shape}\")\n",
        "            return aligned\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error during DTW alignment: {e}\")\n",
        "            return sequences  # fallback\n",
        "\n",
        "class VisualizationManager(BaseManager):\n",
        "    \"\"\"Handles all visualization tasks.\"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def visualize_sequences_creation(X_sequences, num_sequences=5):\n",
        "        \"\"\"\n",
        "        Visualizes a few sequences to understand the windowing process.\n",
        "\n",
        "        This method plots the first feature of selected sequences over time to illustrate how sequences\n",
        "        are created from the data and saves the plot as a PNG file.\n",
        "\n",
        "        Args:\n",
        "            X_sequences (np.ndarray): Sequences of shape (n_sequences, window_size, n_features).\n",
        "            num_sequences (int, optional): Number of sequences to visualize. Defaults to 5.\n",
        "\n",
        "        Returns:\n",
        "            None\n",
        "\n",
        "        Raises:\n",
        "            Exception: If there is an error during the visualization process.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            logger.info(f\"Visualizing {num_sequences} sequences with shape {X_sequences.shape}\")\n",
        "            window_size = X_sequences.shape[1]\n",
        "            features = X_sequences.shape[2]\n",
        "            plt.figure(figsize=(15, num_sequences * 3))\n",
        "            for i in range(num_sequences):\n",
        "                plt.subplot(num_sequences, 1, i+1)\n",
        "                # Plot the first feature for simplicity\n",
        "                plt.plot(X_sequences[i, :, 0], label='Feature 1')\n",
        "                plt.title(f'Sequence {i+1}')\n",
        "                plt.xlabel('Time Step')\n",
        "                plt.ylabel('Feature Value')\n",
        "                plt.legend()\n",
        "            plt.tight_layout()\n",
        "            plt.savefig(\"visualizations/Sequences_Creation.png\")\n",
        "            plt.close()\n",
        "            logger.info(\"Saved Sequences Creation visualization as 'visualizations/Sequences_Creation.png'\")\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error during Sequences Creation visualization: {e}\", exc_info=True)\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def visualize_dtw_alignment(X_sequences, X_aligned, num_sequences=3):\n",
        "        \"\"\"\n",
        "        Visualizes a few sequences before and after DTW alignment.\n",
        "\n",
        "        This method plots the first feature of selected sequences in their original and aligned forms\n",
        "        side by side to demonstrate the effect of Dynamic Time Warping (DTW) alignment and saves the plot\n",
        "        as a PNG file.\n",
        "\n",
        "        Args:\n",
        "            X_sequences (np.ndarray): Original sequences of shape (n_sequences, window_size, n_features).\n",
        "            X_aligned (np.ndarray): Aligned sequences of shape (n_sequences, window_size, n_features).\n",
        "            num_sequences (int, optional): Number of sequences to visualize. Defaults to 3.\n",
        "\n",
        "        Returns:\n",
        "            None\n",
        "\n",
        "        Raises:\n",
        "            Exception: If there is an error during the visualization process.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            import numpy as np\n",
        "            window_size = X_sequences.shape[1]\n",
        "            features = X_sequences.shape[2]\n",
        "            plt.figure(figsize=(15, num_sequences * 6))\n",
        "            for i in range(num_sequences):\n",
        "                plt.subplot(num_sequences, 2, 2*i+1)\n",
        "                plt.plot(X_sequences[i, :, 0], label='Original')\n",
        "                plt.title(f'Sequence {i+1} - Original')\n",
        "                plt.xlabel('Time Step')\n",
        "                plt.ylabel('Feature 1')\n",
        "                plt.legend()\n",
        "\n",
        "                plt.subplot(num_sequences, 2, 2*i+2)\n",
        "                plt.plot(X_aligned[i, :, 0], label='Aligned', color='orange')\n",
        "                plt.title(f'Sequence {i+1} - Aligned')\n",
        "                plt.xlabel('Time Step')\n",
        "                plt.ylabel('Feature 1')\n",
        "                plt.legend()\n",
        "            plt.tight_layout()\n",
        "            plt.savefig(\"visualizations/DTW_Alignment.png\")\n",
        "            plt.close()\n",
        "            logger.info(\"Saved DTW Alignment visualization as 'visualizations/DTW_Alignment.png'\")\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error during DTW Alignment visualization: {e}\")\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def visualize_hyperparameter_tuning_results(lstm_cv_results, cnn_cv_results, transformer_cv_results):\n",
        "        \"\"\"\n",
        "        Visualizes the average F1-Score for each hyperparameter tuning strategy across different models.\n",
        "\n",
        "        This method creates a grouped bar chart comparing the average F1-Scores achieved by different\n",
        "        hyperparameter tuning strategies (e.g., Bayesian Optimization, Random Search, Hyperband) for\n",
        "        LSTM, CNN, and Transformer models, and saves the plot as an interactive HTML file.\n",
        "\n",
        "        Args:\n",
        "            lstm_cv_results (dict): Cross-validation results for the LSTM model, keyed by strategy.\n",
        "            cnn_cv_results (dict): Cross-validation results for the CNN model, keyed by strategy.\n",
        "            transformer_cv_results (dict): Cross-validation results for the Transformer model, keyed by strategy.\n",
        "\n",
        "        Returns:\n",
        "            None\n",
        "\n",
        "        Raises:\n",
        "            Exception: If there is an error during the visualization process.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            # Plot average F1-Score for each strategy\n",
        "            strategies = ['BayesianOptimization', 'RandomSearch', 'Hyperband']\n",
        "            lstm_f1 = [np.mean([res['F1-Score'] for res in lstm_cv_results[strategy]]) for strategy in strategies]\n",
        "            cnn_f1 = [np.mean([res['F1-Score'] for res in cnn_cv_results[strategy]]) for strategy in strategies]\n",
        "            transformer_f1 = [np.mean([res['F1-Score'] for res in transformer_cv_results[strategy]]) for strategy in strategies]\n",
        "\n",
        "            df = pd.DataFrame({\n",
        "                'Strategy': strategies,\n",
        "                'LSTM_F1_Score': lstm_f1,\n",
        "                'CNN_F1_Score': cnn_f1,\n",
        "                'Transformer_F1_Score': transformer_f1\n",
        "            })\n",
        "\n",
        "            fig = px.bar(df, x='Strategy', y=['LSTM_F1_Score', 'CNN_F1_Score', 'Transformer_F1_Score'],\n",
        "                         barmode='group', title='Average F1-Score by Hyperparameter Tuning Strategy')\n",
        "            fig.write_html(\"visualizations/Hyperparameter_Tuning_F1_Score.html\")\n",
        "            logger.info(\"Saved Hyperparameter Tuning F1-Score visualization as 'visualizations/Hyperparameter_Tuning_F1_Score.html'\")\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error during Hyperparameter Tuning Results visualization: {e}\")\\\n",
        "\n",
        "    @staticmethod\n",
        "    def visualize_model_performance_comparison(metrics_evaluation):\n",
        "        \"\"\"\n",
        "        Visualizes and compares the performance metrics of different models.\n",
        "\n",
        "        This method creates a grouped bar chart comparing Accuracy, F1-Score, and ROC AUC of various models\n",
        "        and saves the plot as an interactive HTML file.\n",
        "\n",
        "        Args:\n",
        "            metrics_evaluation (dict): Dictionary containing performance metrics for each model.\n",
        "                                       Example format:\n",
        "                                       {\n",
        "                                           'Model_A': {'Accuracy': 0.95, 'F1-Score': 0.93, 'ROC_AUC': 0.96},\n",
        "                                           'Model_B': {'Accuracy': 0.90, 'F1-Score': 0.88, 'ROC_AUC': 0.92},\n",
        "                                           ...\n",
        "                                       }\n",
        "\n",
        "        Returns:\n",
        "            None\n",
        "\n",
        "        Raises:\n",
        "            KeyError: If expected keys are missing in `metrics_evaluation`.\n",
        "            Exception: If there is an error during the visualization process.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            df = pd.DataFrame({\n",
        "                'Model': list(metrics_evaluation.keys()),\n",
        "                'Accuracy': [metrics_evaluation[model]['Accuracy'] for model in metrics_evaluation],\n",
        "                'F1_Score': [metrics_evaluation[model]['F1-Score'] for model in metrics_evaluation],\n",
        "                'ROC_AUC': [metrics_evaluation[model]['ROC_AUC'] for model in metrics_evaluation]\n",
        "            })\n",
        "\n",
        "            df_melted = df.melt(id_vars='Model', value_vars=['Accuracy', 'F1_Score', 'ROC_AUC'],\n",
        "                                var_name='Metric', value_name='Score')\n",
        "\n",
        "            fig = px.bar(\n",
        "                df_melted,\n",
        "                x='Model',\n",
        "                y='Score',\n",
        "                color='Metric',\n",
        "                barmode='group',\n",
        "                title='Model Performance Comparison',\n",
        "                labels={\n",
        "                    'Score': 'Score',\n",
        "                    'Metric': 'Metric',\n",
        "                    'Model': 'Model'\n",
        "                }\n",
        "            )\n",
        "\n",
        "            fig.write_html(\"visualizations/Model_Performance_Comparison.html\")\n",
        "            logger.info(\"Saved Model Performance Comparison visualization as 'visualizations/Model_Performance_Comparison.html'\")\n",
        "        except KeyError as ke:\n",
        "            logger.error(f\"Key error during Model Performance Comparison visualization: {ke}\")\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error during Model Performance Comparison visualization: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "24n95NCVAhxL"
      },
      "outputs": [],
      "source": [
        "# -------------------------------------------------------------------\n",
        "# Setup logger\n",
        "# -------------------------------------------------------------------\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# -------------------------------------------------------------------\n",
        "# CONFIGURATION\n",
        "# -------------------------------------------------------------------\n",
        "\n",
        "CONFIG = {\n",
        "    'window_size': 90,\n",
        "    'overlap': 0.99,\n",
        "    'random_seed': 42,\n",
        "    'epochs': 1,\n",
        "    'batch_size': 32,\n",
        "    'early_stopping_patience': 3,\n",
        "    'tuner_trials': 1  # number of trials for hyperparameter tuning\n",
        "}\n",
        "\n",
        "# -------------------------------------------------------------------\n",
        "# MAIN PIPELINE\n",
        "# -------------------------------------------------------------------\n",
        "\n",
        "def main_pipeline():\n",
        "    logger = logging.getLogger(__name__)\n",
        "    metrics = {}\n",
        "\n",
        "    # 1. Load CSV and Prepare Data\n",
        "    logger.info(\"Reading CSV file and preparing data...\")\n",
        "    df = pd.read_csv(\"/content/final_labels_cleaned.csv\")\n",
        "\n",
        "    # Define the stride intervals for filtering the 'Frame' column\n",
        "    STRIDE_INTERVALS = [\n",
        "        (0, 5937), (5938, 9521), (15938, 18914),\n",
        "        (25059, 28499), (28500, 31683), (35059, 38099),\n",
        "        (38100, 41203), (47684, 50606), (56885, 59990),\n",
        "        (66006, 69046), (75526, 79271), (85384, 88231),\n",
        "        (88232, 91271), (96936, 99848), (99849, 105864),\n",
        "        (105865, 108570), (115114, 118026), (118027, 121067),\n",
        "        (127611, 130587), (135852, 138698), (138699, 141516),\n",
        "        (147147, 150252), (150253, 152508), (152509, 155726)\n",
        "    ]\n",
        "\n",
        "    # Build a boolean mask to filter rows based on the 'Frame' column\n",
        "    mask = np.zeros(len(df), dtype=bool)\n",
        "    for low, high in STRIDE_INTERVALS:\n",
        "        mask |= (df['Frame'] >= low) & (df['Frame'] <= high)\n",
        "\n",
        "    df = df[mask]\n",
        "    logger.info(f\"Filtered data shape: {df.shape}\")\n",
        "\n",
        "    # Use only the synergy columns as features; ignore Frame and Time (Seconds)\n",
        "    X = df[[\"Synergy_1\", \"Synergy_2\", \"Synergy_3\", \"Synergy_4\", \"Synergy_5\"]].values\n",
        "\n",
        "    # Process target labels\n",
        "    y_raw = df[\"PredictedStage\"].values\n",
        "    label_encoder = LabelEncoder()\n",
        "    labels_encoded = label_encoder.fit_transform(y_raw)\n",
        "    num_classes = len(np.unique(labels_encoded))\n",
        "    logger.info(f\"Number of classes: {num_classes}\")\n",
        "\n",
        "    # 2. Create Sequences for Model\n",
        "    logger.info(\"Creating input sequences for the model...\")\n",
        "    try:\n",
        "        X_sequences, seq_labels, center_indices = FeatureExtractor.create_sequences(X, window_size=CONFIG['window_size'], overlap=CONFIG['overlap'], labels=labels_encoded)        # visualization for sequence creation\n",
        "        VisualizationManager.visualize_sequences_creation(X_sequences, num_sequences=5)\n",
        "        if X_sequences is None or X_sequences.shape[0] < 5:\n",
        "            logger.error(\"Insufficient number of sequences created.\")\n",
        "            return\n",
        "        logger.info(f\"Created input sequences: {X_sequences.shape}\")\n",
        "\n",
        "        logger.info(\"Aligning sequences using DTW...\")\n",
        "        try:\n",
        "            X_aligned = FeatureExtractor.align_sequences_with_dtw(X_sequences)\n",
        "            # Visualize DTW alignment immediately after obtaining aligned sequences\n",
        "            VisualizationManager.visualize_dtw_alignment(X_sequences, X_aligned, num_sequences=3)\n",
        "            logger.info(f\"Aligned sequences shape: {X_aligned.shape}\")\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error during DTW alignment: {e}\")\n",
        "            X_aligned = X_sequences\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error during sequence creation: {e}\")\n",
        "        return\n",
        "    X_final = X_aligned\n",
        "\n",
        "    # 3. Select Samples for SHAP/LIME (Optional)\n",
        "    logger.info(\"Selecting 3 samples per class for SHAP and LIME analyses...\")\n",
        "    try:\n",
        "        X_selected_samples, y_selected_samples = UtilityManager.select_three_samples_per_class(X_final, labels_encoded)\n",
        "        if X_selected_samples is None or y_selected_samples is None or X_selected_samples.size == 0:\n",
        "            logger.error(\"No samples selected for SHAP/LIME analyses.\")\n",
        "            return\n",
        "        logger.info(f\"Selected samples: {X_selected_samples.shape}, labels: {y_selected_samples.shape}\")\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error during sample selection: {e}\")\n",
        "        return\n",
        "\n",
        "    # 4. Train-Test Split\n",
        "    logger.info(\"Splitting data into training and testing sets...\")\n",
        "    try:\n",
        "        X_train_nn, X_test_nn, y_train, y_test = train_test_split(\n",
        "            X_final, seq_labels,\n",
        "            test_size=0.2, stratify=seq_labels,\n",
        "            random_state=CONFIG['random_seed']\n",
        "        )\n",
        "        logger.info(f\"Training set: {X_train_nn.shape}, Testing set: {X_test_nn.shape}\")\n",
        "        # Flatten for RF if needed\n",
        "        if X_train_nn.ndim == 3:\n",
        "            X_train_split = X_train_nn.reshape(X_train_nn.shape[0], -1)\n",
        "        else:\n",
        "            X_train_split = X_train_nn\n",
        "        if X_test_nn.ndim == 3:\n",
        "            X_test_split = X_test_nn.reshape(X_test_nn.shape[0], -1)\n",
        "        else:\n",
        "            X_test_split = X_test_nn\n",
        "        y_train_split = y_train\n",
        "        y_test_split = y_test\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error during train-test split: {e}\")\n",
        "        return\n",
        "\n",
        "    # 5. Compute Class Weights\n",
        "    logger.info(\"Computing class weights...\")\n",
        "    try:\n",
        "        classes = np.unique(labels_encoded)\n",
        "        class_weights = compute_class_weight(class_weight=\"balanced\", classes=classes, y=labels_encoded)\n",
        "        class_weight_dict = dict(zip(classes, class_weights))\n",
        "        logger.info(f\"Class weights: {class_weight_dict}\")\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error computing class weights: {e}\")\n",
        "        class_weight_dict = None\n",
        "\n",
        "    # 6. Oversample Training Data for Neural Networks\n",
        "    logger.info(\"Oversampling training data for neural networks...\")\n",
        "    try:\n",
        "        from collections import Counter\n",
        "        class_counts = Counter(y_train)\n",
        "        min_count = min(class_counts.values())\n",
        "        if min_count > 1:\n",
        "            n_neighbors = min(5, max(1, min_count - 1))\n",
        "            logger.info(f\"Applying SMOTE with n_neighbors={n_neighbors}\")\n",
        "            sm = SMOTE(random_state=CONFIG['random_seed'], k_neighbors=n_neighbors)\n",
        "            X_oversampled, y_train_res = sm.fit_resample(\n",
        "                X_train_nn.reshape(X_train_nn.shape[0], -1), y_train\n",
        "            )\n",
        "            time_steps = X_train_nn.shape[1]\n",
        "            n_feats = X_train_nn.shape[2]\n",
        "            X_oversampled = X_oversampled.reshape(-1, time_steps, n_feats)\n",
        "        else:\n",
        "            logger.warning(\"Using RandomOverSampler for neural network training.\")\n",
        "            ros = RandomOverSampler(random_state=CONFIG['random_seed'])\n",
        "            X_oversampled, y_train_res = ros.fit_resample(\n",
        "                X_train_nn.reshape(X_train_nn.shape[0], -1), y_train\n",
        "            )\n",
        "            time_steps = X_train_nn.shape[1]\n",
        "            n_feats = X_train_nn.shape[2]\n",
        "            X_oversampled = X_oversampled.reshape(-1, time_steps, n_feats)\n",
        "        logger.info(f\"Oversampled data shape: {X_oversampled.shape}, {y_train_res.shape}\")\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error during oversampling: {e}\")\n",
        "        return\n",
        "\n",
        "    # 7. Hyperparameter Tuning via Cross-Validation (Placeholders)\n",
        "    logger.info(\"Starting hyperparameter tuning (placeholders)...\")\n",
        "    tuning_strategies = [\"BayesianOptimization\", \"RandomSearch\", \"Hyperband\"]\n",
        "    lstm_cv_results = {strategy: [] for strategy in tuning_strategies}\n",
        "    cnn_cv_results = {strategy: [] for strategy in tuning_strategies}\n",
        "    transformer_cv_results = {strategy: [] for strategy in tuning_strategies}\n",
        "    outer_cv = StratifiedKFold(n_splits=2, shuffle=True, random_state=CONFIG[\"random_seed\"])\n",
        "    logger.info(\"Starting hyperparameter tuning (placeholders) for LSTM, CNN, Transformer...\")\n",
        "    for fold, (train_idx, val_idx) in enumerate(outer_cv.split(X_oversampled, y_train_res), 1):\n",
        "\n",
        "        logger.info(f\"=== Fold {fold} ===\")\n",
        "        X_train_fold = X_oversampled[train_idx]\n",
        "        X_val_fold = X_oversampled[val_idx]\n",
        "        y_train_fold = y_train_res[train_idx]\n",
        "        y_val_fold = y_train_res[val_idx]\n",
        "        try:\n",
        "            classes_fold = np.unique(y_train_fold)\n",
        "            cw_fold = compute_class_weight(class_weight=\"balanced\", classes=classes_fold, y=y_train_fold)\n",
        "            class_weight_dict_fold = dict(zip(classes_fold, cw_fold))\n",
        "        except:\n",
        "            class_weight_dict_fold = None\n",
        "        input_shape = (X_train_fold.shape[1], X_train_fold.shape[2])\n",
        "        fold_num_classes = len(np.unique(y_train_fold))\n",
        "\n",
        "\n",
        "        for strategy in tuning_strategies:\n",
        "            logger.info(f\"Tuning LSTM with {strategy} on fold {fold} (placeholder)...\")\n",
        "            try:\n",
        "                tuner, best_hps = hyperparameter_tuning(\n",
        "                    model_builder=LSTMModelBuilder(),\n",
        "                    tuner_type=strategy,\n",
        "                    X_train=X_train_fold, y_train=y_train_fold,\n",
        "                    X_val=X_val_fold, y_val=y_val_fold,\n",
        "                    input_shape=input_shape, num_classes=fold_num_classes,\n",
        "                    fold=fold, strategy=strategy\n",
        "                )\n",
        "                if tuner and best_hps:\n",
        "                    best_hps_dict = best_hps.values\n",
        "                    model = tuner.hypermodel.build(best_hps)\n",
        "                    model.fit(X_train_fold, y_train_fold,\n",
        "                              epochs=CONFIG[\"epochs\"],\n",
        "                              validation_data=(X_val_fold, y_val_fold),\n",
        "                              callbacks=[EarlyStopping(monitor=\"val_loss\", patience=CONFIG[\"early_stopping_patience\"], restore_best_weights=True),\n",
        "                                         ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=1, verbose=1)],\n",
        "                              batch_size=CONFIG[\"batch_size\"],\n",
        "                              class_weight=class_weight_dict_fold,\n",
        "                              verbose=1)\n",
        "                    y_pred_val = np.argmax(model.predict(X_val_fold), axis=1)\n",
        "                    y_pred_proba_val = model.predict(X_val_fold)\n",
        "                    acc = accuracy_score(y_val_fold, y_pred_val)\n",
        "                    prec = precision_score(y_val_fold, y_pred_val, average=\"weighted\", zero_division=0)\n",
        "                    rec = recall_score(y_val_fold, y_pred_val, average=\"weighted\", zero_division=0)\n",
        "                    f1_val = f1_score(y_val_fold, y_pred_val, average=\"weighted\", zero_division=0)\n",
        "                    roc_auc_val = roc_auc_score(y_val_fold, y_pred_proba_val, multi_class=\"ovr\", average=\"weighted\")\n",
        "                    lstm_cv_results[strategy].append({\n",
        "                        \"Fold\": fold, \"Accuracy\": acc, \"Precision\": prec, \"Recall\": rec,\n",
        "                        \"F1-Score\": f1_val, \"ROC_AUC\": roc_auc_val, \"Best_Hyperparameters\": best_hps_dict\n",
        "                    })\n",
        "            except Exception as e:\n",
        "                logger.error(f\"Error tuning LSTM with {strategy} on fold {fold}: {e}\")\n",
        "\n",
        "\n",
        "        for strategy in tuning_strategies:\n",
        "            logger.info(f\"Tuning CNN with {strategy} on fold {fold} (placeholder)...\")\n",
        "            try:\n",
        "                tuner, best_hps = hyperparameter_tuning(\n",
        "                    model_builder=CNNModelBuilder(),\n",
        "                    tuner_type=strategy,\n",
        "                    X_train=X_train_fold, y_train=y_train_fold,\n",
        "                    X_val=X_val_fold, y_val=y_val_fold,\n",
        "                    input_shape=input_shape, num_classes=fold_num_classes,\n",
        "                    fold=fold, strategy=strategy\n",
        "                )\n",
        "                if tuner and best_hps:\n",
        "                    best_hps_dict = best_hps.values\n",
        "                    model = tuner.hypermodel.build(best_hps)\n",
        "                    model.fit(X_train_fold, y_train_fold,\n",
        "                              epochs=CONFIG[\"epochs\"],\n",
        "                              validation_data=(X_val_fold, y_val_fold),\n",
        "                              callbacks=[EarlyStopping(monitor=\"val_loss\", patience=CONFIG[\"early_stopping_patience\"], restore_best_weights=True),\n",
        "                                         ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=1, verbose=1)],\n",
        "                              batch_size=CONFIG[\"batch_size\"],\n",
        "                              class_weight=class_weight_dict_fold,\n",
        "                              verbose=1)\n",
        "                    y_pred_val = np.argmax(model.predict(X_val_fold), axis=1)\n",
        "                    y_pred_proba_val = model.predict(X_val_fold)\n",
        "                    acc = accuracy_score(y_val_fold, y_pred_val)\n",
        "                    prec = precision_score(y_val_fold, y_pred_val, average=\"weighted\", zero_division=0)\n",
        "                    rec = recall_score(y_val_fold, y_pred_val, average=\"weighted\", zero_division=0)\n",
        "                    f1_val = f1_score(y_val_fold, y_pred_val, average=\"weighted\", zero_division=0)\n",
        "                    roc_auc_val = roc_auc_score(y_val_fold, y_pred_proba_val, multi_class=\"ovr\", average=\"weighted\")\n",
        "                    cnn_cv_results[strategy].append({\n",
        "                        \"Fold\": fold, \"Accuracy\": acc, \"Precision\": prec, \"Recall\": rec,\n",
        "                        \"F1-Score\": f1_val, \"ROC_AUC\": roc_auc_val, \"Best_Hyperparameters\": best_hps_dict\n",
        "                    })\n",
        "            except Exception as e:\n",
        "                logger.error(f\"Error tuning CNN with {strategy} on fold {fold}: {e}\")\n",
        "\n",
        "\n",
        "        for strategy in tuning_strategies:\n",
        "            logger.info(f\"Tuning Transformer with {strategy} on fold {fold} (placeholder)...\")\n",
        "            try:\n",
        "                tuner, best_hps = hyperparameter_tuning(\n",
        "                    model_builder=TransformerModelBuilder(),\n",
        "                    tuner_type=strategy,\n",
        "                    X_train=X_train_fold, y_train=y_train_fold,\n",
        "                    X_val=X_val_fold, y_val=y_val_fold,\n",
        "                    input_shape=input_shape, num_classes=fold_num_classes,\n",
        "                    fold=fold, strategy=strategy\n",
        "                )\n",
        "                if tuner and best_hps:\n",
        "                    best_hps_dict = best_hps.values\n",
        "                    model = tuner.hypermodel.build(best_hps)\n",
        "                    model.fit(X_train_fold, y_train_fold,\n",
        "                              epochs=CONFIG[\"epochs\"],\n",
        "                              validation_data=(X_val_fold, y_val_fold),\n",
        "                              callbacks=[EarlyStopping(monitor=\"val_loss\", patience=CONFIG[\"early_stopping_patience\"], restore_best_weights=True),\n",
        "                                         ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=1, verbose=1)],\n",
        "                              batch_size=CONFIG[\"batch_size\"],\n",
        "                              class_weight=class_weight_dict_fold,\n",
        "                              verbose=1)\n",
        "                    y_pred_val = np.argmax(model.predict(X_val_fold), axis=1)\n",
        "                    y_pred_proba_val = model.predict(X_val_fold)\n",
        "                    acc = accuracy_score(y_val_fold, y_pred_val)\n",
        "                    prec = precision_score(y_val_fold, y_pred_val, average=\"weighted\", zero_division=0)\n",
        "                    rec = recall_score(y_val_fold, y_pred_val, average=\"weighted\", zero_division=0)\n",
        "                    f1_val = f1_score(y_val_fold, y_pred_val, average=\"weighted\", zero_division=0)\n",
        "                    roc_auc_val = roc_auc_score(y_val_fold, y_pred_proba_val, multi_class=\"ovr\", average=\"weighted\")\n",
        "                    transformer_cv_results[strategy].append({\n",
        "                        \"Fold\": fold, \"Accuracy\": acc, \"Precision\": prec, \"Recall\": rec,\n",
        "                        \"F1-Score\": f1_val, \"ROC_AUC\": roc_auc_val, \"Best_Hyperparameters\": best_hps_dict\n",
        "                    })\n",
        "            except Exception as e:\n",
        "                logger.error(f\"Error tuning Transformer with {strategy} on fold {fold}: {e}\")\n",
        "\n",
        "    # 8. Aggregate Tuning Results\n",
        "    logger.info(\"Aggregating tuning results...\")\n",
        "    def aggregate_results(cv_results, model_type):\n",
        "        for strategy, results in cv_results.items():\n",
        "            if not results:\n",
        "                logger.warning(f\"No results for {model_type} using {strategy}.\")\n",
        "                continue\n",
        "            avg_acc = np.mean([r[\"Accuracy\"] for r in results])\n",
        "            avg_f1 = np.mean([r[\"F1-Score\"] for r in results])\n",
        "            avg_roc = np.mean([r[\"ROC_AUC\"] for r in results])\n",
        "            combined = 0.5 * avg_f1 + 0.5 * avg_roc\n",
        "            logger.info(f\"{model_type} - {strategy}: Avg Acc={avg_acc:.4f}, Avg F1={avg_f1:.4f}, Avg ROC_AUC={avg_roc:.4f}, Combined={combined:.4f}\")\n",
        "    logger.info(\"Aggregating LSTM Results:\")\n",
        "    aggregate_results(lstm_cv_results, \"LSTM\")\n",
        "    logger.info(\"Aggregating CNN Results:\")\n",
        "    aggregate_results(cnn_cv_results, \"CNN\")\n",
        "    logger.info(\"Aggregating Transformer Results:\")\n",
        "    aggregate_results(transformer_cv_results, \"Transformer\")\n",
        "\n",
        "    # 9. Select Best Hyperparameter Strategy for Each Model\n",
        "    logger.info(\"Selecting best hyperparameter strategy for each model...\")\n",
        "    best_lstm_strategy = None; best_cnn_strategy = None; best_transformer_strategy = None\n",
        "    best_lstm_score = -np.inf; best_cnn_score = -np.inf; best_transformer_score = -np.inf\n",
        "    for strategy, results in lstm_cv_results.items():\n",
        "        if results:\n",
        "            combined = 0.5 * np.mean([r[\"F1-Score\"] for r in results]) + 0.5 * np.mean([r[\"ROC_AUC\"] for r in results])\n",
        "            if combined > best_lstm_score:\n",
        "                best_lstm_score = combined\n",
        "                best_lstm_strategy = strategy\n",
        "    for strategy, results in cnn_cv_results.items():\n",
        "        if results:\n",
        "            combined = 0.5 * np.mean([r[\"F1-Score\"] for r in results]) + 0.5 * np.mean([r[\"ROC_AUC\"] for r in results])\n",
        "            if combined > best_cnn_score:\n",
        "                best_cnn_score = combined\n",
        "                best_cnn_strategy = strategy\n",
        "    for strategy, results in transformer_cv_results.items():\n",
        "        if results:\n",
        "            combined = 0.5 * np.mean([r[\"F1-Score\"] for r in results]) + 0.5 * np.mean([r[\"ROC_AUC\"] for r in results])\n",
        "            if combined > best_transformer_score:\n",
        "                best_transformer_score = combined\n",
        "                best_transformer_strategy = strategy\n",
        "    logger.info(f\"Best LSTM Strategy: {best_lstm_strategy} (score={best_lstm_score:.4f})\")\n",
        "    logger.info(f\"Best CNN Strategy: {best_cnn_strategy} (score={best_cnn_score:.4f})\")\n",
        "    logger.info(f\"Best Transformer Strategy: {best_transformer_strategy} (score={best_transformer_score:.4f})\")\n",
        "    VisualizationManager.visualize_hyperparameter_tuning_results(lstm_cv_results, cnn_cv_results, transformer_cv_results)\n",
        "\n",
        "    # 10. Retrain Final Models on Entire Oversampled Data\n",
        "    logger.info(\"Retraining final models on entire oversampled dataset...\")\n",
        "    lstm_models = {}; cnn_best_model = None; transformer_best_model = None\n",
        "    time_steps = X_oversampled.shape[1]; n_feats = X_oversampled.shape[2]\n",
        "    if best_lstm_strategy and lstm_cv_results[best_lstm_strategy]:\n",
        "        try:\n",
        "            best_lstm_fold = max(lstm_cv_results[best_lstm_strategy], key=lambda r: r[\"F1-Score\"])\n",
        "            best_hps_dict = best_lstm_fold[\"Best_Hyperparameters\"]\n",
        "        except:\n",
        "            best_hps_dict = {}\n",
        "        logger.info(f\"LSTM best hyperparameters: {best_hps_dict}\")\n",
        "        lstm_model_builder = LSTMModelBuilder()\n",
        "        final_lstm_model = lstm_model_builder.build_lstm_model_final(best_hps_dict, (time_steps, n_feats), num_classes)\n",
        "        final_lstm_model.fit(X_oversampled, y_train_res,\n",
        "                             epochs=CONFIG[\"epochs\"],\n",
        "                             validation_split=0.2,\n",
        "                             callbacks=[EarlyStopping(monitor=\"val_loss\", patience=CONFIG[\"early_stopping_patience\"], restore_best_weights=True),\n",
        "                                        ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=1, verbose=1)],\n",
        "                             batch_size=CONFIG[\"batch_size\"],\n",
        "                             class_weight=class_weight_dict,\n",
        "                             verbose=1)\n",
        "        lstm_models[best_lstm_strategy] = final_lstm_model\n",
        "        final_lstm_model.save(f\"models/LSTM_{best_lstm_strategy}_best_model.h5\")\n",
        "        logger.info(f\"Saved final LSTM model: models/LSTM_{best_lstm_strategy}_best_model.h5\")\n",
        "    if best_cnn_strategy and cnn_cv_results[best_cnn_strategy]:\n",
        "        try:\n",
        "            best_cnn_fold = max(cnn_cv_results[best_cnn_strategy], key=lambda r: r[\"F1-Score\"])\n",
        "            best_hps_dict = best_cnn_fold[\"Best_Hyperparameters\"]\n",
        "        except:\n",
        "            best_hps_dict = {}\n",
        "        cnn_model_builder = CNNModelBuilder()\n",
        "        final_cnn_model = cnn_model_builder.build_cnn_model_final(best_hps_dict, (time_steps, n_feats), num_classes)\n",
        "        final_cnn_model.fit(X_oversampled, y_train_res,\n",
        "                            epochs=CONFIG[\"epochs\"],\n",
        "                            validation_split=0.2,\n",
        "                            callbacks=[EarlyStopping(monitor=\"val_loss\", patience=CONFIG[\"early_stopping_patience\"], restore_best_weights=True),\n",
        "                                       ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=1, verbose=1)],\n",
        "                            batch_size=CONFIG[\"batch_size\"],\n",
        "                            class_weight=class_weight_dict,\n",
        "                            verbose=1)\n",
        "        cnn_best_model = final_cnn_model\n",
        "        final_cnn_model.save(f\"models/CNN_{best_cnn_strategy}_best_model.h5\")\n",
        "        logger.info(f\"Saved final CNN model: models/CNN_{best_cnn_strategy}_best_model.h5\")\n",
        "    if best_transformer_strategy and transformer_cv_results[best_transformer_strategy]:\n",
        "        try:\n",
        "            best_transformer_fold = max(transformer_cv_results[best_transformer_strategy], key=lambda r: r[\"F1-Score\"])\n",
        "            best_hps_dict = best_transformer_fold[\"Best_Hyperparameters\"]\n",
        "        except:\n",
        "            best_hps_dict = {}\n",
        "        transformer_model_builder = TransformerModelBuilder()\n",
        "        final_transformer_model = transformer_model_builder.build_transformer_model_final(best_hps_dict, (time_steps, n_feats), num_classes)\n",
        "        final_transformer_model.fit(X_oversampled, y_train_res,\n",
        "                                    epochs=CONFIG[\"epochs\"],\n",
        "                                    validation_split=0.2,\n",
        "                                    callbacks=[EarlyStopping(monitor=\"val_loss\", patience=CONFIG[\"early_stopping_patience\"], restore_best_weights=True),\n",
        "                                               ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=1, verbose=1)],\n",
        "                                    batch_size=CONFIG[\"batch_size\"],\n",
        "                                    class_weight=class_weight_dict,\n",
        "                                    verbose=1)\n",
        "        transformer_best_model = final_transformer_model\n",
        "        final_transformer_model.save(f\"models/Transformer_{best_transformer_strategy}_best_model.h5\")\n",
        "        logger.info(f\"Saved final Transformer model: models/Transformer_{best_transformer_strategy}_best_model.h5\")\n",
        "\n",
        "\n",
        "    # 12. Collect Predictions from Each Model\n",
        "    logger.info(\"Collecting predictions from each model for ensemble...\")\n",
        "    y_pred_lstm = y_pred_prob_lstm = None\n",
        "    if best_lstm_strategy in lstm_models:\n",
        "        try:\n",
        "            final_lstm_model = lstm_models[best_lstm_strategy]\n",
        "            y_pred_lstm = np.argmax(final_lstm_model.predict(X_test_nn), axis=1)\n",
        "            y_pred_prob_lstm = final_lstm_model.predict(X_test_nn)\n",
        "        except Exception as e:\n",
        "            logger.error(f\"LSTM prediction error: {e}\")\n",
        "    y_pred_cnn = y_pred_prob_cnn = None\n",
        "    if cnn_best_model:\n",
        "        try:\n",
        "            y_pred_cnn = np.argmax(cnn_best_model.predict(X_test_nn), axis=1)\n",
        "            y_pred_prob_cnn = cnn_best_model.predict(X_test_nn)\n",
        "        except Exception as e:\n",
        "            logger.error(f\"CNN prediction error: {e}\")\n",
        "    y_pred_transformer = y_pred_prob_transformer = None\n",
        "    if transformer_best_model:\n",
        "        try:\n",
        "            y_pred_transformer = np.argmax(transformer_best_model.predict(X_test_nn), axis=1)\n",
        "            y_pred_prob_transformer = transformer_best_model.predict(X_test_nn)\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Transformer prediction error: {e}\")\n",
        "\n",
        "    # 13. Combine Predictions (Averaging Ensemble)\n",
        "    logger.info(\"Combining predictions from all models...\")\n",
        "    pred_probs_list = []\n",
        "    if y_pred_prob_lstm is not None:\n",
        "        adj = adjust_pred_proba(y_pred_prob_lstm, num_classes)\n",
        "        _, pad = prepare_roc_auc(y_test_split, adj, num_classes, label_encoder)\n",
        "        pred_probs_list.append(pad)\n",
        "    if y_pred_prob_cnn is not None:\n",
        "        adj = adjust_pred_proba(y_pred_prob_cnn, num_classes)\n",
        "        _, pad = prepare_roc_auc(y_test_split, adj, num_classes, label_encoder)\n",
        "        pred_probs_list.append(pad)\n",
        "    if y_pred_prob_transformer is not None:\n",
        "        adj = adjust_pred_proba(y_pred_prob_transformer, num_classes)\n",
        "        _, pad = prepare_roc_auc(y_test_split, adj, num_classes, label_encoder)\n",
        "        pred_probs_list.append(pad)\n",
        "\n",
        "    combined_pred = combined_pred_prob = None\n",
        "    if pred_probs_list:\n",
        "        combined_pred_prob = np.mean(pred_probs_list, axis=0)\n",
        "        combined_pred = np.argmax(combined_pred_prob, axis=1)\n",
        "        y_test_bin = label_binarize(y_test_split, classes=range(num_classes))\n",
        "        acc_combined = accuracy_score(y_test_split, combined_pred)\n",
        "        prec_combined = precision_score(y_test_split, combined_pred, average=\"weighted\", zero_division=0)\n",
        "        rec_combined = recall_score(y_test_split, combined_pred, average=\"weighted\", zero_division=0)\n",
        "        f1_combined = f1_score(y_test_split, combined_pred, average=\"weighted\", zero_division=0)\n",
        "        roc_combined = roc_auc_score(y_test_bin, combined_pred_prob, multi_class=\"ovr\", average=\"weighted\")\n",
        "        logger.info(f\"Combined Model: ACC={acc_combined:.4f}, PREC={prec_combined:.4f}, REC={rec_combined:.4f}, F1={f1_combined:.4f}, ROC_AUC={roc_combined:.4f}\")\n",
        "        metrics[\"Combined_Model\"] = {\n",
        "            \"Accuracy\": acc_combined,\n",
        "            \"Precision\": prec_combined,\n",
        "            \"Recall\": rec_combined,\n",
        "            \"F1-Score\": f1_combined,\n",
        "            \"ROC_AUC\": roc_combined,\n",
        "            \"y_pred\": combined_pred,\n",
        "            \"y_pred_prob\": combined_pred_prob\n",
        "        }\n",
        "    else:\n",
        "        logger.error(\"No model probabilities available for ensemble.\")\n",
        "\n",
        "    # 14. Evaluate All Models & Select Best\n",
        "    logger.info(\"Evaluating models to select the best overall...\")\n",
        "    metrics_evaluation = {}\n",
        "    accuracies = {}\n",
        "    f1_scores_eval = {}\n",
        "    roc_aucs = {}\n",
        "    if y_pred_lstm is not None:\n",
        "        acc = accuracy_score(y_test_split, y_pred_lstm)\n",
        "        prec = precision_score(y_test_split, y_pred_lstm, average=\"weighted\", zero_division=0)\n",
        "        rec = recall_score(y_test_split, y_pred_lstm, average=\"weighted\", zero_division=0)\n",
        "        f1_val = f1_score(y_test_split, y_pred_lstm, average=\"weighted\", zero_division=0)\n",
        "        y_test_bin = label_binarize(y_test_split, classes=range(num_classes))\n",
        "        roc_val = roc_auc_score(y_test_bin, y_pred_prob_lstm, multi_class=\"ovr\", average=\"weighted\") if y_pred_prob_lstm is not None else 0.0\n",
        "        metrics_evaluation[\"LSTM\"] = {\"Accuracy\": acc, \"Precision\": prec, \"Recall\": rec, \"F1-Score\": f1_val, \"ROC_AUC\": roc_val,\n",
        "                                      \"y_pred\": y_pred_lstm, \"y_pred_prob\": y_pred_prob_lstm}\n",
        "        accuracies[\"LSTM\"] = acc; f1_scores_eval[\"LSTM\"] = f1_val; roc_aucs[\"LSTM\"] = roc_val\n",
        "    if y_pred_cnn is not None:\n",
        "        acc = accuracy_score(y_test_split, y_pred_cnn)\n",
        "        prec = precision_score(y_test_split, y_pred_cnn, average=\"weighted\", zero_division=0)\n",
        "        rec = recall_score(y_test_split, y_pred_cnn, average=\"weighted\", zero_division=0)\n",
        "        f1_val = f1_score(y_test_split, y_pred_cnn, average=\"weighted\", zero_division=0)\n",
        "        y_test_bin = label_binarize(y_test_split, classes=range(num_classes))\n",
        "        roc_val = roc_auc_score(y_test_bin, y_pred_prob_cnn, multi_class=\"ovr\", average=\"weighted\") if y_pred_prob_cnn is not None else 0.0\n",
        "        metrics_evaluation[\"CNN\"] = {\"Accuracy\": acc, \"Precision\": prec, \"Recall\": rec, \"F1-Score\": f1_val, \"ROC_AUC\": roc_val,\n",
        "                                     \"y_pred\": y_pred_cnn, \"y_pred_prob\": y_pred_prob_cnn}\n",
        "        accuracies[\"CNN\"] = acc; f1_scores_eval[\"CNN\"] = f1_val; roc_aucs[\"CNN\"] = roc_val\n",
        "    if y_pred_transformer is not None:\n",
        "        acc = accuracy_score(y_test_split, y_pred_transformer)\n",
        "        prec = precision_score(y_test_split, y_pred_transformer, average=\"weighted\", zero_division=0)\n",
        "        rec = recall_score(y_test_split, y_pred_transformer, average=\"weighted\", zero_division=0)\n",
        "        f1_val = f1_score(y_test_split, y_pred_transformer, average=\"weighted\", zero_division=0)\n",
        "        y_test_bin = label_binarize(y_test_split, classes=range(num_classes))\n",
        "        roc_val = roc_auc_score(y_test_bin, y_pred_prob_transformer, multi_class=\"ovr\", average=\"weighted\") if y_pred_prob_transformer is not None else 0.0\n",
        "        metrics_evaluation[\"Transformer\"] = {\"Accuracy\": acc, \"Precision\": prec, \"Recall\": rec, \"F1-Score\": f1_val, \"ROC_AUC\": roc_val,\n",
        "                                               \"y_pred\": y_pred_transformer, \"y_pred_prob\": y_pred_prob_transformer}\n",
        "        accuracies[\"Transformer\"] = acc; f1_scores_eval[\"Transformer\"] = f1_val; roc_aucs[\"Transformer\"] = roc_val\n",
        "\n",
        "    if \"Combined_Model\" in metrics:\n",
        "        cm = metrics[\"Combined_Model\"]\n",
        "        metrics_evaluation[\"Combined_Model\"] = cm\n",
        "        accuracies[\"Combined_Model\"] = cm[\"Accuracy\"]\n",
        "        f1_scores_eval[\"Combined_Model\"] = cm[\"F1-Score\"]\n",
        "        roc_aucs[\"Combined_Model\"] = cm[\"ROC_AUC\"]\n",
        "\n",
        "    best_model_name = None; best_model_score = -np.inf\n",
        "    for m in accuracies:\n",
        "        combined_score = 0.5 * f1_scores_eval[m] + 0.5 * roc_aucs[m]\n",
        "        if combined_score > best_model_score:\n",
        "            best_model_score = combined_score\n",
        "            best_model_name = m\n",
        "    logger.info(f\"Best Model Overall: {best_model_name} with Combined Score: {best_model_score:.4f}\")\n",
        "    VisualizationManager.visualize_model_performance_comparison(metrics_evaluation)\n",
        "\n",
        "    # 15. Interpretability (LIME) on All Models\n",
        "    logger.info(\"Running interpretability (LIME) on all models...\")\n",
        "    all_models = {\n",
        "        \"LSTM\": lstm_models.get(best_lstm_strategy),\n",
        "        \"CNN\": cnn_best_model,\n",
        "        \"Transformer\": transformer_best_model\n",
        "\n",
        "    }\n",
        "    for m_name, m_instance in all_models.items():\n",
        "        if m_instance is None:\n",
        "            logger.warning(f\"{m_name} not available for interpretability.\")\n",
        "            continue\n",
        "        if m_name in [\"LSTM\", \"CNN\", \"Transformer\"]:\n",
        "            X_test_model = X_test_nn\n",
        "            y_test_model = y_test_split\n",
        "            X_train_model = X_train_nn\n",
        "            y_train_model = y_train_split\n",
        "            if X_test_model.ndim == 3:\n",
        "                w_size = X_test_model.shape[1]\n",
        "                feats = X_test_model.shape[2]\n",
        "                feature_names = [f\"Feature_{i+1}\" for i in range(w_size * feats)]\n",
        "            else:\n",
        "                feature_names = [f\"Feature_{i+1}\" for i in range(X_test_model.shape[1])]\n",
        "        else:\n",
        "            X_test_model = X_test_split\n",
        "            y_test_model = y_test_split\n",
        "            X_train_model = X_train_split\n",
        "            y_train_model = y_train_split\n",
        "            feature_names = [f\"Feature_{i+1}\" for i in range(X_test_model.shape[1])]\n",
        "        ModelInterpretability.lime_analysis(model=m_instance,\n",
        "                                            X_train=X_train_model,\n",
        "                                            y_train=y_train_model,\n",
        "                                            X_test=X_test_model,\n",
        "                                            y_test=y_test_model,\n",
        "                                            feature_names=feature_names,\n",
        "                                            model_type=m_name)\n",
        "\n",
        "    # 16. Save the Best Model\n",
        "    logger.info(\"Saving the best model for future use...\")\n",
        "    timestamp = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "    if best_model_name == \"LSTM\" and best_lstm_strategy in lstm_models:\n",
        "        lstm_models[best_lstm_strategy].save(f\"models/LSTM_best_{timestamp}.h5\")\n",
        "        logger.info(f\"Saved best LSTM as models/LSTM_best_{timestamp}.h5\")\n",
        "    elif best_model_name == \"CNN\" and cnn_best_model:\n",
        "        cnn_best_model.save(f\"models/CNN_best_{timestamp}.h5\")\n",
        "        logger.info(f\"Saved best CNN as models/CNN_best_{timestamp}.h5\")\n",
        "    elif best_model_name == \"Transformer\" and transformer_best_model:\n",
        "        transformer_best_model.save(f\"models/Transformer_best_{timestamp}.h5\")\n",
        "        logger.info(f\"Saved best Transformer as models/Transformer_best_{timestamp}.h5\")\n",
        "    elif best_model_name == \"Combined_Model\":\n",
        "        logger.info(\"Best model is the Combined Model (ensemble). Individual models are already saved.\")\n",
        "    else:\n",
        "        logger.warning(\"No valid best model selected to save.\")\n",
        "\n",
        "    logger.info(\"Pipeline complete.\")\n",
        "\n",
        "\n",
        "    #Save the predictions to a CSV file\n",
        "    logger.info(\"Saving predictions to a CSV file...\")\n",
        "    df_predictions = pd.read_csv(\"/content/final_labels_cleaned.csv\")\n",
        "    mask = np.zeros(len(df_predictions), dtype=bool)\n",
        "    for low, high in STRIDE_INTERVALS:\n",
        "        mask |= (df_predictions['Frame'] >= low) & (df_predictions['Frame'] <= high)\n",
        "    df_predictions = df_predictions[mask]\n",
        "    X_features = df_predictions[[\"Synergy_1\", \"Synergy_2\", \"Synergy_3\", \"Synergy_4\", \"Synergy_5\"]].values\n",
        "\n",
        "    # Create sequences from the features and record the center indices\n",
        "    X_sequences_pred, _, center_indices = FeatureExtractor.create_sequences(\n",
        "        X_features, window_size=CONFIG['window_size'], overlap=CONFIG['overlap'], labels=None\n",
        "    )\n",
        "\n",
        "    if best_model_name == \"LSTM\":\n",
        "        y_pred_sequence = np.argmax(lstm_models[best_lstm_strategy].predict(X_sequences_pred), axis=1)\n",
        "    elif best_model_name == \"CNN\":\n",
        "        y_pred_sequence = np.argmax(cnn_best_model.predict(X_sequences_pred), axis=1)\n",
        "    elif best_model_name == \"Transformer\":\n",
        "        y_pred_sequence = np.argmax(transformer_best_model.predict(X_sequences_pred), axis=1)\n",
        "    else:\n",
        "        logger.error(\"No valid best model found for prediction.\")\n",
        "        return\n",
        "\n",
        "    # Create an array for full predictions and assign each sequence prediction to its center row\n",
        "    full_predictions = np.full(len(df_predictions), -1, dtype=int)\n",
        "    for idx, pred in zip(center_indices, y_pred_sequence):\n",
        "        full_predictions[idx] = pred\n",
        "\n",
        "    # Fill missing predictions by forward and backward filling\n",
        "    pred_series = pd.Series(full_predictions)\n",
        "    pred_series.replace(-1, np.nan, inplace=True)\n",
        "    pred_series = pred_series.ffill().bfill().astype(int)\n",
        "\n",
        "    predicted_labels = label_encoder.inverse_transform(pred_series.values)\n",
        "    df_predictions[\"PredictedStage\"] = predicted_labels\n",
        "    df_predictions.to_csv(\"final_labels_with_predictions.csv\", index=False)\n",
        "    logger.info(\"CSV file with predictions saved as final_labels_with_predictions.csv\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main_pipeline()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s7NuIDwDnn9k"
      },
      "outputs": [],
      "source": [
        "# --------------------------------------------------\n",
        "# Desired state distribution and penalty weight\n",
        "# --------------------------------------------------\n",
        "DESIRED_DISTRIBUTION = np.array([0.10, 0.40, 0.10, 0.20, 0.20])\n",
        "LAMBDA_PENALTY = 100  # Penalty weight for deviation from the target distribution\n",
        "\n",
        "# --------------------------------------------------\n",
        "# Helper function to enforce symmetry and positive–definiteness\n",
        "# --------------------------------------------------\n",
        "def enforce_symmetry(cov, min_covar=1e-3):\n",
        "    cov_sym = 0.5 * (cov + cov.T)\n",
        "    eigvals, eigvecs = np.linalg.eigh(cov_sym)\n",
        "    eigvals[eigvals < min_covar] = min_covar  # Ensure all eigenvalues are at least min_covar\n",
        "    cov_pd = eigvecs @ np.diag(eigvals) @ eigvecs.T\n",
        "    return cov_pd\n",
        "\n",
        "# --------------------------------------------------\n",
        "# Helper function to filter synergy data to stride intervals\n",
        "# --------------------------------------------------\n",
        "def filter_stride_intervals(df, frame_col='Frame'):\n",
        "    STRIDE_INTERVALS = [\n",
        "        (0, 5937), (5938, 9521), (15938, 18914),\n",
        "        (25059, 28499), (28500, 31683), (35059, 38099),\n",
        "        (38100, 41203), (47684, 50606), (56885, 59990),\n",
        "        (66006, 69046), (75526, 79271), (85384, 88231),\n",
        "        (88232, 91271), (96936, 99848), (99849, 105864),\n",
        "        (105865, 108570), (115114, 118026), (118027, 121067),\n",
        "        (127611, 130587), (135852, 138698), (138699, 141516),\n",
        "        (147147, 150252), (150253, 152508), (152509, 155726)\n",
        "    ]\n",
        "    mask = pd.Series(False, index=df.index)\n",
        "    for start, end in STRIDE_INTERVALS:\n",
        "        mask |= (df[frame_col] >= start) & (df[frame_col] <= end)\n",
        "    return df[mask].copy()\n",
        "\n",
        "# --------------------------------------------------\n",
        "# Compute deviation from the desired state distribution\n",
        "# --------------------------------------------------\n",
        "def compute_distribution_error(model):\n",
        "    stat_dist = stationary_distribution(model.transmat_)\n",
        "    error = np.sum(np.abs(stat_dist - DESIRED_DISTRIBUTION))\n",
        "    return error\n",
        "\n",
        "def stationary_distribution(transmat):\n",
        "    eigvals, eigvecs = np.linalg.eig(transmat.T)\n",
        "    idx = np.argmin(np.abs(eigvals - 1))\n",
        "    stat = np.real(eigvecs[:, idx])\n",
        "    total = np.sum(stat)\n",
        "    stat = stat / total if total != 0 else np.array([0.10, 0.40, 0.10, 0.20, 0.20])\n",
        "    return stat\n",
        "\n",
        "# --------------------------------------------------\n",
        "# Tuning function using ASHA for synergy HMM\n",
        "# --------------------------------------------------\n",
        "def tune_pipeline_synergy_eval(config):\n",
        "    # Load synergy data with ground-truth labels\n",
        "    df_synergy = pd.read_csv(\"final_labels_with_predictions.csv\")\n",
        "    synergy_cols = ['Synergy_1', 'Synergy_2', 'Synergy_3', 'Synergy_4', 'Synergy_5']\n",
        "    df_synergy = df_synergy[['Frame'] + synergy_cols + ['PredictedStage']]\n",
        "\n",
        "    # Filter only stride intervals and non-zero labels\n",
        "    df_stride = df_synergy[df_synergy['PredictedStage'] != 0].copy()\n",
        "    df_stride.reset_index(drop=True, inplace=True)\n",
        "    df_stride = filter_stride_intervals(df_stride, frame_col='Frame')\n",
        "    df_stride.reset_index(drop=True, inplace=True)\n",
        "\n",
        "    # Extract features and ground truth\n",
        "    X = df_stride[synergy_cols].values\n",
        "    y_true = df_stride['PredictedStage'].values\n",
        "\n",
        "    # Check for NaN or inf in X and replace if necessary\n",
        "    if np.isnan(X).any() or np.isinf(X).any():\n",
        "        print(\"Warning: Input X contains NaN or Inf. Replacing with zeros.\")\n",
        "        X = np.nan_to_num(X)\n",
        "\n",
        "    # Define transition matrix with hyperparameters\n",
        "    transmat = np.array([\n",
        "        [config[\"alpha\"],  1 - config[\"alpha\"],  0.0,               0.0,                0.0],\n",
        "        [0.0,              config[\"beta\"],       1 - config[\"beta\"], 0.0,                0.0],\n",
        "        [0.0,              0.0,                  config[\"gamma\"],    1 - config[\"gamma\"], 0.0],\n",
        "        [0.0,              0.0,                  0.0,                config[\"delta\"],     1 - config[\"delta\"]],\n",
        "        [1 - config[\"theta\"], 0.0,               0.0,                0.0,                config[\"theta\"]]\n",
        "    ])\n",
        "\n",
        "    # Train the HMM\n",
        "    synergy_hmm = hmm.GaussianHMM(n_components=5, covariance_type='full',\n",
        "                                  n_iter=10, random_state=42, init_params='mc', min_covar=1e-3)\n",
        "    synergy_hmm.startprob_ = np.array([1, 0, 0, 0, 0])\n",
        "    synergy_hmm.transmat_ = transmat\n",
        "    synergy_hmm.fit(X)\n",
        "\n",
        "    # Enforce symmetry on covariance\n",
        "    synergy_hmm.covars_ = np.array([enforce_symmetry(cov) for cov in synergy_hmm.covars_])\n",
        "\n",
        "    # Predict gait stages (convert 0-index to 1-index)\n",
        "    predicted_states = synergy_hmm.predict(X) + 1\n",
        "    acc = accuracy_score(y_true, predicted_states)\n",
        "\n",
        "    # Compute the penalty for distribution error\n",
        "    distribution_error = compute_distribution_error(synergy_hmm)\n",
        "    total_score = acc - LAMBDA_PENALTY * distribution_error\n",
        "\n",
        "    # Check for invalid total_score values\n",
        "    if np.isnan(total_score) or np.isinf(total_score):\n",
        "        total_score = -1e6  # Assign a large penalty for invalid cases\n",
        "\n",
        "    tune.report({\"score\": total_score, \"accuracy\": acc})\n",
        "\n",
        "# --------------------------------------------------\n",
        "# Hyperparameter tuning setup using ASHA\n",
        "# --------------------------------------------------\n",
        "search_space = {\n",
        "    \"alpha\": tune.uniform(0.05, 0.95),\n",
        "    \"beta\":  tune.uniform(0.05, 0.95),\n",
        "    \"gamma\": tune.uniform(0.05, 0.95),\n",
        "    \"delta\": tune.uniform(0.05, 0.95),\n",
        "    \"theta\": tune.uniform(0.05, 0.95)\n",
        "}\n",
        "\n",
        "asha_scheduler = ASHAScheduler(\n",
        "    time_attr=\"training_iteration\",\n",
        "    max_t=100,\n",
        "    metric=\"score\",\n",
        "    mode=\"max\"\n",
        ")\n",
        "\n",
        "ray.init(ignore_reinit_error=True, include_dashboard=False)\n",
        "\n",
        "analysis = tune.run(\n",
        "    tune_pipeline_synergy_eval,\n",
        "    config=search_space,\n",
        "    scheduler=asha_scheduler,\n",
        "    num_samples=100,\n",
        "    reuse_actors=True,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "best_config = analysis.get_best_config(metric=\"score\", mode=\"max\")\n",
        "print(\"Best synergy hyperparameters (ASHA):\", best_config)\n",
        "\n",
        "# --------------------------------------------------\n",
        "# Final training using best hyperparameters\n",
        "# --------------------------------------------------\n",
        "df_synergy = pd.read_csv(\"/content/final_labels_cleaned_synery.csv\")\n",
        "synergy_cols = ['Synergy_1', 'Synergy_2', 'Synergy_3', 'Synergy_4', 'Synergy_5']\n",
        "df_synergy = df_synergy[['Frame'] + synergy_cols + ['PredictedStage']]\n",
        "\n",
        "df_stride = df_synergy[df_synergy['PredictedStage'] != 0].copy()\n",
        "df_stride.reset_index(drop=True, inplace=True)\n",
        "df_stride = filter_stride_intervals(df_stride, frame_col='Frame')\n",
        "df_stride.reset_index(drop=True, inplace=True)\n",
        "\n",
        "X = df_stride[synergy_cols].values\n",
        "y_true = df_stride['PredictedStage'].values\n",
        "\n",
        "# Check X for NaN or inf\n",
        "if np.isnan(X).any() or np.isinf(X).any():\n",
        "    print(\"Warning: Final training input X contains NaN or Inf. Replacing with zeros.\")\n",
        "    X = np.nan_to_num(X)\n",
        "\n",
        "transmat = np.array([\n",
        "    [best_config[\"alpha\"], 1 - best_config[\"alpha\"], 0.0, 0.0, 0.0],\n",
        "    [0.0, best_config[\"beta\"], 1 - best_config[\"beta\"], 0.0, 0.0],\n",
        "    [0.0, 0.0, best_config[\"gamma\"], 1 - best_config[\"gamma\"], 0.0],\n",
        "    [0.0, 0.0, 0.0, best_config[\"delta\"], 1 - best_config[\"delta\"]],\n",
        "    [1 - best_config[\"theta\"], 0.0, 0.0, 0.0, best_config[\"theta\"]]\n",
        "])\n",
        "\n",
        "synergy_hmm = hmm.GaussianHMM(n_components=5, covariance_type='full', n_iter=10,\n",
        "                              random_state=42, init_params='mc', min_covar=1e-3)\n",
        "synergy_hmm.startprob_ = np.array([1, 0, 0, 0, 0])\n",
        "synergy_hmm.transmat_ = transmat\n",
        "synergy_hmm.fit(X)\n",
        "\n",
        "# Enforce symmetry on covariance in final model as well\n",
        "synergy_hmm.covars_ = np.array([enforce_symmetry(cov) for cov in synergy_hmm.covars_])\n",
        "\n",
        "predicted_states = synergy_hmm.predict(X) + 1\n",
        "print(classification_report(y_true, predicted_states))\n",
        "# Compute the confusion matrix\n",
        "cm = confusion_matrix(y_true, predicted_states)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(cm)\n",
        "\n",
        "# Plot the confusion matrix as a heatmap\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "plt.xlabel('Predicted Labels')\n",
        "plt.ylabel('True Labels')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}